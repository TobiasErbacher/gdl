{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tWXR6GaL0Grm",
        "outputId": "2c023989-c7e8-4438-9c24-d40fe78c5788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current PyTorch version: 2.6.0+cu124\n",
            "Current CUDA version: 12.4\n",
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
            "Collecting torch==2.5.0+cu124\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torch-2.5.0%2Bcu124-cp311-cp311-linux_x86_64.whl (908.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.20.0+cu124\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.20.0%2Bcu124-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.5.0+cu124\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.5.0%2Bcu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.1.0 (from torch==2.5.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0+cu124) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.0+cu124) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.0+cu124) (11.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.0+cu124) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.0+cu124) (3.0.2)\n",
            "Installing collected packages: triton, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-2.5.0+cu124 torchaudio-2.5.0+cu124 torchvision-0.20.0+cu124 triton-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "ac60bbbe102b4e92bccdc9fdb77796d9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"Current PyTorch version: {torch.__version__}\")\n",
        "print(f\"Current CUDA version: {torch.version.cuda}\")\n",
        "\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "# PyTorch 2.5.0 with CUDA 12.4\n",
        "!pip install torch==2.5.0+cu124 torchvision==0.20.0+cu124 torchaudio==2.5.0+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "# now geometric stuff\n",
        "#need ro restart session"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.5.0+cu124.html\n",
        "!pip install torch-geometric\n",
        "!pip install matplotlib seaborn PyYAML tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL9WrlPy1PnM",
        "outputId": "7b57fc0e-4379-416c-dd0c-f15ce101699c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.5.0+cu124.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_scatter-2.1.2%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_sparse-0.6.18%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.14.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Installing collected packages: torch-scatter, torch-sparse\n",
            "Successfully installed torch-scatter-2.1.2+pt25cu124 torch-sparse-0.6.18+pt25cu124\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.14)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import json\n",
        "from typing import List, Tuple, Dict, Optional, Union\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "\n",
        "# PyTorch Geometric imports\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "from torch_geometric.utils import to_networkx, degree\n",
        "import networkx as nx\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "6oKCI4Dw1UIr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "42tZ20Hw1Us6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PonderGAT(nn.Module):\n",
        "    \"\"\"\n",
        "    PonderNet-style Graph Attention Network with Gumbel-Sigmoid halting.\n",
        "\n",
        "    We implement an adaptive computation for graph neural networks, where each node\n",
        "    independently determines how many computation steps it needs through a learned halting mechanism.\n",
        "    We do a node-level Bernoulli sampling thinking the node - layer decision of halting as a condition probability\n",
        "    such that the actual probability of halting becomes a geometric distribution. This is as suggested in pondernet paper\n",
        "    with the difference that here we use a Gumbel approach for training instead of their \"expected\" loss - since we claim that\n",
        "    it should be more representative of the actual behaviour it will experience by inference and preserving the computational graph\n",
        "    for backprop.\n",
        "\n",
        "    In addition, we use a attention network with regularization for the message passing since the idea is that the\n",
        "    attention weights should capture on each layer\n",
        "    the importance of each neighbour,adding some degree of directionality as it was suggested by CO-GNN -\n",
        "    although they use a completely different approach.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        hidden_features: int,\n",
        "        out_features: int,\n",
        "        num_layers: int = 5,\n",
        "        heads: int = 1,\n",
        "        dropout: float = 0.5,\n",
        "        halting_hidden: int = 32,\n",
        "        beta_kl: float = 0.01,\n",
        "        beta_attn: float = 0.01,\n",
        "        prior_lambda: float = 0.2,\n",
        "        use_mlp_input: bool = True,\n",
        "        shared_gat: bool = False,\n",
        "        gumbel_temperature: float = 1.0,\n",
        "        use_residual: bool = True,\n",
        "        residual_alpha: float = 0.1\n",
        "    ):\n",
        "        super(PonderGAT, self).__init__()\n",
        "\n",
        "        self.in_features = in_features\n",
        "        self.hidden_features = hidden_features\n",
        "        self.out_features = out_features\n",
        "        self.num_layers = num_layers\n",
        "        self.heads = heads\n",
        "        self.beta_kl = beta_kl\n",
        "        self.beta_attn = beta_attn\n",
        "        self.prior_lambda = prior_lambda\n",
        "        self.shared_gat = shared_gat\n",
        "        self.gumbel_temperature = gumbel_temperature\n",
        "        self.use_residual = use_residual\n",
        "        self.residual_alpha = residual_alpha\n",
        "\n",
        "        # hidden dimension considering multi-head concatenation\n",
        "        self.hidden_dim = hidden_features * heads\n",
        "\n",
        "        # first projection MLP: maps x_i to h_i^(0)\n",
        "        if use_mlp_input:\n",
        "            self.input_mlp = nn.Sequential(\n",
        "                nn.Linear(in_features, 2*self.hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(2*self.hidden_dim, self.hidden_dim)\n",
        "            )\n",
        "        else:\n",
        "            self.input_mlp = nn.Linear(in_features, self.hidden_dim)\n",
        "\n",
        "        # GAT layers - perform message passing with attention\n",
        "        if shared_gat:\n",
        "            # Single GAT layer applied repeatedly -> in case we want so reduce the use of wieghts\n",
        "            self.gat_layers = nn.ModuleList([\n",
        "                GATConv(\n",
        "                    in_channels=self.hidden_dim,\n",
        "                    out_channels=hidden_features,\n",
        "                    heads=heads,\n",
        "                    concat=True,\n",
        "                    dropout=dropout,\n",
        "                    add_self_loops=True\n",
        "                )\n",
        "            ])\n",
        "        else:\n",
        "            # Separate GAT layer for each step\n",
        "            self.gat_layers = nn.ModuleList([\n",
        "                GATConv(\n",
        "                    in_channels=self.hidden_dim,\n",
        "                    out_channels=hidden_features,\n",
        "                    heads=heads,\n",
        "                    concat=True,\n",
        "                    dropout=dropout,\n",
        "                    add_self_loops=True\n",
        "                ) for _ in range(num_layers)\n",
        "            ])\n",
        "\n",
        "        # Halting networks - one for each layer\n",
        "        self.halting_networks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(self.hidden_dim + 1, halting_hidden),  # +1 for distance\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(halting_hidden, 1)\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Classification networks - one for each layer\n",
        "        self.classification_networks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(self.hidden_dim, out_features)\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"i use the same idea of the authors for initializing the halting network with the 1/n+1 expectation.\"\"\"\n",
        "        if isinstance(self.input_mlp, nn.Sequential):\n",
        "            for module in self.input_mlp:\n",
        "                if isinstance(module, nn.Linear):\n",
        "                    nn.init.xavier_uniform_(module.weight)\n",
        "                    if module.bias is not None:\n",
        "                        nn.init.zeros_(module.bias)\n",
        "        else:\n",
        "            nn.init.xavier_uniform_(self.input_mlp.weight)\n",
        "            nn.init.zeros_(self.input_mlp.bias)\n",
        "\n",
        "        x = (self.num_layers + 1)\n",
        "        b = math.log((1/x)/(1-(1/x)))\n",
        "        for halt_net in self.halting_networks:\n",
        "            halt_net[-1].bias.data.fill_(b)\n",
        "\n",
        "    def gumbel_sigmoid(self, logits, temperature=1.0, hard=False, eps=1e-10):\n",
        "        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + eps) + eps)\n",
        "\n",
        "        y_soft = torch.sigmoid((logits + gumbel_noise) / temperature)\n",
        "\n",
        "        if hard:\n",
        "            y_hard = (y_soft > 0.5).float()\n",
        "            # straight-through trick: use hard values in forward, but soft gradient in backward\n",
        "            return y_hard - y_soft.detach() + y_soft\n",
        "\n",
        "        return y_soft\n",
        "\n",
        "    def forward(self, x, edge_index, training=True):\n",
        "        device = x.device\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        h_0 = self.input_mlp(x)\n",
        "\n",
        "        # halting data for decision\n",
        "        not_halted_mask = torch.ones(batch_size, dtype=torch.bool, device=device)\n",
        "        node_outputs = torch.zeros(batch_size, self.out_features, device=device)\n",
        "        halting_layers = torch.zeros(batch_size, dtype=torch.long, device=device)\n",
        "\n",
        "        all_logits = []\n",
        "        all_lambdas = []\n",
        "        all_attention = []\n",
        "\n",
        "        # embedding for distance calculation\n",
        "        h = h_0\n",
        "        h_prev = h_0\n",
        "\n",
        "        for l in range(self.num_layers):\n",
        "            gat_layer = self.gat_layers[0] if self.shared_gat else self.gat_layers[l]\n",
        "\n",
        "            # message passing that updates all node representations (even halted ones) -> we will ignore them\n",
        "            h_new_all, attn_weights = gat_layer(h, edge_index, return_attention_weights=True)\n",
        "            h_new_all = F.elu(h_new_all)\n",
        "\n",
        "            # layer norm -> authors did it and seems to actually help a lot!\n",
        "            mean = h_new_all.mean(dim=-1, keepdim=True)\n",
        "            var = h_new_all.var(dim=-1, keepdim=True, unbiased=False)\n",
        "            h_new_all = (h_new_all - mean) / torch.sqrt(var + 1e-5)\n",
        "\n",
        "            # in case we want to use it but i am not doing it by default. just wanted to check if it will increase by a lot but i dont see that .\n",
        "            if self.use_residual:\n",
        "                h_new_all = h_new_all + self.residual_alpha * h\n",
        "\n",
        "            # update non-halted nodes, keep previous embedding for halted nodes\n",
        "            h_new = torch.where(not_halted_mask.unsqueeze(1), h_new_all, h)\n",
        "            h_new = self.dropout(h_new)\n",
        "\n",
        "            # normalized embedding distance for all nodes\n",
        "            distances = torch.norm(h_new - h_prev, dim=1, keepdim=True) / math.sqrt(self.hidden_dim)\n",
        "\n",
        "            #halting probabilities for all nodes\n",
        "            halting_input = torch.cat([h_new, distances], dim=1)\n",
        "            raw_lambda = self.halting_networks[l](halting_input).squeeze(-1)\n",
        "            lambda_l = torch.sigmoid(raw_lambda)\n",
        "\n",
        "            # classification logits for all nodes\n",
        "            logits_l = self.classification_networks[l](h_new)\n",
        "\n",
        "            # and we store the loss computation\n",
        "            all_logits.append(logits_l)\n",
        "            all_lambdas.append(lambda_l)\n",
        "            all_attention.append(attn_weights)\n",
        "\n",
        "            # for the nodes that haven't halted yet, we need to decide whether to halt at this layer\n",
        "            if not_halted_mask.any():\n",
        "                active_indices = torch.nonzero(not_halted_mask, as_tuple=True)[0]\n",
        "                active_lambda = lambda_l[active_indices] #prob of active nodes\n",
        "\n",
        "                if training:\n",
        "                    # for training, we use Gumbel-Sigmoid for differentiable sampling\n",
        "                    active_logits = raw_lambda[active_indices]\n",
        "                    gumbel_samples = self.gumbel_sigmoid(\n",
        "                        active_logits,\n",
        "                        temperature=self.gumbel_temperature,\n",
        "                        hard=True\n",
        "                    )\n",
        "                    halt_decisions = (gumbel_samples > 0.5)\n",
        "                else:\n",
        "                    # in evaluation, we use direct Bernoulli sampling\n",
        "                    halt_decisions = torch.bernoulli(active_lambda).bool()\n",
        "\n",
        "                # nodes that halt at this layer\n",
        "                halting_indices = active_indices[halt_decisions]\n",
        "\n",
        "                if len(halting_indices) > 0:\n",
        "                    node_outputs_new = node_outputs.clone()\n",
        "                    node_outputs_new[halting_indices] = logits_l[halting_indices]\n",
        "                    node_outputs = node_outputs_new #stored logits\n",
        "\n",
        "                    halting_layers_new = halting_layers.clone()\n",
        "                    halting_layers_new[halting_indices] = l + 1\n",
        "                    halting_layers = halting_layers_new  #storing halting layer.\n",
        "\n",
        "                    not_halted_mask_new = not_halted_mask.clone()\n",
        "                    not_halted_mask_new[halting_indices] = False\n",
        "                    not_halted_mask = not_halted_mask_new #and we update the halted mask\n",
        "\n",
        "            # and we reupdate h and h_prev for the next layer\n",
        "            h_prev = h\n",
        "            h = h_new\n",
        "\n",
        "        # now last case of reaching the end but did not halt\n",
        "        if not_halted_mask.any():\n",
        "            still_active = torch.nonzero(not_halted_mask, as_tuple=True)[0]\n",
        "\n",
        "            # final layer logits and halting layer\n",
        "            node_outputs_new = node_outputs.clone()\n",
        "            node_outputs_new[still_active] = logits_l[still_active]\n",
        "            node_outputs = node_outputs_new\n",
        "\n",
        "            halting_layers_new = halting_layers.clone()\n",
        "            halting_layers_new[still_active] = self.num_layers\n",
        "            halting_layers = halting_layers_new\n",
        "\n",
        "        # then with all the lambdas we can build p_n for loss computation -> the geometric.\n",
        "        lambda_matrix = torch.stack(all_lambdas, dim=1)  # [N, L]\n",
        "        p_n = torch.zeros(batch_size, self.num_layers, device=device)\n",
        "\n",
        "        # p_n calculation\n",
        "        one_minus_lambda = 1 - lambda_matrix\n",
        "        cumprod = torch.cumprod(one_minus_lambda, dim=1)\n",
        "\n",
        "        p_n[:, 0] = lambda_matrix[:, 0] #first layer.\n",
        "        # next: p_n[l] = lambda[l] * prod(1-lambda[j]) for j=0...l-1\n",
        "        for l in range(1, self.num_layers):\n",
        "            p_n[:, l] = lambda_matrix[:, l] * cumprod[:, l-1]\n",
        "\n",
        "        # and the normalization to 1\n",
        "        sum_p = p_n.sum(dim=1, keepdim=True)\n",
        "        leftover = (1.0 - sum_p).clamp(min=0.0)\n",
        "        p_n[:, -1] = p_n[:, -1] + leftover.squeeze(-1)\n",
        "\n",
        "        return all_logits, p_n, all_attention, halting_layers, node_outputs\n",
        "\n",
        "    def compute_loss(self, all_logits, p_n, labels, halting_layers, final_logits, all_attention=None, mask=None):\n",
        "        device = labels.device\n",
        "        # the datasets bring masks for training, val and test.\n",
        "        if mask is not None:\n",
        "            masked_p_n = p_n[mask]\n",
        "            masked_labels = labels[mask]\n",
        "            masked_final_logits = final_logits[mask]\n",
        "        else:\n",
        "            masked_p_n = p_n\n",
        "            masked_labels = labels\n",
        "            masked_final_logits = final_logits\n",
        "\n",
        "        # 1. Task Loss - Cross Entropy on the sampled layer predictions\n",
        "        task_loss = F.cross_entropy(masked_final_logits, masked_labels)\n",
        "\n",
        "        # 2. KL Divergence with Geometric Prior\n",
        "        # geometric prior distribution\n",
        "        layer_indices = torch.arange(1, self.num_layers + 1, device=device).float()\n",
        "        prior_probs = self.prior_lambda * ((1 - self.prior_lambda) ** (layer_indices - 1))\n",
        "        prior_probs = prior_probs / prior_probs.sum()  # and norm to 1\n",
        "        # actual KL divergence: p_n * log(p_n / prior)\n",
        "        epsilon = 1e-10\n",
        "        masked_p_n_safe = masked_p_n + epsilon\n",
        "        prior_probs_safe = prior_probs + epsilon\n",
        "\n",
        "        kl_div = masked_p_n_safe * (torch.log(masked_p_n_safe) - torch.log(prior_probs_safe.unsqueeze(0)))\n",
        "        kl_loss = self.beta_kl * kl_div.sum(dim=1).mean()\n",
        "\n",
        "        # 3. Attention Entropy Loss\n",
        "        attn_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "        if all_attention is not None and self.beta_attn > 0:\n",
        "            total_entropy = 0.0\n",
        "            count = 0\n",
        "\n",
        "            for attn_tuple in all_attention:\n",
        "                _, attn_weights = attn_tuple\n",
        "                if isinstance(attn_weights, list):\n",
        "                    attn_weights = attn_weights[0]\n",
        "\n",
        "                # sadly we need non-zero values for log calculation\n",
        "                attn_weights = torch.clamp(attn_weights, min=1e-10)\n",
        "                entropy = -(attn_weights * torch.log(attn_weights)).sum()\n",
        "                total_entropy += entropy\n",
        "                count += attn_weights.numel()\n",
        "\n",
        "            if count > 0:\n",
        "                attn_loss = self.beta_attn * (total_entropy / count)\n",
        "\n",
        "        # TOTAL LOSS\n",
        "        total_loss = task_loss + kl_loss + attn_loss\n",
        "\n",
        "        loss_dict = {\n",
        "            'task_loss': task_loss.item(),\n",
        "            'kl_loss': kl_loss.item(),\n",
        "            'attn_loss': attn_loss.item(),\n",
        "            'total_loss': total_loss.item()\n",
        "        }\n",
        "\n",
        "        return total_loss, loss_dict\n",
        "\n",
        "    def inference(self, x, edge_index):\n",
        "        \"\"\"\n",
        "        bernoulli sampling at each layer.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            _, _, _, halting_layers, final_logits = self.forward(\n",
        "                x, edge_index, training=False\n",
        "            )\n",
        "\n",
        "            final_preds = final_logits.argmax(dim=1)\n",
        "\n",
        "        return final_preds, final_logits, halting_layers"
      ],
      "metadata": {
        "id": "ZDt2xtaG1VMw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "EIBB2_uI2pb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, data, optimizer, device):\n",
        "    \"\"\"\n",
        "    one epoch training using gumbal\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    x = data.x.to(device)\n",
        "    edge_index = data.edge_index.to(device)\n",
        "    labels = data.y.to(device)\n",
        "    mask = data.train_mask.to(device)\n",
        "\n",
        "    # forward pass with Gumbel sampling\n",
        "    all_logits, p_n, all_attention, halting_layers, final_logits = model(\n",
        "        x, edge_index, training=True\n",
        "    )\n",
        "\n",
        "    # loss\n",
        "    loss, loss_dict = model.compute_loss(\n",
        "        all_logits, p_n, labels, halting_layers,\n",
        "        final_logits, all_attention, mask\n",
        "    )\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # accuracy on the halting layer logits\n",
        "    pred = final_logits.argmax(dim=1)\n",
        "    correct = pred[mask].eq(labels[mask]).sum().item()\n",
        "    total = mask.sum().item()\n",
        "    acc = correct / total if total > 0 else 0\n",
        "\n",
        "    # average halting layer\n",
        "    avg_layers = halting_layers[mask].float().mean().item()\n",
        "\n",
        "    return loss.item(), loss_dict, acc, avg_layers\n",
        "\n",
        "\n",
        "def evaluate(model, data, mask_name, device):\n",
        "    model.eval()\n",
        "\n",
        "    x = data.x.to(device)\n",
        "    edge_index = data.edge_index.to(device)\n",
        "    labels = data.y.to(device)\n",
        "\n",
        "    if mask_name == 'train':\n",
        "        mask = data.train_mask.to(device)\n",
        "    elif mask_name == 'val':\n",
        "        mask = data.val_mask.to(device)\n",
        "    elif mask_name == 'test':\n",
        "        mask = data.test_mask.to(device)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown mask: {mask_name}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred, final_logits, halting_layers = model.inference(x, edge_index)\n",
        "\n",
        "        correct = pred[mask].eq(labels[mask]).sum().item()\n",
        "        total = mask.sum().item()\n",
        "        acc = correct / total if total > 0 else 0\n",
        "\n",
        "        avg_layers = halting_layers[mask].float().mean().item()\n",
        "\n",
        "    return acc, avg_layers"
      ],
      "metadata": {
        "id": "5uAG2cFL2kZN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "t479Ci_I2ucC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_training_curves(train_stats, dataset_name, save_dir):\n",
        "    \"\"\"\n",
        "    Plot training curves including loss, accuracy, and average layers.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Plot 1: Training loss components\n",
        "    axes[0, 0].plot(train_stats['train_loss'], 'b-', label='Total Loss')\n",
        "    if 'task_loss' in train_stats:\n",
        "        axes[0, 0].plot(train_stats['task_loss'], 'g-', label='Task Loss')\n",
        "    if 'kl_loss' in train_stats:\n",
        "        axes[0, 0].plot(train_stats['kl_loss'], 'r-', label='KL Loss')\n",
        "    if 'attn_loss' in train_stats:\n",
        "        axes[0, 0].plot(train_stats['attn_loss'], 'y-', label='Attn Loss')\n",
        "    axes[0, 0].set_title('Loss Components')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Training and validation accuracy\n",
        "    axes[0, 1].plot(train_stats['train_acc'], 'b-', label='Train')\n",
        "    axes[0, 1].plot(train_stats['val_acc'], 'r-', label='Val')\n",
        "    axes[0, 1].set_title('Accuracy')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Accuracy')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Average layers\n",
        "    axes[1, 0].plot(train_stats['train_layers'], 'b-', label='Train')\n",
        "    axes[1, 0].plot(train_stats['val_layers'], 'r-', label='Val')\n",
        "    axes[1, 0].set_title('Average Layers')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Layers')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Loss curve\n",
        "    axes[1, 1].semilogy(train_stats['train_loss'], 'b-', label='Train')\n",
        "    axes[1, 1].set_title('Loss (log scale)')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Loss')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_dir}/training_curves_{dataset_name}.png', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def visualize_layer_distribution(halting_layers, num_layers, dataset_name, save_dir):\n",
        "    \"\"\"\n",
        "    distribution of halting layers across nodes.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    halting_np = halting_layers.cpu().numpy()\n",
        "\n",
        "    layer_counts = np.bincount(halting_np, minlength=num_layers+1)[1:] # count nodes halting at each layer\n",
        "\n",
        "    plt.bar(range(1, num_layers+1), layer_counts, alpha=0.7, width=0.7, color='skyblue', edgecolor='navy')\n",
        "\n",
        "    for i, count in enumerate(layer_counts):\n",
        "        plt.text(i+1, count + max(layer_counts)*0.01, str(count), ha='center')\n",
        "\n",
        "    plt.title(f'Distribution of Halting Layers - {dataset_name}')\n",
        "    plt.xlabel('Layer')\n",
        "    plt.ylabel('Number of Nodes')\n",
        "    plt.xticks(range(1, num_layers+1))\n",
        "    plt.grid(True, axis='y', alpha=0.3)\n",
        "    plt.figtext(0.5, 0.01,\n",
        "               f'Mean Layer: {halting_np.mean():.2f} | Std Dev: {halting_np.std():.2f} | Min: {halting_np.min()} | Max: {halting_np.max()}',\n",
        "               ha='center', bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
        "    plt.savefig(f'{save_dir}/layer_distribution_{dataset_name}.png', dpi=300)\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "7L7VmMmB2qxJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "yK-9384Q2weZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_pondergat(dataset, device, model_params=None, train_params=None, save_dir='./results'):\n",
        "    \"\"\"\n",
        "    Main function to train PonderGAT with node-level Gumbel-Sigmoid halting.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    data = dataset[0].to(device)\n",
        "\n",
        "    if model_params is None:\n",
        "        model_params = {\n",
        "            'hidden_features': 64,\n",
        "            'num_layers': 5,\n",
        "            'heads': 8,\n",
        "            'dropout': 0.4,\n",
        "            'halting_hidden': 64,\n",
        "            'beta_kl': 0.005,\n",
        "            'beta_attn': 0.001,\n",
        "            'prior_lambda': 0.05,\n",
        "            'use_mlp_input': True,\n",
        "            'shared_gat': False,\n",
        "            'gumbel_temperature': 1.0,\n",
        "            'use_residual': False,\n",
        "            'residual_alpha': 0.1\n",
        "        }\n",
        "\n",
        "    if train_params is None:\n",
        "        train_params = {\n",
        "            'lr': 0.001,\n",
        "            'weight_decay': 5e-5,\n",
        "            'epochs': 1000,\n",
        "            'patience': 500,\n",
        "            'anneal_rate': 0.003\n",
        "        }\n",
        "\n",
        "    model = PonderGAT(\n",
        "        in_features=dataset.num_node_features,\n",
        "        hidden_features=model_params['hidden_features'],\n",
        "        out_features=dataset.num_classes,\n",
        "        num_layers=model_params['num_layers'],\n",
        "        heads=model_params['heads'],\n",
        "        dropout=model_params['dropout'],\n",
        "        halting_hidden=model_params['halting_hidden'],\n",
        "        beta_kl=model_params['beta_kl'],\n",
        "        beta_attn=model_params['beta_attn'],\n",
        "        prior_lambda=model_params['prior_lambda'],\n",
        "        use_mlp_input=model_params['use_mlp_input'],\n",
        "        shared_gat=model_params['shared_gat'],\n",
        "        gumbel_temperature=model_params['gumbel_temperature'],\n",
        "        use_residual=model_params['use_residual'],\n",
        "        residual_alpha=model_params['residual_alpha']\n",
        "    ).to(device)\n",
        "\n",
        "    print(model)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total_params}\")\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=train_params['lr'],\n",
        "        weight_decay=train_params['weight_decay']\n",
        "    )\n",
        "\n",
        "    # scheduler with warmup and decay\n",
        "    def lr_lambda(epoch):\n",
        "        if epoch < 20:\n",
        "            return epoch / 20.0\n",
        "        else:\n",
        "            return max(0.1, 1.0 * (0.98 ** (epoch - 20)))  # exp decay\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "    train_stats = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_acc': [], 'val_acc': [],\n",
        "        'train_layers': [], 'val_layers': [],\n",
        "        'task_loss': [], 'kl_loss': [],\n",
        "        'attn_loss': []\n",
        "    }\n",
        "\n",
        "\n",
        "    best_val_acc = 0\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "    best_model_path = f'{save_dir}/best_model_{dataset.name}.pt'\n",
        "\n",
        "    # Training loop:\n",
        "    start_time = time.time()\n",
        "    print(f\"Starting training on {dataset.name} dataset\")\n",
        "\n",
        "    for epoch in range(1, train_params['epochs'] + 1):\n",
        "        # anneal Gumbel-Sigmoid temperature\n",
        "        model.gumbel_temperature = max(\n",
        "            0.1,  # min temperature\n",
        "            model_params['gumbel_temperature'] * np.exp(-train_params['anneal_rate'] * epoch)\n",
        "        )\n",
        "        #train\n",
        "        train_loss, loss_dict, train_acc, train_layers = train_epoch(model, data, optimizer, device)\n",
        "\n",
        "        #eval\n",
        "        val_acc, val_layers = evaluate(model, data, 'val', device)\n",
        "\n",
        "        #update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        train_stats['train_loss'].append(train_loss)\n",
        "        train_stats['val_loss'].append(loss_dict['total_loss'])\n",
        "        train_stats['train_acc'].append(train_acc)\n",
        "        train_stats['val_acc'].append(val_acc)\n",
        "        train_stats['train_layers'].append(train_layers)\n",
        "        train_stats['val_layers'].append(val_layers)\n",
        "        train_stats['task_loss'].append(loss_dict['task_loss'])\n",
        "        train_stats['kl_loss'].append(loss_dict['kl_loss'])\n",
        "        train_stats['attn_loss'].append(loss_dict['attn_loss'])\n",
        "\n",
        "        # for early stopping improvement\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_epoch = epoch\n",
        "            patience_counter = 0\n",
        "\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # start counting patience after 50 epochs to allow exploration\n",
        "        if epoch < 50:\n",
        "            patience_counter = 0\n",
        "\n",
        "        if patience_counter >= train_params['patience']:\n",
        "            print(f\"Early stopping after {epoch} epochs\")\n",
        "            break\n",
        "\n",
        "        # progress\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            print(f\"Epoch: {epoch:03d}, Loss: {train_loss:.4f}, \"\n",
        "                  f\"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, \"\n",
        "                  f\"Train Layers: {train_layers:.2f}, Val Layers: {val_layers:.2f}, \"\n",
        "                  f\"Temp: {model.gumbel_temperature:.4f}\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"Total training time: {total_time:.2f} seconds\")\n",
        "\n",
        "    # final evaluation\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "\n",
        "    train_acc, train_layers = evaluate(model, data, 'train', device)\n",
        "    val_acc, val_layers = evaluate(model, data, 'val', device)\n",
        "    test_acc, test_layers = evaluate(model, data, 'test', device)\n",
        "\n",
        "    print(f\"\\nFinal Results - {dataset.name}:\")\n",
        "    print(f\"  Best epoch: {best_epoch}\")\n",
        "    print(f\"  Train accuracy: {train_acc:.4f}, layers: {train_layers:.2f}\")\n",
        "    print(f\"  Val accuracy: {val_acc:.4f}, layers: {val_layers:.2f}\")\n",
        "    print(f\"  Test accuracy: {test_acc:.4f}, layers: {test_layers:.2f}\")\n",
        "\n",
        "    # halting layer distribution for visualization\n",
        "    with torch.no_grad():\n",
        "        _, _, halting_layers = model.inference(data.x.to(device), data.edge_index.to(device))\n",
        "\n",
        "    visualize_training_curves(train_stats, dataset.name, save_dir)\n",
        "    visualize_layer_distribution(halting_layers, model.num_layers, dataset.name, save_dir)\n",
        "\n",
        "    # results\n",
        "    results = {\n",
        "        'model_params': model_params,\n",
        "        'train_params': train_params,\n",
        "        'final_results': {\n",
        "            'train_acc': train_acc,\n",
        "            'val_acc': val_acc,\n",
        "            'test_acc': test_acc,\n",
        "            'train_layers': train_layers,\n",
        "            'val_layers': val_layers,\n",
        "            'test_layers': test_layers,\n",
        "            'best_epoch': best_epoch,\n",
        "            'training_time': total_time\n",
        "        },\n",
        "        'train_stats': {k: v for k, v in train_stats.items()}\n",
        "    }\n",
        "\n",
        "    # Save results\n",
        "    with open(f'{save_dir}/results_{dataset.name}.json', 'w') as f:\n",
        "        # Convert numpy values to python types for JSON serialization\n",
        "        results_json = {}\n",
        "        for k, v in results.items():\n",
        "            if isinstance(v, dict):\n",
        "                results_json[k] = {}\n",
        "                for kk, vv in v.items():\n",
        "                    if isinstance(vv, dict):\n",
        "                        results_json[k][kk] = {}\n",
        "                        for kkk, vvv in vv.items():\n",
        "                            if isinstance(vvv, (np.int64, np.int32, np.float64, np.float32)):\n",
        "                                results_json[k][kk][kkk] = vvv.item() if hasattr(vvv, 'item') else vvv\n",
        "                            else:\n",
        "                                results_json[k][kk][kkk] = vvv\n",
        "                    elif isinstance(vv, (np.int64, np.int32, np.float64, np.float32)):\n",
        "                        results_json[k][kk] = vv.item() if hasattr(vv, 'item') else vv\n",
        "                    elif isinstance(vv, list) and len(vv) > 0 and isinstance(vv[0], (np.int64, np.int32, np.float64, np.float32)):\n",
        "                        results_json[k][kk] = [x.item() if hasattr(x, 'item') else x for x in vv]\n",
        "                    else:\n",
        "                        results_json[k][kk] = vv\n",
        "            else:\n",
        "                results_json[k] = v\n",
        "\n",
        "        json.dump(results_json, f, indent=2)\n",
        "\n",
        "    return model, results\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run PonderGAT on citation datasets.\n",
        "    \"\"\"\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load dataset\n",
        "    dataset_name = \"Cora\"  # Options: \"Cora\", \"CiteSeer\", \"PubMed\"\n",
        "    dataset = Planetoid(root=f'./data/{dataset_name}', name=dataset_name, transform=NormalizeFeatures())\n",
        "\n",
        "    print(f\"Dataset: {dataset_name}\")\n",
        "    print(f\"Number of nodes: {dataset[0].num_nodes}\")\n",
        "    print(f\"Number of edges: {dataset[0].num_edges}\")\n",
        "    print(f\"Number of features: {dataset.num_node_features}\")\n",
        "    print(f\"Number of classes: {dataset.num_classes}\")\n",
        "\n",
        "    # Train model\n",
        "    model, results = train_pondergat(dataset, device, save_dir=f'./results/{dataset_name}')\n",
        "\n",
        "    return model, results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSG39OEb2vpH",
        "outputId": "a62a5b3e-1306-4fc0-a76b-a882bfc161dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: Cora\n",
            "Number of nodes: 2708\n",
            "Number of edges: 10556\n",
            "Number of features: 1433\n",
            "Number of classes: 7\n",
            "PonderGAT(\n",
            "  (input_mlp): Sequential(\n",
            "    (0): Linear(in_features=1433, out_features=1024, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.4, inplace=False)\n",
            "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  )\n",
            "  (gat_layers): ModuleList(\n",
            "    (0-4): 5 x GATConv(512, 64, heads=8)\n",
            "  )\n",
            "  (halting_networks): ModuleList(\n",
            "    (0-4): 5 x Sequential(\n",
            "      (0): Linear(in_features=513, out_features=64, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_networks): ModuleList(\n",
            "    (0-4): 5 x Sequential(\n",
            "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.4, inplace=False)\n",
            "      (3): Linear(in_features=512, out_features=7, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.4, inplace=False)\n",
            ")\n",
            "Total parameters: 4807656\n",
            "Starting training on Cora dataset\n",
            "Epoch: 001, Loss: 2.0059, Train Acc: 0.1571, Val Acc: 0.1560, Train Layers: 3.30, Val Layers: 3.47, Temp: 0.9970\n",
            "Epoch: 010, Loss: 2.0365, Train Acc: 0.1643, Val Acc: 0.2120, Train Layers: 2.66, Val Layers: 2.74, Temp: 0.9704\n",
            "Epoch: 020, Loss: 1.8466, Train Acc: 0.2857, Val Acc: 0.2920, Train Layers: 2.36, Val Layers: 2.65, Temp: 0.9418\n",
            "Epoch: 030, Loss: 1.6694, Train Acc: 0.3500, Val Acc: 0.3480, Train Layers: 2.66, Val Layers: 2.92, Temp: 0.9139\n",
            "Epoch: 040, Loss: 1.0840, Train Acc: 0.6643, Val Acc: 0.5920, Train Layers: 2.74, Val Layers: 2.90, Temp: 0.8869\n",
            "Epoch: 050, Loss: 0.5684, Train Acc: 0.8714, Val Acc: 0.7300, Train Layers: 2.44, Val Layers: 2.70, Temp: 0.8607\n",
            "Epoch: 060, Loss: 0.6400, Train Acc: 0.8071, Val Acc: 0.7180, Train Layers: 3.07, Val Layers: 2.94, Temp: 0.8353\n",
            "Epoch: 070, Loss: 0.4262, Train Acc: 0.8786, Val Acc: 0.7580, Train Layers: 2.81, Val Layers: 2.95, Temp: 0.8106\n",
            "Epoch: 080, Loss: 0.1812, Train Acc: 0.9643, Val Acc: 0.7640, Train Layers: 2.49, Val Layers: 2.78, Temp: 0.7866\n",
            "Epoch: 090, Loss: 0.2194, Train Acc: 0.9571, Val Acc: 0.7580, Train Layers: 2.79, Val Layers: 2.89, Temp: 0.7634\n",
            "Epoch: 100, Loss: 0.1757, Train Acc: 0.9714, Val Acc: 0.7720, Train Layers: 2.79, Val Layers: 2.77, Temp: 0.7408\n",
            "Epoch: 110, Loss: 0.1165, Train Acc: 0.9714, Val Acc: 0.7620, Train Layers: 2.74, Val Layers: 2.80, Temp: 0.7189\n",
            "Epoch: 120, Loss: 0.1709, Train Acc: 0.9786, Val Acc: 0.7660, Train Layers: 2.87, Val Layers: 2.78, Temp: 0.6977\n",
            "Epoch: 130, Loss: 0.0812, Train Acc: 0.9786, Val Acc: 0.7920, Train Layers: 2.44, Val Layers: 2.87, Temp: 0.6771\n",
            "Epoch: 140, Loss: 0.1190, Train Acc: 0.9714, Val Acc: 0.7940, Train Layers: 2.87, Val Layers: 2.79, Temp: 0.6570\n",
            "Epoch: 150, Loss: 0.1150, Train Acc: 0.9786, Val Acc: 0.7900, Train Layers: 2.66, Val Layers: 2.82, Temp: 0.6376\n",
            "Epoch: 160, Loss: 0.1158, Train Acc: 0.9929, Val Acc: 0.7700, Train Layers: 2.92, Val Layers: 2.90, Temp: 0.6188\n",
            "Epoch: 170, Loss: 0.0583, Train Acc: 1.0000, Val Acc: 0.7620, Train Layers: 2.62, Val Layers: 2.79, Temp: 0.6005\n",
            "Epoch: 180, Loss: 0.0588, Train Acc: 1.0000, Val Acc: 0.7640, Train Layers: 2.66, Val Layers: 2.87, Temp: 0.5827\n",
            "Epoch: 190, Loss: 0.0823, Train Acc: 0.9857, Val Acc: 0.7800, Train Layers: 2.76, Val Layers: 2.84, Temp: 0.5655\n",
            "Epoch: 200, Loss: 0.0656, Train Acc: 0.9857, Val Acc: 0.7600, Train Layers: 2.61, Val Layers: 2.81, Temp: 0.5488\n",
            "Epoch: 210, Loss: 0.0588, Train Acc: 1.0000, Val Acc: 0.7760, Train Layers: 2.86, Val Layers: 2.82, Temp: 0.5326\n",
            "Epoch: 220, Loss: 0.0554, Train Acc: 0.9857, Val Acc: 0.7740, Train Layers: 2.59, Val Layers: 2.80, Temp: 0.5169\n",
            "Epoch: 230, Loss: 0.0335, Train Acc: 1.0000, Val Acc: 0.7580, Train Layers: 2.75, Val Layers: 2.91, Temp: 0.5016\n",
            "Epoch: 240, Loss: 0.0475, Train Acc: 0.9929, Val Acc: 0.7460, Train Layers: 2.73, Val Layers: 2.85, Temp: 0.4868\n",
            "Epoch: 250, Loss: 0.0477, Train Acc: 1.0000, Val Acc: 0.7540, Train Layers: 2.94, Val Layers: 2.91, Temp: 0.4724\n",
            "Epoch: 260, Loss: 0.0443, Train Acc: 1.0000, Val Acc: 0.7720, Train Layers: 2.76, Val Layers: 2.81, Temp: 0.4584\n",
            "Epoch: 270, Loss: 0.0296, Train Acc: 0.9929, Val Acc: 0.7700, Train Layers: 2.61, Val Layers: 2.81, Temp: 0.4449\n",
            "Epoch: 280, Loss: 0.0245, Train Acc: 1.0000, Val Acc: 0.7480, Train Layers: 2.74, Val Layers: 3.03, Temp: 0.4317\n",
            "Epoch: 290, Loss: 0.0358, Train Acc: 1.0000, Val Acc: 0.7660, Train Layers: 2.55, Val Layers: 2.76, Temp: 0.4190\n",
            "Epoch: 300, Loss: 0.0531, Train Acc: 0.9857, Val Acc: 0.7640, Train Layers: 2.72, Val Layers: 2.82, Temp: 0.4066\n",
            "Epoch: 310, Loss: 0.0253, Train Acc: 1.0000, Val Acc: 0.7620, Train Layers: 2.59, Val Layers: 2.71, Temp: 0.3946\n",
            "Epoch: 320, Loss: 0.0443, Train Acc: 0.9929, Val Acc: 0.7640, Train Layers: 2.80, Val Layers: 2.79, Temp: 0.3829\n",
            "Epoch: 330, Loss: 0.0356, Train Acc: 0.9857, Val Acc: 0.7660, Train Layers: 2.63, Val Layers: 2.81, Temp: 0.3716\n",
            "Epoch: 340, Loss: 0.0392, Train Acc: 0.9929, Val Acc: 0.7680, Train Layers: 2.74, Val Layers: 2.80, Temp: 0.3606\n",
            "Epoch: 350, Loss: 0.0165, Train Acc: 1.0000, Val Acc: 0.7500, Train Layers: 2.66, Val Layers: 2.79, Temp: 0.3499\n",
            "Epoch: 360, Loss: 0.0370, Train Acc: 0.9929, Val Acc: 0.7620, Train Layers: 2.76, Val Layers: 2.89, Temp: 0.3396\n",
            "Epoch: 370, Loss: 0.0169, Train Acc: 1.0000, Val Acc: 0.7680, Train Layers: 2.62, Val Layers: 2.94, Temp: 0.3296\n",
            "Epoch: 380, Loss: 0.0176, Train Acc: 1.0000, Val Acc: 0.7640, Train Layers: 2.89, Val Layers: 2.79, Temp: 0.3198\n",
            "Epoch: 390, Loss: 0.0325, Train Acc: 0.9929, Val Acc: 0.7400, Train Layers: 2.61, Val Layers: 2.80, Temp: 0.3104\n",
            "Epoch: 400, Loss: 0.0373, Train Acc: 0.9929, Val Acc: 0.7400, Train Layers: 2.74, Val Layers: 2.83, Temp: 0.3012\n",
            "Epoch: 410, Loss: 0.0303, Train Acc: 0.9929, Val Acc: 0.7460, Train Layers: 2.73, Val Layers: 2.95, Temp: 0.2923\n",
            "Epoch: 420, Loss: 0.0154, Train Acc: 1.0000, Val Acc: 0.7380, Train Layers: 2.77, Val Layers: 2.93, Temp: 0.2837\n",
            "Epoch: 430, Loss: 0.0174, Train Acc: 1.0000, Val Acc: 0.7320, Train Layers: 2.84, Val Layers: 2.81, Temp: 0.2753\n",
            "Epoch: 440, Loss: 0.0121, Train Acc: 1.0000, Val Acc: 0.7640, Train Layers: 2.66, Val Layers: 2.79, Temp: 0.2671\n",
            "Epoch: 450, Loss: 0.0252, Train Acc: 0.9929, Val Acc: 0.7560, Train Layers: 2.84, Val Layers: 2.84, Temp: 0.2592\n",
            "Epoch: 460, Loss: 0.0261, Train Acc: 1.0000, Val Acc: 0.7600, Train Layers: 2.76, Val Layers: 2.71, Temp: 0.2516\n",
            "Epoch: 470, Loss: 0.0091, Train Acc: 1.0000, Val Acc: 0.7540, Train Layers: 2.58, Val Layers: 2.71, Temp: 0.2441\n",
            "Epoch: 480, Loss: 0.0144, Train Acc: 1.0000, Val Acc: 0.7540, Train Layers: 2.54, Val Layers: 2.67, Temp: 0.2369\n",
            "Epoch: 490, Loss: 0.0324, Train Acc: 1.0000, Val Acc: 0.7460, Train Layers: 2.71, Val Layers: 2.76, Temp: 0.2299\n",
            "Epoch: 500, Loss: 0.0075, Train Acc: 1.0000, Val Acc: 0.7620, Train Layers: 2.56, Val Layers: 2.79, Temp: 0.2231\n",
            "Epoch: 510, Loss: 0.0245, Train Acc: 0.9929, Val Acc: 0.7540, Train Layers: 2.81, Val Layers: 2.84, Temp: 0.2165\n",
            "Epoch: 520, Loss: 0.0113, Train Acc: 1.0000, Val Acc: 0.7500, Train Layers: 2.69, Val Layers: 2.72, Temp: 0.2101\n",
            "Epoch: 530, Loss: 0.0312, Train Acc: 0.9929, Val Acc: 0.7480, Train Layers: 2.79, Val Layers: 2.74, Temp: 0.2039\n",
            "Epoch: 540, Loss: 0.0153, Train Acc: 1.0000, Val Acc: 0.7460, Train Layers: 2.61, Val Layers: 2.89, Temp: 0.1979\n",
            "Epoch: 550, Loss: 0.0089, Train Acc: 1.0000, Val Acc: 0.7420, Train Layers: 2.80, Val Layers: 2.86, Temp: 0.1920\n",
            "Epoch: 560, Loss: 0.0080, Train Acc: 1.0000, Val Acc: 0.7500, Train Layers: 2.52, Val Layers: 2.79, Temp: 0.1864\n",
            "Epoch: 570, Loss: 0.0068, Train Acc: 1.0000, Val Acc: 0.7460, Train Layers: 2.56, Val Layers: 2.86, Temp: 0.1809\n",
            "Epoch: 580, Loss: 0.0133, Train Acc: 1.0000, Val Acc: 0.7480, Train Layers: 2.84, Val Layers: 2.79, Temp: 0.1755\n",
            "Epoch: 590, Loss: 0.0100, Train Acc: 1.0000, Val Acc: 0.7420, Train Layers: 2.71, Val Layers: 2.82, Temp: 0.1703\n",
            "Epoch: 600, Loss: 0.0439, Train Acc: 0.9929, Val Acc: 0.7600, Train Layers: 2.48, Val Layers: 2.84, Temp: 0.1653\n",
            "Epoch: 610, Loss: 0.0194, Train Acc: 0.9857, Val Acc: 0.7460, Train Layers: 2.61, Val Layers: 2.86, Temp: 0.1604\n",
            "Epoch: 620, Loss: 0.0137, Train Acc: 1.0000, Val Acc: 0.7600, Train Layers: 2.56, Val Layers: 2.77, Temp: 0.1557\n",
            "Epoch: 630, Loss: 0.0087, Train Acc: 1.0000, Val Acc: 0.7580, Train Layers: 2.61, Val Layers: 2.77, Temp: 0.1511\n",
            "Early stopping after 631 epochs\n",
            "Total training time: 60.18 seconds\n",
            "\n",
            "Final Results - Cora:\n",
            "  Best epoch: 131\n",
            "  Train accuracy: 0.9929, layers: 3.06\n",
            "  Val accuracy: 0.7780, layers: 2.82\n",
            "  Test accuracy: 0.8010, layers: 2.78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-7475534f58fb>:145: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tjY1d5Q_21bs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}