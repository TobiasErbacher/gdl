@inproceedings{gilmer2017neural,
  title={Neural message passing for quantum chemistry},
  author={Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E},
  booktitle={International conference on machine learning},
  pages={1263--1272},
  year={2017},
  organization={PMLR}
}

@ARTICLE{spinelli2021,
  author={Spinelli, Indro and Scardapane, Simone and Uncini, Aurelio},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Adaptive Propagation Graph Convolutional Network}, 
  year={2021},
  volume={32},
  number={10},
  pages={4755-4760},
  keywords={Laplace equations;Convolutional codes;Protocols;Neural networks;Learning systems;Adaptive systems;Adaptation models;Convolutional network;graph data;graph neural network (GNN);node classification},
  doi={10.1109/TNNLS.2020.3025110}
}


@InProceedings{xu2018,
  title = 	 {Representation Learning on Graphs with Jumping Knowledge Networks},
  author =       {Xu, Keyulu and Li, Chengtao and Tian, Yonglong and Sonobe, Tomohiro and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5453--5462},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/xu18c/xu18c.pdf},
  url = 	 {https://proceedings.mlr.press/v80/xu18c.html},
  abstract = 	 {Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of "neighboring" nodes that a node’s representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture – jumping knowledge (JK) networks – that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models’ performance.}
}

@article{liu2019,
	abstractnote = {&lt;p&gt;We present, GeniePath, a scalable approach for learning adaptive receptive fields of neural networks defined on permutation invariant graph data. In GeniePath, we propose an adaptive path layer consists of two complementary functions designed for breadth and depth exploration respectively, where the former learns the importance of different sized neighborhoods, while the latter extracts and filters signals aggregated from neighbors of different hops away. Our method works in both transductive and inductive settings, and extensive experiments compared with competitive methods show that our approaches yield state-of-the-art results on large graphs.&lt;/p&gt;},
	author = {Liu, Ziqi and Chen, Chaochao and Li, Longfei and Zhou, Jun and Li, Xiaolong and Song, Le and Qi, Yuan},
	doi = {10.1609/aaai.v33i01.33014424},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	month = {Jul.},
	number = {01},
	pages = {4424-4431},
	title = {GeniePath: Graph Neural Networks with Adaptive Receptive Paths},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4354},
	volume = {33},
	year = {2019},
	bdsk-url-1 = {https://ojs.aaai.org/index.php/AAAI/article/view/4354},
	bdsk-url-2 = {https://doi.org/10.1609/aaai.v33i01.33014424}}


@inproceedings{ma2021,
  author = {Ma, Xiaojun and Wang, Junshan and Chen, Hanyue and Song, Guojie},
  title = {Improving Graph Neural Networks with Structural Adaptive Receptive Fields},
  year = {2021},
  isbn = {9781450383127},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3442381.3449896},
  doi = {10.1145/3442381.3449896},
  abstract = {The abundant information in graphs helps us to learn more expressive node representations. Different nodes in the neighborhood have different importance to the central node. Thus, average weight aggregation in most Graph Neural Networks would fail to model such difference. GAT-based models introduce the attention mechanism to solve this problem, but they ignore the rich structural information and may suffer from the problem of over-smoothing. In this paper, we propose Graph Neural Networks with STructural Adaptive Receptive fields (STAR-GNN), which adaptively construct a receptive field for each node with structural information and further achieve better aggregation of information. Firstly, we model local structural distribution based on anonymous random walks, followed by using the structural information to construct receptive fields guided with mutual information. Then, as the generated receptive fields are irregular, we design a sub-graph aggregator to boost node representations and theoretically prove that it has the ability to capture the complex structures in receptive fields. Experimental results demonstrate the power of STAR-GNN in learning structural receptive fields adaptively and encoding more informative structural characteristics in real-world networks.},
  booktitle = {Proceedings of the Web Conference 2021},
  pages = {2438-2447},
  numpages = {10},
  keywords = {graph neural networks, mutual information, receptive fields},
  location = {Ljubljana, Slovenia},
  series = {WWW '21}
}

@misc{banino2021,
      title={PonderNet: Learning to Ponder}, 
      author={Andrea Banino and Jan Balaguer and Charles Blundell},
      year={2021},
      eprint={2107.05407},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.05407}, 
      note={8th ICML Workshop on Automated Machine Learning (AutoML 2021)}
}

@inproceedings{lai2020,
  author = {Lai, Kwei-Herng and Zha, Daochen and Zhou, Kaixiong and Hu, Xia},
  title = {Policy-GNN: Aggregation Optimization for Graph Neural Networks},
  year = {2020},
  isbn = {9781450379984},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3394486.3403088},
  doi = {10.1145/3394486.3403088},
  abstract = {Graph data are pervasive in many real-world applications. Recently, increasing attention has been paid on graph neural networks (GNNs), which aim to model the local graph structures and capture the hierarchical patterns by aggregating the information from neighbors with stackable network modules. Motivated by the observation that different nodes often require different iterations of aggregation to fully capture the structural information, in this paper, we propose to explicitly sample diverse iterations of aggregation for different nodes to boost the performance of GNNs. It is a challenging task to develop an effective aggregation strategy for each node, given complex graphs and sparse features. Moreover, it is not straightforward to derive an efficient algorithm since we need to feed the sampled nodes into different number of network layers. To address the above challenges, we propose Policy-GNN, a meta-policy framework that models the sampling procedure and message passing of GNNs into a combined learning process. Specifically, Policy-GNN uses a meta-policy to adaptively determine the number of aggregations for each node. The meta-policy is trained with deep reinforcement learning~(RL) by exploiting the feedback from the model. We further introduce parameter sharing and a buffer mechanism to boost the training efficiency. Experimental results on three real-world benchmark datasets suggest that Policy-GNN significantly outperforms the state-of-the-art alternatives, showing the promise in aggregation optimization for GNNs.},
  booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages = {461–471},
  numpages = {11},
  keywords = {node classification, meta-policy learning, graph neural networks, deep reinforcement learning, Markov decision process},
  location = {Virtual Event, CA, USA},
  series = {KDD '20}
}

@inproceedings{finkelshtein2024,
  author = {Finkelshtein, Ben and Huang, Xingyue and Bronstein, Michael and Ceylan, İsmail İlkan},
  title = {Cooperative graph neural networks},
  year = {2024},
  publisher = {JMLR.org},
  abstract = {Graph neural networks are popular architectures for graph machine learning, based on iterative computation of node representations of an input graph through a series of invariant transformations. A large class of graph neural networks follow a standard message-passing paradigm: at every layer, each node state is updated based on an aggregate of messages from its neighborhood. In this work, we propose a novel framework for training graph neural networks, where every node is viewed as a player that can choose to either 'listen', 'broadcast', 'listen and broadcast', or to 'isolate'. The standard message propagation scheme can then be viewed as a special case of this framework where every node 'listens and broadcasts' to all neighbors. Our approach offers a more flexible and dynamic message-passing paradigm, where each node can determine its own strategy based on their state, effectively exploring the graph topology while learning. We provide a theoretical analysis of the new message-passing scheme which is further supported by an extensive empirical analysis on synthetic and real-world data.},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  articleno = {546},
  numpages = {27},
  location = {Vienna, Austria},
  series = {ICML'24}
}

@inproceedings{xiao2021,
  author = {Xiao, Teng and Chen, Zhengyu and Wang, Donglin and Wang, Suhang},
  title = {Learning How to Propagate Messages in Graph Neural Networks},
  year = {2021},
  isbn = {9781450383325},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3447548.3467451},
  doi = {10.1145/3447548.3467451},
  abstract = {This paper studies the problem of learning message propagation strategies for graph neural networks (GNNs). One of the challenges for graph neural networks is that of defining the propagation strategy. For instance, the choices of propagation steps are often specialized to a single graph and are not personalized to different nodes. To compensate for this, in this paper, we present learning to propagate, a general learning framework that not only learns the GNN parameters for prediction but more importantly, can explicitly learn the interpretable and personalized propagate strategies for different nodes and various types of graphs. We introduce the optimal propagation steps as latent variables to help find the maximum-likelihood estimation of the GNN parameters in a variational Expectation- Maximization (VEM) framework. Extensive experiments on various types of graph benchmarks demonstrate that our proposed frame- work can significantly achieve better performance compared with the state-of-the-art methods, and can effectively learn personalized and interpretable propagate strategies of messages in GNNs.},
  booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages = {1894–1903},
  numpages = {10},
  keywords = {graph neural networks, graph representation learning},
  location = {Virtual Event, Singapore},
  series = {KDD '21}
}

@inproceedings{kipf2017,
  author = {Kipf, Thomas N. and Welling, Max},
  added-at = {2020-07-15T00:50:01.000+0200},
  booktitle = {Proceedings of the 5th International Conference on Learning Representations},
  interhash = {54b65044b71f10c31476ed76422ab85d},
  intrahash = {71ee5be8cafc25d7a3869bcb49fc5c3c},
  keywords = {},
  location = {Palais des Congr{\`e}s Neptune, Toulon, France},
  series = {ICLR '17},
  timestamp = {2020-07-15T00:50:01.000+0200},
  title = {{Semi-Supervised Classification with Graph Convolutional Networks}},
  url = {https://openreview.net/forum?id=SJU4ayYgl},
  venue = {ICLR},
  year = 2017
}


@article{sen2008,
	abstract = {Many real-world applications produce networked data such as the worldwide web (hypertext documents connected through hyperlinks), social networks (such as people connected by friendship links), communication networks (computers connected through communication links), and biological networks (such as protein interaction networks). A recent focus in machine-learning research has been to extend traditional machine-learning classification techniques to classify nodes in such networks. In this article, we provide a brief introduction to this area of research and how it has progressed during the past decade. We introduce four of the most widely used inference algorithms for classifying networked data and empirically compare them on both synthetic and real-world data.},
	author = {Sen, Prithviraj and Namata, Galileo and Bilgic, Mustafa and Getoor, Lise and Gallagher, Brian and Eliassi-Rad, Tina},
	doi = {https://doi.org/10.1609/aimag.v29i3.2157},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1609/aimag.v29i3.2157},
	journal = {AI Magazine},
	number = {3},
	pages = {93-106},
	title = {Collective Classification in Network Data},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1609/aimag.v29i3.2157},
	volume = {29},
	year = {2008},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1609/aimag.v29i3.2157},
	bdsk-url-2 = {https://doi.org/10.1609/aimag.v29i3.2157}
}


@article{mccallum2000,
	abstract = {Domain-specific internet portals are growing in popularity because they gather content from the Web and organize it for easy access, retrieval and search. For example, www.campsearch.com allows complex queries by age, location, cost and specialty over summer camps. This functionality is not possible with general, Web-wide search engines. Unfortunately these portals are difficult and time-consuming to maintain. This paper advocates the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific Internet portals. We describe new research in reinforcement learning, information extraction and text classification that enables efficient spidering, the identification of informative text segments, and the population of topic hierarchies. Using these techniques, we have built a demonstration system: a portal for computer science research papers. It already contains over 50,000 papers and is publicly available at www.cora.justresearch.com. These techniques are widely applicable to portal creation in other domains.},
	author = {McCallum, Andrew Kachites and Nigam, Kamal and Rennie, Jason and Seymore, Kristie},
	date = {2000/07/01},
	date-added = {2025-04-06 19:27:41 +0200},
	date-modified = {2025-04-06 19:27:41 +0200},
	doi = {10.1023/A:1009953814988},
	id = {McCallum2000},
	isbn = {1573-7659},
	journal = {Information Retrieval},
	number = {2},
	pages = {127--163},
	title = {Automating the Construction of Internet Portals with Machine Learning},
	url = {https://doi.org/10.1023/A:1009953814988},
	volume = {3},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1023/A:1009953814988}
}

@inproceedings{shchur2018,
  title     = {Pitfalls of Graph Neural Network Evaluation},
  author    = {Shchur, Oleksandr and Mumme, Maximilian and Bojchevski, Aleksandar and G\"unnemann, Stephan},
  booktitle = {Relational Representation Learning Workshop (R2L) at NeurIPS},
  year      = {2018}
}

@inproceedings{mcauley2015,
  author = {McAuley, Julian and Targett, Christopher and Shi, Qinfeng and van den Hengel, Anton},
  title = {Image-Based Recommendations on Styles and Substitutes},
  year = {2015},
  isbn = {9781450336215},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2766462.2767755},
  doi = {10.1145/2766462.2767755},
  abstract = {Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on fine-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem defined on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.},
  booktitle = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages = {43-52},
  numpages = {10},
  keywords = {metric learning, recommender systems, visual features},
  location = {Santiago, Chile},
  series = {SIGIR '15}
}

@inproceedings{namata2012,
  title={Query-driven Active Surveying for Collective Classification},
  author={Namata, Galileo and London, Ben and Getoor, Lise and Huang, Bert},
  booktitle={International Workshop on Mining and Learning with Graphs (MLG)},
  year={2012}
}

@misc{graves2017,
      title={Adaptive Computation Time for Recurrent Neural Networks}, 
      author={Alex Graves},
      year={2017},
      eprint={1603.08983},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1603.08983}, 
}

@inproceedings{scardapane2020,
  author={Scardapane, Simone and Comminiello, Danilo and Scarpiniti, Michele and Baccarelli, Enzo and Uncini, Aurelio},
  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Differentiable Branching In Deep Networks for Fast Inference}, 
  year={2020},
  volume={},
  number={},
  pages={4167-4171},
  keywords={Neural networks;Signal processing algorithms;Benchmark testing;Computational efficiency;Task analysis;Speech processing;Image classification;Deep network;energy efficiency;multi-branch architectures;inference time},
  doi={10.1109/ICASSP40776.2020.9054209}
}


@inproceedings{Maddison2017,
	title = {The concrete distribution: A continuous relaxation of discrete random variables},
	author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
	booktitle = {Proceedings of the international conference on learning Representations},
	publisher = {International Conference on Learning Representations},
	year = {2017}
}

@inproceedings{Jang2017,
  author	= {Eric Jang and Shixiang Gu and Ben Poole},
  title	= {Categorical Reparameterization with Gumbel-Softmax},
  year	= {2017},
  booktitle = {Proceedings of the international conference on learning Representations},
	publisher = {International Conference on Learning Representations},
}

@inproceedings{Klicpera2019,
      title={Predict then Propagate: Graph Neural Networks meet Personalized PageRank}, 
      author={Johannes Klicpera and Aleksandar Bojchevski and Stephan Günnemann},
      year={2019},
      booktitle = {Proceedings of the international conference on learning Representations},
	    publisher = {International Conference on Learning Representations},
}

@inproceedings{veličković2018graphattentionnetworks,
  title={Graph Attention Networks},
  author={Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio},
  booktitle={Proceedings of the international conference on learning Representations},
  publisher = {International Conference on Learning Representations},
  year={2018},
  url={https://openreview.net/forum?id=rJXMpikCZ},
}

@inproceedings{xu2019powerfulgraphneuralnetworks,
  title={How Powerful are Graph Neural Networks?},
  author={Keyulu Xu and Weihua Hu and Jure Leskovec and Stefanie Jegelka},
  booktitle={Proceedings of the international conference on learning Representations},
  publisher = {International Conference on Learning Representations},
  year={2019},
  url={https://openreview.net/forum?id=ryGs6iA5Km},
}
