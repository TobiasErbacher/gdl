{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import yaml\n",
    "import torch\n",
    "import logging\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim import Adam, Optimizer\n",
    "from collections import defaultdict\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "from replication.model_and_layer import APGCN\n",
    "from seeds import test_seeds, gen_seeds, quick_seeds\n",
    "from data import get_dataset, set_train_val_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open('results/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('results/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def summary(results):\n",
    "    report={}\n",
    "    for k, v in results.items():\n",
    "        if k != 'steps' and k != 'probs':\n",
    "            boots_series = sns.algorithms.bootstrap(results[k], func=np.mean, n_boot=1000)\n",
    "            report[k] = np.mean(results[k])\n",
    "            report[f'{k}_ci'] = np.max(np.abs(sns.utils.ci(boots_series, 95) - report[k]))\n",
    "        else:\n",
    "            array = np.array([k.mean().cpu().detach().numpy() for k in results['steps']])\n",
    "            boots_series = sns.algorithms.bootstrap(array, func=np.mean, n_boot=1000)\n",
    "            report[k] = np.mean(array)\n",
    "            report[f'{k}_ci'] = np.max(np.abs(sns.utils.ci(boots_series, 95) - report[k]))\n",
    "    return report\n",
    "\n",
    "def plot_density(results):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    z =[(x.cpu().numpy()).astype(int) for x in results['steps']]\n",
    "    z = np.vstack(z)\n",
    "    z = np.mean(z,axis=0)\n",
    "\n",
    "    sns.distplot(z, hist = False, kde = True,\n",
    "                 kde_kws = {'shade': True, 'linewidth': 3}, \n",
    "                 ax=ax)\n",
    "    plt.xlabel('Number of Steps')\n",
    "    plt.ylabel('Density')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module, optimizer: Optimizer, data: Data, train_halt, weight_decay: float):\n",
    "    model.train()\n",
    "    \n",
    "    for param in model.prop.parameters():\n",
    "        param.requires_grad = train_halt\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits, steps, reminders = model(data)\n",
    "    \n",
    "    loss = F.nll_loss(logits[data.train_mask], data.y[data.train_mask])\n",
    "    l2_reg = sum((torch.sum(param ** 2) for param in model.reg_params))\n",
    "    loss += weight_decay/2 * l2_reg + model.prop_penalty *(\n",
    "            steps[data.train_mask] + reminders[data.train_mask]).mean()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return\n",
    "\n",
    "def evaluate(model: torch.nn.Module, data: Data, test: bool, weight_decay: float):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits, steps, reminders = model(data)\n",
    "        \n",
    "        loss = F.nll_loss(logits[data.train_mask], data.y[data.train_mask])\n",
    "        l2_reg = sum((torch.sum(param ** 2) for param in model.reg_params))\n",
    "        loss += weight_decay/2 * l2_reg + model.prop_penalty *(\n",
    "                steps[data.train_mask] + reminders[data.train_mask]).mean()\n",
    "\n",
    "    eval_dict = {}\n",
    "    keys = ['train','val']\n",
    "    eval_dict['steps'] = steps\n",
    "    for key in keys:\n",
    "        mask = data[f'{key}_mask']\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        eval_dict[f'{key}_acc'] = acc\n",
    "    return eval_dict, loss\n",
    "\n",
    "\n",
    "def test_acc(model: torch.nn.Module, data: Data):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits, steps, reminders = model(data)\n",
    "    mask = data['test_mask']\n",
    "    pred = logits[mask].max(1)[1]\n",
    "    acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(dataset: InMemoryDataset,\n",
    "        model: torch.nn.Module,\n",
    "        seeds: np.ndarray,\n",
    "        test: bool = False,\n",
    "        max_epochs: int = 10000,\n",
    "        patience: int = 100,\n",
    "        lr: float = 0.01,\n",
    "        weight_decay: float = 0.01,\n",
    "        num_development: int = 1500,\n",
    "        device: str = 'cuda'):\n",
    "    \n",
    "\n",
    "    best_dict = defaultdict(list)\n",
    "\n",
    "    for seed in tqdm(seeds):\n",
    "        for _ in range(config['niter_per_seed']):\n",
    "            torch_seed = gen_seeds()\n",
    "            torch.manual_seed(seed=torch_seed)\n",
    "            \n",
    "            dataset.data = set_train_val_test_split(\n",
    "                seed,\n",
    "                dataset.data,\n",
    "                num_development=num_development,\n",
    "                num_per_class=20\n",
    "                ).to(device)\n",
    "            \n",
    "            model.to(device).reset_parameters()\n",
    "            optimizer = Adam(model.parameters(),lr=lr)\n",
    "            \n",
    "            patience_counter = 0\n",
    "            best_loss = 999\n",
    "            tmp_dict = {'val_acc': 0}\n",
    "            \n",
    "            start_time = time.perf_counter()\n",
    "            for epoch in range(1, max_epochs + 1):\n",
    "                if patience_counter == patience:\n",
    "                    break\n",
    "\n",
    "                train(model, optimizer, dataset.data, epoch%5==0, weight_decay)\n",
    "                eval_dict, loss = evaluate(model, dataset.data, test, weight_decay)\n",
    "                                \n",
    "                if(eval_dict['val_acc'] > tmp_dict['val_acc']) or (\n",
    "                  (eval_dict['val_acc'] == tmp_dict['val_acc']) and loss < best_loss):\n",
    "                    patience_counter = 0\n",
    "                    tmp_dict['epoch'] = epoch\n",
    "                    tmp_dict['runtime'] = time.perf_counter() - start_time\n",
    "                    \n",
    "                    for k, v in eval_dict.items():\n",
    "                        tmp_dict[k] = v\n",
    "\n",
    "                    best_state = {key: value.cpu() for key, value\n",
    "                                      in model.state_dict().items()}\n",
    "                \n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    patience_counter = 0\n",
    "                            \n",
    "            model.load_state_dict(best_state)\n",
    "            tmp_dict['test_acc'] = test_acc(model,dataset.data)\n",
    "            print(\"Epoch: {:.1f}\"\" Train: {:.2f}\"\" Val: {:.2f}\"\" Test: {:.2f}\".format(\n",
    "                  tmp_dict['epoch'],\n",
    "                  tmp_dict['train_acc'] * 100,\n",
    "                  tmp_dict['val_acc'] * 100,\n",
    "                  tmp_dict['test_acc'] * 100))\n",
    "                \n",
    "            for k, v in tmp_dict.items():\n",
    "                best_dict[k].append(v)\n",
    "\n",
    "    return dict(best_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "#Datasets: 'citeseer', 'cora_ml' 'pubmed' 'ms_academic', 'amazon_electronics_computers', 'amazon_electronics_photo'\n",
    "#Num Developent: 1500,1500,1500,5000,1500,1500\n",
    "# weight_decay 0 for Amazon Datasets 8e-03 for the others\n",
    "config = {'dataset_name': 'A.Computers',\n",
    "          'test': True,\n",
    "          'use_lcc': True,\n",
    "          'num_development': 1500,\n",
    "          'niter_per_seed': 5,\n",
    "          'hidden_units': 64,\n",
    "          'lr': 0.01,\n",
    "          'dropout': 0.5,\n",
    "          'weight_decay': 0\n",
    "         }\n",
    "    \n",
    "dataset = get_dataset(\n",
    "    name=config['dataset_name'],\n",
    "    use_lcc=config['use_lcc']\n",
    "    )\n",
    "\n",
    "dataset.data = dataset.data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3ddfbecfc1465f863851066c9f92cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m APGCN(dataset,\u001b[38;5;241m10\u001b[39m, prop_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m run(\n\u001b[1;32m      4\u001b[0m     dataset,\n\u001b[1;32m      5\u001b[0m     model,\n\u001b[1;32m      6\u001b[0m     seeds\u001b[38;5;241m=\u001b[39mtest_seeds \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m val_seeds,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#seeds= quick_seeds,\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     lr\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      9\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     10\u001b[0m     test\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     11\u001b[0m     num_development\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_development\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     12\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#save_obj(results,'results_' + config['dataset_name'])\u001b[39;00m\n\u001b[1;32m     16\u001b[0m report \u001b[38;5;241m=\u001b[39m summary(results)\n",
      "Cell \u001b[0;32mIn[4], line 39\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(dataset, model, seeds, test, max_epochs, patience, lr, weight_decay, num_development, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m patience_counter \u001b[38;5;241m==\u001b[39m patience:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m train(model, optimizer, dataset\u001b[38;5;241m.\u001b[39mdata, epoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m, weight_decay)\n\u001b[1;32m     40\u001b[0m eval_dict, loss \u001b[38;5;241m=\u001b[39m evaluate(model, dataset\u001b[38;5;241m.\u001b[39mdata, test, weight_decay)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(eval_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m tmp_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     43\u001b[0m   (eval_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m tmp_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m loss \u001b[38;5;241m<\u001b[39m best_loss):\n",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, data, train_halt, weight_decay)\u001b[0m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(logits[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\n\u001b[1;32m     11\u001b[0m l2_reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m((torch\u001b[38;5;241m.\u001b[39msum(param \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mreg_params))\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weight_decay\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m l2_reg \u001b[38;5;241m+\u001b[39m model\u001b[38;5;241m.\u001b[39mprop_penalty \u001b[38;5;241m*\u001b[39m(\n\u001b[0;32m---> 13\u001b[0m         steps[data\u001b[38;5;241m.\u001b[39mtrain_mask] \u001b[38;5;241m+\u001b[39m reminders[data\u001b[38;5;241m.\u001b[39mtrain_mask])\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.12/site-packages/torch_geometric/data/data.py:554\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    552\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value)\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_store\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object was created by an older version of PyG. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf this error occurred while loading an already existing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset, remove the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory in the dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot folder and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = APGCN(dataset,10, prop_penalty=0.05)\n",
    "\n",
    "results = run(\n",
    "    dataset,\n",
    "    model,\n",
    "    seeds=test_seeds if config['test'] else val_seeds,\n",
    "    #seeds= quick_seeds,\n",
    "    lr=config['lr'],\n",
    "    weight_decay=config['weight_decay'],\n",
    "    test=config['test'],\n",
    "    num_development=config['num_development'],\n",
    "    device=device\n",
    "    )\n",
    "\n",
    "#save_obj(results,'results_' + config['dataset_name'])\n",
    "report = summary(results)\n",
    "\n",
    "print(\"FINAL\\n\"\n",
    "      \"Train Accuracy: {:.2f} ± {:.2f}%\\n\"\n",
    "      \"Stopping Accuracy: {:.2f} ± {:.2f}%\\n\"\n",
    "      \"Test     Accuracy: {:.2f} ± {:.2f}%\\n\"\n",
    "      \"Steps: {:.2f} ± {:.2f}\\n\" \n",
    "      \"Epochs:  {:.2f} ± {:.2f}\\n\"\n",
    "      \"Runtime: {:.4f} ± {:.4f}\\n\"\n",
    "      .format(\n",
    "          report['train_acc'] * 100,\n",
    "          report['train_acc_ci'] * 100,\n",
    "          report['val_acc'] * 100,\n",
    "          report['val_acc_ci'] * 100,\n",
    "          report['test_acc']*100,\n",
    "          report['test_acc_ci']*100,\n",
    "          report['steps'],\n",
    "          report['steps_ci'],\n",
    "          report['epoch'],\n",
    "          report['epoch_ci'],\n",
    "          report['runtime'],\n",
    "          report['runtime_ci']))\n",
    "\n",
    "plot_density(results)\n",
    "\n",
    "del model, dataset\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
