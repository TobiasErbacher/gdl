{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cooperative Graph Neural Networks ([CoGNN](https://doi.org/10.48550/arXiv.2310.01267))\n",
    "\n",
    "This part was adapted by Tobias Erbacher from the [authors' github](https://github.com/benfinkelshtein/CoGNN/tree/main). We recommend to run this on a GPU service like [Google Colab](https://colab.research.google.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "To ensure we are using the same modules as the authors, we need to install the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==2.0.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
    "%pip install torch-geometric==2.3.0\n",
    "%pip install torchmetrics ogb rdkit\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute this notebook, we will assume that the datasets are already installed. If you cloned our github repository, you will be able to find them in [this folder](https://github.com/TobiasErbacher/gdl/tree/main/replication/data). In particular, we are using:\n",
    "\n",
    "- Computers <font color='red'>(Link?)</font>\n",
    "- Photo <font color='red'>(Link?)</font>\n",
    "- [CiteSeer](https://linqs.org/datasets/) <font color='red'>(Which one?)</font>\n",
    "- CoraML <font color='red'>(Link?)</font>\n",
    "- MS-Academic <font color='red'>(Link?)</font>\n",
    "- PubMed <font color='red'>(Link?)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first check whether a GPU is available and if so then set it as the default device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "assert GPU_AVAILABLE\n",
    "device = torch.device(\"cuda:0\")\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import random\n",
    "import torch\n",
    "from torch_geometric.typing import OptTensor\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "from enum import Enum, auto\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from typing import NamedTuple, Tuple, Any, Callable\n",
    "from ogb.utils.features import get_atom_feature_dims, get_bond_feature_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import the custom classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.class_encoder_laplace import LapPENodeEncoder, LAP_DIM_PE\n",
    "from helper.class_encoder_kernel import RWSENodeEncoder, KER_DIM_PE\n",
    "from helper.class_encoder import PosEncoder\n",
    "from helper.class_metric import MetricType\n",
    "from helper.class_concat2node import Concat2NodeEncoder\n",
    "from helper.class_actionnetargs import ActionNetArgs\n",
    "from helper.class_activationtype import ActivationType\n",
    "from helper.class_datasetbysplit import DatasetBySplit\n",
    "from helper.class_metric_lam import LossesAndMetrics\n",
    "from helper.class_cognn import CoGNN\n",
    "from helper.class_dataset import DataSet\n",
    "from helper.class_model import ModelType\n",
    "from helper.class_pool import Pool\n",
    "from helper.class_encoder import PosEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us set up an argument parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--dataset\", dest=\"dataset\", default=DataSet.roman_empire, type=DataSet.from_string,\n",
    "                        choices=list(DataSet), required=False)\n",
    "    parser.add_argument(\"--pool\", dest=\"pool\", default=Pool.NONE, type=Pool.from_string,\n",
    "                        choices=list(Pool), required=False)\n",
    "\n",
    "    # gumbel\n",
    "    parser.add_argument(\"--learn_temp\", dest=\"learn_temp\", default=False, action='store_true', required=False)\n",
    "    parser.add_argument(\"--temp_model_type\", dest=\"temp_model_type\", default=ModelType.LIN,\n",
    "                        type=ModelType.from_string, choices=list(ModelType), required=False)\n",
    "    parser.add_argument(\"--tau0\", dest=\"tau0\", default=0.5, type=float, required=False)\n",
    "    parser.add_argument(\"--temp\", dest=\"temp\", default=0.01, type=float, required=False)\n",
    "\n",
    "    # optimization\n",
    "    parser.add_argument(\"--max_epochs\", dest=\"max_epochs\", default=3000, type=int, required=False)\n",
    "    parser.add_argument(\"--batch_size\", dest=\"batch_size\", default=32, type=int, required=False)\n",
    "    parser.add_argument(\"--lr\", dest=\"lr\", default=1e-3, type=float, required=False)\n",
    "    parser.add_argument(\"--dropout\", dest=\"dropout\", default=0.2, type=float, required=False)\n",
    "\n",
    "    # env cls parameters\n",
    "    parser.add_argument(\"--env_model_type\", dest=\"env_model_type\", default=ModelType.MEAN_GNN,\n",
    "                        type=ModelType.from_string, choices=list(ModelType), required=False)\n",
    "    parser.add_argument(\"--env_num_layers\", dest=\"env_num_layers\", default=3, type=int, required=False)\n",
    "    parser.add_argument(\"--env_dim\", dest=\"env_dim\", default=128, type=int, required=False)\n",
    "    parser.add_argument(\"--skip\", dest=\"skip\", default=False, action='store_true', required=False)\n",
    "    parser.add_argument(\"--batch_norm\", dest=\"batch_norm\", default=False, action='store_true', required=False)\n",
    "    parser.add_argument(\"--layer_norm\", dest=\"layer_norm\", default=False, action='store_true', required=False)\n",
    "    parser.add_argument(\"--dec_num_layers\", dest=\"dec_num_layers\", default=1, type=int, required=False)\n",
    "    parser.add_argument(\"--pos_enc\", dest=\"pos_enc\", default=PosEncoder.NONE,\n",
    "                        type=PosEncoder.from_string, choices=list(PosEncoder), required=False)\n",
    "\n",
    "    # policy cls parameters\n",
    "    parser.add_argument(\"--act_model_type\", dest=\"act_model_type\", default=ModelType.MEAN_GNN,\n",
    "                        type=ModelType.from_string, choices=list(ModelType), required=False)\n",
    "    parser.add_argument(\"--act_num_layers\", dest=\"act_num_layers\", default=1, type=int, required=False)\n",
    "    parser.add_argument(\"--act_dim\", dest=\"act_dim\", default=16, type=int, required=False)\n",
    "\n",
    "    # reproduce\n",
    "    parser.add_argument(\"--seed\", dest=\"seed\", type=int, default=0, required=False)\n",
    "    \n",
    "\n",
    "    # dataset dependant parameters \n",
    "    parser.add_argument(\"--fold\", dest=\"fold\", default=None, type=int, required=False)\n",
    "\n",
    "    # optimizer and scheduler\n",
    "    parser.add_argument(\"--weight_decay\", dest=\"weight_decay\", default=0, type=float, required=False)\n",
    "    ## for steplr scheduler only\n",
    "    parser.add_argument(\"--step_size\", dest=\"step_size\", default=None, type=int, required=False)\n",
    "    parser.add_argument(\"--gamma\", dest=\"gamma\", default=None, type=float, required=False)\n",
    "    ## for cosine with warmup scheduler only\n",
    "    parser.add_argument(\"--num_warmup_epochs\", dest=\"num_warmup_epochs\", default=None, type=int, required=False)\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "args = parse_arguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to later on be able to set a seed for the experiments, we will define this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the $\\text{\\textcolor{red}{checmical dataset}}$ we will need an atom and bond encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super(AtomEncoder, self).__init__()\n",
    "\n",
    "        self.atom_embedding_list = torch.nn.ModuleList()\n",
    "\n",
    "        for _, dim in enumerate(get_atom_feature_dims()):\n",
    "            emb = torch.nn.Embedding(dim, emb_dim)\n",
    "            torch.nn.init.xavier_uniform_(emb.weight.data)\n",
    "            self.atom_embedding_list.append(emb)\n",
    "\n",
    "    def forward(self, x, pestat):\n",
    "        x_embedding = 0\n",
    "        for i in range(x.shape[1]):\n",
    "            x_embedding += self.atom_embedding_list[i](x[:, i])\n",
    "\n",
    "        return x_embedding\n",
    "\n",
    "class BondEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super(BondEncoder, self).__init__()\n",
    "\n",
    "        self.bond_embedding_list = torch.nn.ModuleList()\n",
    "\n",
    "        for _, dim in enumerate(get_bond_feature_dims()):\n",
    "            emb = torch.nn.Embedding(dim, emb_dim)\n",
    "            torch.nn.init.xavier_uniform_(emb.weight.data)\n",
    "            self.bond_embedding_list.append(emb)\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        bond_embedding = 0\n",
    "        for i in range(edge_attr.shape[1]):\n",
    "            bond_embedding += self.bond_embedding_list[i](edge_attr[:, i])\n",
    "\n",
    "        return bond_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define some encoder models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLinear(torch.nnLinear):\n",
    "    def forward(self, x: torch.Tensor, pestat=None) -> torch.Tensor:\n",
    "        return super().forward(x)\n",
    "\n",
    "class DataSetEncoders(Enum):\n",
    "    \"\"\"\n",
    "        an object for the different encoders\n",
    "    \"\"\"\n",
    "    NONE = auto()\n",
    "    MOL = auto()\n",
    "\n",
    "    @staticmethod\n",
    "    def from_string(s: str):\n",
    "        try:\n",
    "            return DataSetEncoders[s]\n",
    "        except KeyError:\n",
    "            raise ValueError()\n",
    "\n",
    "    def node_encoder(self, in_dim: int, emb_dim: int):\n",
    "        if self is DataSetEncoders.NONE:\n",
    "            return EncoderLinear(in_features=in_dim, out_features=emb_dim)\n",
    "        elif self is DataSetEncoders.MOL:\n",
    "            return AtomEncoder(emb_dim)\n",
    "        else:\n",
    "            raise ValueError(f'DataSetEncoders {self.name} not supported')\n",
    "\n",
    "    def edge_encoder(self, emb_dim: int, model_type):\n",
    "        if self is DataSetEncoders.NONE:\n",
    "            return None\n",
    "        elif self is DataSetEncoders.MOL:\n",
    "            if model_type.is_gcn():\n",
    "                return None\n",
    "            else:\n",
    "                return BondEncoder(emb_dim)\n",
    "        else:\n",
    "            raise ValueError(f'DataSetEncoders {self.name} not supported')\n",
    "\n",
    "    def use_encoders(self) -> bool:\n",
    "        return self is not DataSetEncoders.NONE\n",
    "\n",
    "class PosEncoder(Enum):\n",
    "    \"\"\"\n",
    "        an object for the different encoders\n",
    "    \"\"\"\n",
    "    NONE = auto()\n",
    "    LAP = auto()\n",
    "    RWSE = auto()\n",
    "\n",
    "    @staticmethod\n",
    "    def from_string(s: str):\n",
    "        try:\n",
    "            return PosEncoder[s]\n",
    "        except KeyError:\n",
    "            raise ValueError()\n",
    "\n",
    "    def get(self, in_dim: int, emb_dim: int, expand_x: bool):\n",
    "        if self is PosEncoder.NONE:\n",
    "            return None\n",
    "        elif self is PosEncoder.LAP:\n",
    "            return LapPENodeEncoder(dim_in=in_dim, dim_emb=emb_dim, expand_x=expand_x)\n",
    "        elif self is PosEncoder.RWSE:\n",
    "            return RWSENodeEncoder(dim_in=in_dim, dim_emb=emb_dim, expand_x=expand_x)\n",
    "        else:\n",
    "            raise ValueError(f'DataSetEncoders {self.name} not supported')\n",
    "\n",
    "    def DIM_PE(self):\n",
    "        if self is PosEncoder.NONE:\n",
    "            return None\n",
    "        elif self is PosEncoder.LAP:\n",
    "            return LAP_DIM_PE\n",
    "        elif self is PosEncoder.RWSE:\n",
    "            return KER_DIM_PE\n",
    "        else:\n",
    "            raise ValueError(f'DataSetEncoders {self.name} not supported')\n",
    "\n",
    "    def get_pe(self, data: Data, device):\n",
    "        if self is PosEncoder.NONE:\n",
    "            return None\n",
    "        elif self is PosEncoder.LAP:\n",
    "            return [data.EigVals.to(device), data.EigVecs.to(device)]\n",
    "        elif self is PosEncoder.RWSE:\n",
    "            return data.pestat_RWSE.to(device)\n",
    "        else:\n",
    "            raise ValueError(f'DataSetEncoders {self.name} not supported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the $\\textcolor{red}{???}$, we will need some parameters to which end we define a class container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelArgs(NamedTuple):\n",
    "    learn_temp: bool\n",
    "    temp_model_type: ModelType\n",
    "    tau0: float\n",
    "    temp: float\n",
    "    gin_mlp_func: Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we will define a container class for the arguments required to set up the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvArgs(NamedTuple):\n",
    "    model_type: ModelType\n",
    "    num_layers: int\n",
    "    env_dim: int\n",
    "\n",
    "    layer_norm: bool\n",
    "    skip: bool\n",
    "    batch_norm: bool\n",
    "    dropout: float\n",
    "    act_type: ActivationType\n",
    "    dec_num_layers: int\n",
    "    pos_enc: PosEncoder\n",
    "    dataset_encoders: DataSetEncoders\n",
    "\n",
    "    metric_type: MetricType\n",
    "    in_dim: int\n",
    "    out_dim: int\n",
    "\n",
    "    gin_mlp_func: Callable\n",
    "\n",
    "    def load_net(self) -> torch.nn.ModuleList:\n",
    "        if self.pos_enc is PosEncoder.NONE:\n",
    "            enc_list = [self.dataset_encoders.node_encoder(in_dim=self.in_dim, emb_dim=self.env_dim)]\n",
    "        else:\n",
    "            if self.dataset_encoders is DataSetEncoders.NONE:\n",
    "                enc_list = [self.pos_enc.get(in_dim=self.in_dim, emb_dim=self.env_dim)]\n",
    "            else:\n",
    "                enc_list = [Concat2NodeEncoder(enc1_cls=self.dataset_encoders.node_encoder,\n",
    "                                               enc2_cls=self.pos_enc.get,\n",
    "                                               in_dim=self.in_dim, emb_dim=self.env_dim,\n",
    "                                               enc2_dim_pe=self.pos_enc.DIM_PE())]\n",
    "\n",
    "        component_list =\\\n",
    "            self.model_type.get_component_list(in_dim=self.env_dim, hidden_dim=self.env_dim,  out_dim=self.env_dim,\n",
    "                                               num_layers=self.num_layers, bias=True, edges_required=True,\n",
    "                                               gin_mlp_func=self.gin_mlp_func)\n",
    "\n",
    "        if self.dec_num_layers > 1:\n",
    "            mlp_list = (self.dec_num_layers - 1) * [torch.nn.Linear(self.env_dim, self.env_dim),\n",
    "                                                    torch.nn.Dropout(self.dropout), self.act_type.nn()]\n",
    "            mlp_list = mlp_list + [torch.nn.Linear(self.env_dim, self.out_dim)]\n",
    "            dec_list = [torch.nn.Sequential(*mlp_list)]\n",
    "        else:\n",
    "            dec_list = [torch.nn.Linear(self.env_dim, self.out_dim)]\n",
    "\n",
    "        return torch.nn.ModuleList(enc_list + component_list + dec_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set up the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(object):\n",
    "    def __init__(self, args: Namespace):\n",
    "        super().__init__()\n",
    "        for arg in vars(args):\n",
    "            value_arg = getattr(args, arg)\n",
    "            print(f\"{arg}: {value_arg}\")\n",
    "            self.__setattr__(arg, value_arg)\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        set_seed(seed=self.seed)\n",
    "\n",
    "        # parameters\n",
    "        self.metric_type = self.dataset.get_metric_type()\n",
    "        self.decimal = self.dataset.num_after_decimal()\n",
    "        self.task_loss = self.metric_type.get_task_loss()\n",
    "\n",
    "        # asserts\n",
    "        self.dataset.asserts(args)\n",
    "\n",
    "    def run(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        dataset = self.dataset.load(seed=self.seed, pos_enc=self.pos_enc)\n",
    "        if self.metric_type.is_multilabel():\n",
    "            dataset.data.y = dataset.data.y.to(dtype=torch.float)\n",
    "\n",
    "        folds = self.dataset.get_folds(fold=self.fold)\n",
    "\n",
    "        # locally used parameters\n",
    "        out_dim = self.metric_type.get_out_dim(dataset=dataset)\n",
    "        gin_mlp_func = self.dataset.gin_mlp_func()\n",
    "        env_act_type = self.dataset.env_activation_type()\n",
    "\n",
    "        # named tuples\n",
    "        gumbel_args = GumbelArgs(learn_temp=self.learn_temp, temp_model_type=self.temp_model_type, tau0=self.tau0,\n",
    "                                 temp=self.temp, gin_mlp_func=gin_mlp_func)\n",
    "        env_args = \\\n",
    "            EnvArgs(model_type=self.env_model_type, num_layers=self.env_num_layers, env_dim=self.env_dim,\n",
    "                    layer_norm=self.layer_norm, skip=self.skip, batch_norm=self.batch_norm, dropout=self.dropout,\n",
    "                    act_type=env_act_type, metric_type=self.metric_type, in_dim=dataset[0].x.shape[1], out_dim=out_dim,\n",
    "                    gin_mlp_func=gin_mlp_func, dec_num_layers=self.dec_num_layers, pos_enc=self.pos_enc,\n",
    "                    dataset_encoders=self.dataset.get_dataset_encoders())\n",
    "        action_args = \\\n",
    "            ActionNetArgs(model_type=self.act_model_type, num_layers=self.act_num_layers,\n",
    "                          hidden_dim=self.act_dim, dropout=self.dropout, act_type=ActivationType.RELU,\n",
    "                          env_dim=self.env_dim, gin_mlp_func=gin_mlp_func)\n",
    "\n",
    "        # folds\n",
    "        metrics_list = []\n",
    "        edge_ratios_list = []\n",
    "        for num_fold in folds:\n",
    "            set_seed(seed=self.seed)\n",
    "            dataset_by_split = self.dataset.select_fold_and_split(num_fold=num_fold, dataset=dataset)\n",
    "            best_losses_n_metrics, edge_ratios =\\\n",
    "                self.single_fold(dataset_by_split=dataset_by_split, gumbel_args=gumbel_args, env_args=env_args,\n",
    "                                 action_args=action_args, num_fold=num_fold)\n",
    "\n",
    "            # print final\n",
    "            print_str = f'Fold {num_fold}/{len(folds)}'\n",
    "            for name in best_losses_n_metrics._fields:\n",
    "                print_str += f\",{name}={round(getattr(best_losses_n_metrics, name), self.decimal)}\"\n",
    "            print(print_str)\n",
    "            print()\n",
    "            metrics_list.append(best_losses_n_metrics.get_fold_metrics())\n",
    "\n",
    "            if edge_ratios is not None:\n",
    "                edge_ratios_list.append(edge_ratios)\n",
    "\n",
    "        metrics_matrix = torch.stack(metrics_list, dim=0)  # (F, 3)\n",
    "        metrics_mean = torch.mean(metrics_matrix, dim=0).tolist()  # (3,)\n",
    "        if len(edge_ratios_list) > 0:\n",
    "            edge_ratios = torch.mean(torch.stack(edge_ratios_list, dim=0), dim=0)\n",
    "        else:\n",
    "            edge_ratios = None\n",
    "\n",
    "        # prints\n",
    "        print(f'Final Rewired train={round(metrics_mean[0], self.decimal)},'\n",
    "              f'val={round(metrics_mean[1], self.decimal)},'\n",
    "              f'test={round(metrics_mean[2], self.decimal)}')\n",
    "        if len(folds) > 1:\n",
    "            metrics_std = torch.std(metrics_matrix, dim=0).tolist()  # (3,)\n",
    "            print(f'Final Rewired train={round(metrics_mean[0], self.decimal)}+-{round(metrics_std[0], self.decimal)},'\n",
    "                  f'val={round(metrics_mean[1], self.decimal)}+-{round(metrics_std[1], self.decimal)},'\n",
    "                  f'test={round(metrics_mean[2], self.decimal)}+-{round(metrics_std[2], self.decimal)}')\n",
    "    \n",
    "        return metrics_mean, edge_ratios\n",
    "            \n",
    "    def single_fold(self, dataset_by_split: DatasetBySplit, gumbel_args: GumbelArgs, env_args: EnvArgs,\n",
    "                    action_args: ActionNetArgs, num_fold: int) -> Tuple[LossesAndMetrics, OptTensor]:\n",
    "        model = CoGNN(gumbel_args=gumbel_args, env_args=env_args, action_args=action_args,\n",
    "                      pool=self.pool).to(device=self.device)\n",
    "\n",
    "        optimizer = self.dataset.optimizer(model=model, lr=self.lr, weight_decay=self.weight_decay)\n",
    "        scheduler = self.dataset.scheduler(optimizer=optimizer, step_size=self.step_size, gamma=self.gamma,\n",
    "                                           num_warmup_epochs=self.num_warmup_epochs, max_epochs=self.max_epochs)\n",
    "\n",
    "        with tqdm.tqdm(total=self.max_epochs, file=sys.stdout) as pbar:\n",
    "            best_losses_n_metrics, edge_ratios =\\\n",
    "                self.train_and_test(dataset_by_split=dataset_by_split, model=model, optimizer=optimizer,\n",
    "                                    scheduler=scheduler, pbar=pbar, num_fold=num_fold)\n",
    "        return best_losses_n_metrics, edge_ratios\n",
    "\n",
    "    def train_and_test(self, dataset_by_split: DatasetBySplit, model, optimizer, scheduler, pbar, num_fold: int)\\\n",
    "            -> Tuple[LossesAndMetrics, OptTensor]:\n",
    "        train_loader = DataLoader(dataset_by_split.train, batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(dataset_by_split.val, batch_size=self.batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(dataset_by_split.test, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        best_losses_n_metrics = self.metric_type.get_worst_losses_n_metrics()\n",
    "        for epoch in range(self.max_epochs):\n",
    "            self.train(train_loader=train_loader, model=model, optimizer=optimizer)\n",
    "            train_loss, train_metric, _ =\\\n",
    "                self.test(loader=train_loader, model=model, split_mask_name='train_mask', calc_edge_ratio=False)\n",
    "            if self.dataset.is_expressivity():\n",
    "                val_loss, val_metric = train_loss, train_metric\n",
    "                test_loss, test_metric = train_loss, train_metric\n",
    "            else:\n",
    "                val_loss, val_metric, _ =\\\n",
    "                    self.test(loader=val_loader, model=model, split_mask_name='val_mask', calc_edge_ratio=False)\n",
    "                test_loss, test_metric, _ =\\\n",
    "                    self.test(loader=test_loader, model=model, split_mask_name='test_mask', calc_edge_ratio=False)\n",
    "\n",
    "            losses_n_metrics = \\\n",
    "                LossesAndMetrics(train_loss=train_loss, val_loss=val_loss, test_loss=test_loss,\n",
    "                                 train_metric=train_metric, val_metric=val_metric, test_metric=test_metric)\n",
    "            if scheduler is not None:\n",
    "                scheduler.step(losses_n_metrics.val_metric)\n",
    "\n",
    "            # best metrics\n",
    "            if self.metric_type.src_better_than_other(src=losses_n_metrics.val_metric,\n",
    "                                                      other=best_losses_n_metrics.val_metric):\n",
    "                best_losses_n_metrics = losses_n_metrics\n",
    "\n",
    "            # prints\n",
    "            log_str = f'Split: {num_fold}, epoch: {epoch}'\n",
    "            for name in losses_n_metrics._fields:\n",
    "                log_str += f\",{name}={round(getattr(losses_n_metrics, name), self.decimal)}\"\n",
    "            log_str += f\"({round(best_losses_n_metrics.test_metric, self.decimal)})\"\n",
    "            pbar.set_description(log_str)\n",
    "            pbar.update(n=1)\n",
    "\n",
    "        edge_ratios = None\n",
    "        if self.dataset.not_synthetic():\n",
    "            _, _, edge_ratios =\\\n",
    "                self.test(loader=test_loader, model=model, split_mask_name='test_mask', calc_edge_ratio=True)\n",
    "\n",
    "        return best_losses_n_metrics, edge_ratios\n",
    "\n",
    "    def train(self, train_loader, model, optimizer):\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:\n",
    "            if self.batch_norm and (data.x.shape[0] == 1 or data.num_graphs == 1):\n",
    "                continue\n",
    "            optimizer.zero_grad()\n",
    "            node_mask = self.dataset.get_split_mask(data=data, batch_size=data.num_graphs,\n",
    "                                                    split_mask_name='train_mask').to(self.device)\n",
    "            edge_attr = data.edge_attr\n",
    "            if data.edge_attr is not None:\n",
    "                edge_attr = edge_attr.to(device=self.device)\n",
    "\n",
    "            # forward\n",
    "            scores, _ =\\\n",
    "                model(data.x.to(device=self.device), edge_index=data.edge_index.to(device=self.device),\n",
    "                      batch=data.batch.to(device=self.device), edge_attr=edge_attr, edge_ratio_node_mask=None,\n",
    "                      pestat=self.pos_enc.get_pe(data=data, device=self.device))\n",
    "            train_loss = self.task_loss(scores[node_mask], data.y.to(device=self.device)[node_mask])\n",
    "\n",
    "            # backward\n",
    "            train_loss.backward()\n",
    "            if self.dataset.clip_grad():\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "\n",
    "    def test(self, loader, model, split_mask_name: str, calc_edge_ratio: bool)\\\n",
    "            -> Tuple[float, Any, torch.Tensor]:\n",
    "        model.eval()\n",
    "\n",
    "        total_loss, total_metric, total_edge_ratios = 0, 0, 0\n",
    "        total_scores = np.empty(shape=(0, model.env_args.out_dim))\n",
    "        total_y = None\n",
    "        for data in loader:\n",
    "            if self.batch_norm and (data.x.shape[0] == 1 or data.num_graphs == 1):\n",
    "                continue\n",
    "            node_mask = self.dataset.get_split_mask(data=data, batch_size=data.num_graphs,\n",
    "                                                    split_mask_name=split_mask_name).to(device=self.device)\n",
    "            if calc_edge_ratio:\n",
    "                edge_ratio_node_mask =\\\n",
    "                    self.dataset.get_edge_ratio_node_mask(data=data, split_mask_name=split_mask_name).to(device=self.device)\n",
    "            else:\n",
    "                edge_ratio_node_mask = None\n",
    "            edge_attr = data.edge_attr\n",
    "            if data.edge_attr is not None:\n",
    "                edge_attr = edge_attr.to(device=self.device)\n",
    "\n",
    "            # forward\n",
    "            scores, edge_ratios =\\\n",
    "                model(data.x.to(device=self.device), edge_index=data.edge_index.to(device=self.device),\n",
    "                      edge_attr=edge_attr, batch=data.batch.to(device=self.device),\n",
    "                      edge_ratio_node_mask=edge_ratio_node_mask,\n",
    "                      pestat=self.pos_enc.get_pe(data=data, device=self.device))\n",
    "            \n",
    "            eval_loss = self.task_loss(scores, data.y.to(device=self.device))\n",
    "\n",
    "            # analytics\n",
    "            total_scores = np.concatenate((total_scores, scores[node_mask].detach().cpu().numpy()))\n",
    "            if total_y is None:\n",
    "                total_y = data.y.to(device=self.device)[node_mask].detach().cpu().numpy()\n",
    "            else:\n",
    "                total_y = np.concatenate((total_y, data.y.to(device=self.device)[node_mask].detach().cpu().numpy()))\n",
    "\n",
    "            total_loss += eval_loss.item() * data.num_graphs\n",
    "            total_edge_ratios += edge_ratios * data.num_graphs\n",
    "\n",
    "        metric = self.metric_type.apply_metric(scores=total_scores, target=total_y)\n",
    "\n",
    "        loss = total_loss / len(loader.dataset)\n",
    "        edge_ratios = total_edge_ratios / len(loader.dataset)\n",
    "        return loss, metric, edge_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment(args=args).run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AdvancedTopicsInMachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
