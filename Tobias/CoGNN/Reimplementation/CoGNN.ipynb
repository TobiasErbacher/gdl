{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583af40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import logging\n",
    "import networkx\n",
    "import scipy.sparse\n",
    "from enum import Enum, auto\n",
    "from typing import Dict, NamedTuple, Tuple, List, Union, Optional, Callable, Any\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.cuda import set_device\n",
    "from torch_sparse import coalesce\n",
    "from torch_geometric.data import Data, Batch, InMemoryDataset, download_url\n",
    "from torch_geometric.utils import remove_self_loops, add_remaining_self_loops, to_undirected, get_laplacian, to_scipy_sparse_matrix, scatter, to_dense_adj\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.typing import OptTensor, Adj\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.pool import global_mean_pool, global_add_pool\n",
    "from torch_geometric.datasets import HeterophilousGraphDataset\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_geometric.graphgym.loader import index2mask, set_dataset_attr\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "import torch_geometric.transforms as T\n",
    "from torchmetrics import MeanAbsoluteError, Accuracy, AveragePrecision, AUROC\n",
    "\n",
    "from ogb.utils import smiles2graph\n",
    "from ogb.utils.url import decide_download\n",
    "from ogb.utils.features import get_atom_feature_dims, get_bond_feature_dims\n",
    "from ogb.utils.torch_util import replace_numpy_with_torchtensor\n",
    "\n",
    "from hashlib import md5\n",
    "from shutil import rmtree\n",
    "from functools import partial\n",
    "from os import getcwd\n",
    "from os.path import join, dirname, abspath, isdir, exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb8bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ROOT_DIR = dirname(dirname(abspath(__file__)))\n",
    "except:\n",
    "    ROOT_DIR = dirname(dirname(abspath(getcwd())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba8d995",
   "metadata": {},
   "source": [
    "To ensure reproducibility, we define the `set_seed()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47555aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc71fbf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Defining the model options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a90bb1",
   "metadata": {},
   "source": [
    "The `MolConv` class represents the base layer of the `MessagePassing` class. The default aggregation is SUM. When we call the model to compute the output, for each node it does the following:\n",
    "\n",
    "1. `message()`: Figure out who are my neighbors.\n",
    "\n",
    "2. `message()`: Add the edge attributes to the neighbors' embeddings and scale this sum by the edge weights.\n",
    "\n",
    "3. `update()`: Combine (aggregate) the resulting embeddings from (2) to produce the new node's embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457c4890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolConv(MessagePassing):\n",
    "    def __init__(self, aggr='add'):\n",
    "        super().__init__(aggr=aggr)  # 'add', 'mean' or 'max'\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_attr: OptTensor, edge_weight: OptTensor = None) -> Tensor:\n",
    "        if edge_attr is None:\n",
    "            if edge_weight is None:\n",
    "                return x_j\n",
    "            else:\n",
    "                return edge_weight.view(-1, 1) * x_j\n",
    "        else:\n",
    "            if edge_weight is None:\n",
    "                return x_j + edge_attr\n",
    "            else:\n",
    "                return edge_weight.view(-1, 1) * (x_j + edge_attr)\n",
    "\n",
    "    def update(self, aggr_out: Tensor) -> Tensor:\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e4063",
   "metadata": {},
   "source": [
    "The `WeightedGCNConv` class extends the previous base convolution layer. It requires:\n",
    "\n",
    "- The number of input channels `in_channels` (dimension of a node's feature vector),\n",
    "\n",
    "- The number of output channels `out_channels` (dimension of a node's feature vector after convolution),\n",
    "\n",
    "- Whether or not to include a `bias` term in the linear layer, and\n",
    "\n",
    "- other arguments `kwargs` passed to the `MolConv` class.\n",
    "\n",
    "The forward pass is designed to:\n",
    "\n",
    "1. Create a collection of the graph's edges without self-loops to avoid double counting of nodes' features in the normalization.\n",
    "\n",
    "2. Create a collection of the edge attributes inclduing the self-loops for the propagation step so that the node can \"keep its prior state in memory\".\n",
    "\n",
    "3. Normalize the graph's edge weights by their degree.\n",
    "\n",
    "4. Perform a propagation step and map the output using the linear layer to the desired number of output channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291ec00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedGCNConv(MolConv):\n",
    "    def __init__(self, in_channels: int, out_channels: int, bias: bool, **kwargs):\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.add_self_loops = True\n",
    "        self.improved = False\n",
    "\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels, bias=bias)\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj, edge_attr: OptTensor = None, edge_weight: OptTensor = None) -> Tensor:\n",
    "        edge_index = remove_self_loops(edge_index=edge_index)[0]\n",
    "        _, edge_attr = add_remaining_self_loops(edge_index, edge_attr, fill_value=1, num_nodes=x.shape[0])\n",
    "\n",
    "        # propagate_type: (x: Tensor, edge_weight: OptTensor)\n",
    "        edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
    "            edge_index, edge_weight, x.size(self.node_dim),\n",
    "            self.improved, self.add_self_loops, self.flow, x.dtype)\n",
    "        out = self.propagate(edge_index, x=x, edge_weight=edge_weight, size=None, edge_attr=edge_attr)\n",
    "        out = self.lin(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11495dd7",
   "metadata": {},
   "source": [
    "In the `WeightedGNNConv` class we remove the normalization step as compared to the `WeightedGCNConv` and before running the output of the propagation step through a linear layer, we first concatenate to it the original node's features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdd71ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedGNNConv(MolConv):\n",
    "    def __init__(self, in_channels: int, out_channels: int, aggr='add', bias=True):\n",
    "        super().__init__(aggr=aggr)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.lin = torch.nn.Linear(2 * in_channels, out_channels, bias=bias)\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj, edge_attr: OptTensor = None, edge_weight: OptTensor = None) -> Tensor:\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, edge_weight=edge_weight)\n",
    "        out = self.lin(torch.cat((x, out), dim=-1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412c22ea",
   "metadata": {},
   "source": [
    "A further simplifaction is implemented by `GraphLinear` which essentially simply passes the node's features to a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae6a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLinear(torch.nn.Linear):\n",
    "    def forward(self, x: Tensor, edge_index: Adj, edge_attr: OptTensor = None, edge_weight: OptTensor = None) -> Tensor:\n",
    "        return super().forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931fcc49",
   "metadata": {},
   "source": [
    "Similarly to the `WeightedGCNConv`, the `WeightedGINConv` class extends the `MolConv` base convolution layer. However, instead of the `kwargs` it requires a `mlp_func` function which should return an architecture for a Multilayer Perceptron.\n",
    "\n",
    "In the forward pass, we perform a propagation step identically to the one in the `WeightedGCNConv` class, but instead of using a linear layer to map the output to another dimensionality, we run the output (to which we add the node's features with potential skip connections) through a MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b03e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedGINConv(MolConv):\n",
    "    def __init__(self, in_channels: int, out_channels: int, bias: bool, mlp_func: Callable):\n",
    "        \"\"\"\n",
    "            emb_dim (int): node embedding dimensionality\n",
    "        \"\"\"\n",
    "        super(WeightedGINConv, self).__init__(aggr=\"add\")\n",
    "\n",
    "        self.mlp = mlp_func(in_channels=in_channels, out_channels=out_channels, bias=bias)\n",
    "        self.eps = torch.Tensor([0])\n",
    "        self.eps = torch.nn.Parameter(self.eps)\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj, edge_attr: OptTensor = None, edge_weight: OptTensor = None) -> Tensor:\n",
    "        out = self.propagate(edge_index, x=x, edge_weight=edge_weight, size=None, edge_attr=edge_attr)\n",
    "        return self.mlp((1 + self.eps.to(x.device)) * x + out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af8fa04",
   "metadata": {},
   "source": [
    "The `ModelType` class provides a container for the different types of models from before. In the `get_component_list` function we return a list of the full architecture of the model that we will use, which can later be instantiated with a call to `torch.nn.Sequential()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43b4f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelType(Enum):\n",
    "    \"\"\"\n",
    "        an object for the different core\n",
    "    \"\"\"\n",
    "    GCN = auto()\n",
    "    GIN = auto()\n",
    "    LIN = auto()\n",
    "\n",
    "    SUM_GNN = auto()\n",
    "    MEAN_GNN = auto()\n",
    "\n",
    "    @staticmethod\n",
    "    def from_string(s: str):\n",
    "        try:\n",
    "            return ModelType[s]\n",
    "        except KeyError:\n",
    "            raise ValueError()\n",
    "\n",
    "    def load_component_cls(self):\n",
    "        if self is ModelType.GCN:\n",
    "            return WeightedGCNConv\n",
    "        elif self is ModelType.GIN:\n",
    "            return WeightedGINConv\n",
    "        elif self in [ModelType.SUM_GNN, ModelType.MEAN_GNN]:\n",
    "            return WeightedGNNConv\n",
    "        elif self is ModelType.LIN:\n",
    "            return GraphLinear\n",
    "        else:\n",
    "            raise ValueError(f'model {self.name} not supported')\n",
    "\n",
    "    def is_gcn(self):\n",
    "        return self is ModelType.GCN\n",
    "\n",
    "    def get_component_list(self, in_dim: int, hidden_dim: int, out_dim: int, num_layers: int, bias: bool,\n",
    "                           edges_required: bool, gin_mlp_func: Callable) -> List[torch.nn.Module]:\n",
    "        dim_list = [in_dim] + [hidden_dim] * (num_layers - 1) + [out_dim]\n",
    "        if self is ModelType.GCN:\n",
    "            component_list = [self.load_component_cls()(in_channels=in_dim_i, out_channels=out_dim_i, bias=bias)\n",
    "                              for in_dim_i, out_dim_i in zip(dim_list[:-1], dim_list[1:])]\n",
    "        elif self is ModelType.GIN:\n",
    "            component_list = [self.load_component_cls()(in_channels=in_dim_i, out_channels=out_dim_i, bias=bias,\n",
    "                                                        mlp_func=gin_mlp_func)\n",
    "                              for in_dim_i, out_dim_i in zip(dim_list[:-1], dim_list[1:])]\n",
    "        elif self in [ModelType.SUM_GNN, ModelType.MEAN_GNN]:\n",
    "            aggr = 'mean' if self is ModelType.MEAN_GNN else 'sum'\n",
    "            component_list = [self.load_component_cls()(in_channels=in_dim_i, out_channels=out_dim_i, aggr=aggr,\n",
    "                                                        bias=bias)\n",
    "                              for in_dim_i, out_dim_i in zip(dim_list[:-1], dim_list[1:])]\n",
    "        elif self is ModelType.LIN:\n",
    "            assert not edges_required, f'env does not support {self.name}'\n",
    "            component_list = \\\n",
    "                [self.load_component_cls()(in_features=in_dim_i, out_features=out_dim_i, bias=bias)\n",
    "                 for in_dim_i, out_dim_i in zip(dim_list[:-1], dim_list[1:])]\n",
    "        else:\n",
    "            raise ValueError(f'model {self.name} not supported')\n",
    "        return component_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd62089",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Miscellaneous helper classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6261ff",
   "metadata": {},
   "source": [
    "First, we set up a helper class `BatchIdentity` that ensures differentiability of the loss in the backpropagation step when returned by the `Pool.get()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c8fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchIdentity(torch.nn.Module):\n",
    "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor, batch: Tensor) -> Tensor:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60c5973",
   "metadata": {},
   "source": [
    "The `Pool` class enables us to have the various aggregation types (the authors mistakenly wrote actviation types instead) in a container class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f69e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pool(Enum):\n",
    "    \"\"\"\n",
    "        an object for the different activation types\n",
    "    \"\"\"\n",
    "    NONE = auto()\n",
    "    MEAN = auto()\n",
    "    SUM = auto()\n",
    "\n",
    "    @staticmethod\n",
    "    def from_string(s: str):\n",
    "        try:\n",
    "            return Pool[s]\n",
    "        except KeyError:\n",
    "            raise ValueError()\n",
    "\n",
    "    def get(self):\n",
    "        if self is Pool.MEAN:\n",
    "            return global_mean_pool\n",
    "        elif self is Pool.SUM:\n",
    "            return global_add_pool\n",
    "        elif self is Pool.NONE:\n",
    "            return BatchIdentity()\n",
    "        else:\n",
    "            raise ValueError(f'Pool {self.name} not supported')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4950e3",
   "metadata": {},
   "source": [
    "The `ActivationType` class is a container where we can set up the various activation functions to be used depending on the dataset, as specified in the `DataSet` class. The activation function is also instantiated in the Action Net of the `Experiment` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edda90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationType(Enum):\n",
    "    \"\"\"\n",
    "        an object for the different activation types\n",
    "    \"\"\"\n",
    "    RELU = auto()\n",
    "    GELU = auto()\n",
    "\n",
    "    @staticmethod\n",
    "    def from_string(s: str):\n",
    "        try:\n",
    "            return ActivationType[s]\n",
    "        except KeyError:\n",
    "            raise ValueError()\n",
    "\n",
    "    def get(self):\n",
    "        if self is ActivationType.RELU:\n",
    "            return torch.nn.functional.relu\n",
    "        elif self is ActivationType.GELU:\n",
    "            return torch.nn.functional.gelu\n",
    "        else:\n",
    "            raise ValueError(f'ActivationType {self.name} not supported')\n",
    "\n",
    "    def nn(self) -> torch.nn.Module:\n",
    "        if self is ActivationType.RELU:\n",
    "            return torch.nn.ReLU()\n",
    "        elif self is ActivationType.GELU:\n",
    "            return torch.nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(f'ActivationType {self.name} not supported')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112e69c5",
   "metadata": {},
   "source": [
    "The `GumbelArgs` class is a container for various parameters for the `Experiment` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d92a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelArgs(NamedTuple):\n",
    "    learn_temp: bool\n",
    "    temp_model_type: ModelType\n",
    "    tau0: float\n",
    "    temp: float\n",
    "    gin_mlp_func: Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e635d1e3",
   "metadata": {},
   "source": [
    "The `Concat2NodeEncoder` is a prerequisite class for the `EnvArgs` class below. It essentially concatenates two node encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d7b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Concat2NodeEncoder(torch.nn.Module):\n",
    "    \"\"\"Encoder that concatenates two node encoders.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, enc1_cls, enc2_cls, in_dim, emb_dim, enc2_dim_pe):\n",
    "        super().__init__()\n",
    "        # PE dims can only be gathered once the cfg is loaded.\n",
    "        self.encoder1 = enc1_cls(in_dim=in_dim, emb_dim=emb_dim - enc2_dim_pe)\n",
    "        self.encoder2 = enc2_cls(in_dim=in_dim, emb_dim=emb_dim, expand_x=False)\n",
    "\n",
    "    def forward(self, x, pestat):\n",
    "        x = self.encoder1(x, pestat)\n",
    "        x = self.encoder2(x, pestat)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa92978",
   "metadata": {},
   "source": [
    "Now, the `LossesAndMetrics` class provides a container for losses and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b44c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossesAndMetrics(NamedTuple):\n",
    "    train_loss: float\n",
    "    val_loss: float\n",
    "    test_loss: float\n",
    "    train_metric: float\n",
    "    val_metric: float\n",
    "    test_metric: float\n",
    "\n",
    "    def get_fold_metrics(self):\n",
    "        return torch.tensor([self.train_metric, self.val_metric, self.test_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dec0f4",
   "metadata": {},
   "source": [
    "The previously defined `LossesAndMetrics` will, among others, be used in the following `MetricType` class, which provides a container for various metrics. It enables the following functionalities:\n",
    "\n",
    "- `apply_metric()` instantiates the desired metric type and uses the object to compute the metric result which it returns as a simple float value.\n",
    "\n",
    "- `is_classification()` returns `True` if the metric type belongs to a classification task.\n",
    "\n",
    "- `is_multilabel()` return `True` if a class object is a multilabel classification task.\n",
    "\n",
    "- `get_task_loss()` returns various loss functions depending on the metric type.\n",
    "\n",
    "- `get_out_dim()` returns the output dimension required for the model based on the number of classes of the dataset.\n",
    "\n",
    "- `higher_is_better()` returns `True` if the model's performance is better when the metric's value is higher.\n",
    "\n",
    "- `src_better_than_other()` compares two metric values and says whether the one is better than the other depending on whether `higher_is_better()`.\n",
    "\n",
    "- `get_worst_losses_n_metrics()` returns the worst possible LossesAndMetrics instance depending on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488e026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricType(Enum):\n",
    "    \"\"\"\n",
    "        an object for the different metrics\n",
    "    \"\"\"\n",
    "    # classification\n",
    "    ACCURACY = auto()\n",
    "    MULTI_LABEL_AP = auto()\n",
    "    AUC_ROC = auto()\n",
    "\n",
    "    # regression\n",
    "    MSE_MAE = auto()\n",
    "\n",
    "    def apply_metric(self, scores: np.ndarray, target: np.ndarray) -> float:\n",
    "        if isinstance(scores, np.ndarray):\n",
    "            scores = torch.from_numpy(scores)\n",
    "        if isinstance(target, np.ndarray):\n",
    "            target = torch.from_numpy(target)\n",
    "        num_classes = scores.size(1)  # target.max().item() + 1\n",
    "        if self is MetricType.ACCURACY:\n",
    "            metric = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        elif self is MetricType.MULTI_LABEL_AP:\n",
    "            metric = AveragePrecision(task=\"multilabel\", num_labels=num_classes).to(scores.device)\n",
    "            result = metric(scores, target.int())\n",
    "            return result.item()\n",
    "        elif self is MetricType.MSE_MAE:\n",
    "            metric = MeanAbsoluteError()\n",
    "        elif self is MetricType.AUC_ROC:\n",
    "            metric = AUROC(task=\"multiclass\", num_classes=num_classes)\n",
    "        else:\n",
    "            raise ValueError(f'MetricType {self.name} not supported')\n",
    "\n",
    "        metric = metric.to(scores.device)\n",
    "        result = metric(scores, target)\n",
    "        return result.item()\n",
    "\n",
    "    def is_classification(self) -> bool:\n",
    "        if self in [MetricType.AUC_ROC, MetricType.ACCURACY, MetricType.MULTI_LABEL_AP]:\n",
    "            return True\n",
    "        elif self is MetricType.MSE_MAE:\n",
    "            return False\n",
    "        else:\n",
    "            raise ValueError(f'MetricType {self.name} not supported')\n",
    "\n",
    "    def is_multilabel(self) -> bool:\n",
    "        return self is MetricType.MULTI_LABEL_AP\n",
    "\n",
    "    def get_task_loss(self):\n",
    "        if self.is_classification():\n",
    "            if self.is_multilabel():\n",
    "                return torch.nn.BCEWithLogitsLoss()\n",
    "            else:\n",
    "                return torch.nn.CrossEntropyLoss()\n",
    "        elif self is MetricType.MSE_MAE:\n",
    "            return torch.nn.MSELoss()\n",
    "        else:\n",
    "            raise ValueError(f'MetricType {self.name} not supported')\n",
    "\n",
    "    def get_out_dim(self, dataset: List[Data]) -> int:\n",
    "        if self.is_classification():\n",
    "            if self.is_multilabel():\n",
    "                return dataset[0].y.shape[1]\n",
    "            else:\n",
    "                return int(max([data.y.max().item() for data in dataset]) + 1)\n",
    "        else:\n",
    "            return dataset[0].y.shape[-1]\n",
    "\n",
    "    def higher_is_better(self):\n",
    "        return self.is_classification()\n",
    "\n",
    "    def src_better_than_other(self, src: float, other: float) -> bool:\n",
    "        if self.higher_is_better():\n",
    "            return src > other\n",
    "        else:\n",
    "            return src < other\n",
    "\n",
    "    def get_worst_losses_n_metrics(self) -> LossesAndMetrics:\n",
    "        if self.is_classification():\n",
    "            return LossesAndMetrics(train_loss=math.inf, val_loss=math.inf, test_loss=math.inf,\n",
    "                                    train_metric=-math.inf, val_metric=-math.inf, test_metric=-math.inf)\n",
    "        else:\n",
    "            return LossesAndMetrics(train_loss=math.inf, val_loss=math.inf, test_loss=math.inf,\n",
    "                                    train_metric=math.inf, val_metric=math.inf, test_metric=math.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1fe8af",
   "metadata": {},
   "source": [
    "The `EncoderLinear` class is a simple linear layer and it is different from the `GraphLinear` class defined later in the argument that the forward pass takes, however that does not impact the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bd31d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLinear(torch.nn.Linear):\n",
    "    def forward(self, x: Tensor, pestat=None) -> Tensor:\n",
    "        return super().forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc8afc2",
   "metadata": {},
   "source": [
    "Next up, the `AtomEncoder` class is a network functioning as an encoder of atom features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super(AtomEncoder, self).__init__()\n",
    "\n",
    "        self.atom_embedding_list = torch.nn.ModuleList()\n",
    "\n",
    "        for i, dim in enumerate(get_atom_feature_dims()):\n",
    "            emb = torch.nn.Embedding(dim, emb_dim)\n",
    "            torch.nn.init.xavier_uniform_(emb.weight.data)\n",
    "            self.atom_embedding_list.append(emb)\n",
    "\n",
    "    def forward(self, x, pestat):\n",
    "        x_embedding = 0\n",
    "        for i in range(x.shape[1]):\n",
    "            x_embedding += self.atom_embedding_list[i](x[:, i])\n",
    "\n",
    "        return x_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c96d0c",
   "metadata": {},
   "source": [
    "Similarly to the `AtomEncoder`, the `BondEncoder` class is a network functioning as an encoder of the bonds, i.e. edges, between two atoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f37f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BondEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim):\n",
    "        super(BondEncoder, self).__init__()\n",
    "\n",
    "        self.bond_embedding_list = torch.nn.ModuleList()\n",
    "\n",
    "        for i, dim in enumerate(get_bond_feature_dims()):\n",
    "            emb = torch.nn.Embedding(dim, emb_dim)\n",
    "            torch.nn.init.xavier_uniform_(emb.weight.data)\n",
    "            self.bond_embedding_list.append(emb)\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        bond_embedding = 0\n",
    "        for i in range(edge_attr.shape[1]):\n",
    "            bond_embedding += self.bond_embedding_list[i](edge_attr[:, i])\n",
    "\n",
    "        return bond_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33eaccc",
   "metadata": {},
   "source": [
    "Incorporating the three Encoders above, the `DataSetEncoders` defines how the nodes' and edges' features shall be eoncoded. It does not hold the encoded data but rather the encoder instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e8ceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetEncoders(Enum):\n",
    "    \"\"\"\n",
    "        an object for the different encoders\n",
    "    \"\"\"\n",
    "    NONE = auto()\n",
    "    MOL = auto()\n",
    "\n",
    "    @staticmethod\n",
    "    def from_string(s: str):\n",
    "        try:\n",
    "            return DataSetEncoders[s]\n",
    "        except KeyError:\n",
    "            raise ValueError()\n",
    "\n",
    "    def node_encoder(self, in_dim: int, emb_dim: int):\n",
    "        if self is DataSetEncoders.NONE:\n",
    "            return EncoderLinear(in_features=in_dim, out_features=emb_dim)\n",
    "        elif self is DataSetEncoders.MOL:\n",
    "            return AtomEncoder(emb_dim)\n",
    "        else:\n",
    "            raise ValueError(f'DataSetEncoders {self.name} not supported')\n",
    "\n",
    "    def edge_encoder(self, emb_dim: int, model_type):\n",
    "        if self is DataSetEncoders.NONE:\n",
    "            return None\n",
    "        elif self is DataSetEncoders.MOL:\n",
    "            if model_type.is_gcn():\n",
    "                return None\n",
    "            else:\n",
    "                return BondEncoder(emb_dim)\n",
    "        else:\n",
    "            raise ValueError(f'DataSetEncoders {self.name} not supported')\n",
    "\n",
    "    def use_encoders(self) -> bool:\n",
    "        return self is not DataSetEncoders.NONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da56cf48",
   "metadata": {},
   "source": [
    "After defining various default values for some parameters, the Laplace Positional Embedding node encoder class `LapPENodeEncoder` is a network that injects positional information into the node features via concatenation to the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAP_DIM_PE = 16\n",
    "LAP_MODEL = 'DeepSet' # renamed from original code\n",
    "LAP_LAYERS = 2 # renamed from original code\n",
    "N_HEADS = 4\n",
    "POST_LAYERS = 0\n",
    "LAP_MAX_FREQS = 10\n",
    "LAP_RAW_NORM_TYPE = 'none' # renamed from original code\n",
    "LAP_PASS_AS_VAR = False # renamed from original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ba519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LapPENodeEncoder(torch.nn.Module):\n",
    "    \"\"\"Laplace Positional Embedding node encoder.\n",
    "\n",
    "    LapPE of size dim_pe will get appended to each node feature vector.\n",
    "    If `expand_x` set True, original node features will be first linearly\n",
    "    projected to (dim_emb - dim_pe) size and the concatenated with LapPE.\n",
    "\n",
    "    Args:\n",
    "        dim_emb: Size of final node embedding\n",
    "        expand_x: Expand node features `x` from dim_in to (dim_emb - dim_pe)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_in, dim_emb, expand_x=True):\n",
    "        super().__init__()\n",
    "        dim_pe = LAP_DIM_PE  # Size of Laplace PE embedding\n",
    "        model_type = LAP_MODEL  # Encoder NN model type for PEs\n",
    "        if model_type not in ['Transformer', 'DeepSet']:\n",
    "            raise ValueError(f\"Unexpected PE model {model_type}\")\n",
    "        self.model_type = model_type\n",
    "        n_layers = LAP_LAYERS  # Num. layers in PE encoder model\n",
    "        n_heads = N_HEADS  # Num. attention heads in Trf PE encoder\n",
    "        post_n_layers = POST_LAYERS  # Num. layers to apply after pooling\n",
    "        max_freqs = LAP_MAX_FREQS  # Num. eigenvectors (frequencies)\n",
    "        norm_type = LAP_RAW_NORM_TYPE.lower()  # Raw PE normalization layer type\n",
    "        self.pass_as_var = LAP_PASS_AS_VAR  # Pass PE also as a separate variable\n",
    "\n",
    "        if dim_emb - dim_pe < 0: # formerly 1, but you could have zero feature size\n",
    "            raise ValueError(f\"LapPE size {dim_pe} is too large for \"\n",
    "                             f\"desired embedding size of {dim_emb}.\")\n",
    "\n",
    "        if expand_x and dim_emb - dim_pe > 0:\n",
    "            self.linear_x = torch.nn.Linear(dim_in, dim_emb - dim_pe)\n",
    "        self.expand_x = expand_x and dim_emb - dim_pe > 0\n",
    "\n",
    "        # Initial projection of eigenvalue and the node's eigenvector value\n",
    "        self.linear_A = torch.nn.Linear(2, dim_pe)\n",
    "        if norm_type == 'batchnorm':\n",
    "            self.raw_norm = torch.nn.BatchNorm1d(max_freqs)\n",
    "        else:\n",
    "            self.raw_norm = None\n",
    "\n",
    "        activation = torch.nn.ReLU  # register.act_dict[cfg.gnn.act]\n",
    "        if model_type == 'Transformer':\n",
    "            # Transformer model for LapPE\n",
    "            encoder_layer = torch.nn.TransformerEncoderLayer(d_model=dim_pe,\n",
    "                                                       nhead=n_heads,\n",
    "                                                       batch_first=True)\n",
    "            self.pe_encoder = torch.nn.TransformerEncoder(encoder_layer,\n",
    "                                                    num_layers=n_layers)\n",
    "        else:\n",
    "            # DeepSet model for LapPE\n",
    "            layers = []\n",
    "            if n_layers == 1:\n",
    "                layers.append(activation())\n",
    "            else:\n",
    "                self.linear_A = torch.nn.Linear(2, 2 * dim_pe)\n",
    "                layers.append(activation())\n",
    "                for _ in range(n_layers - 2):\n",
    "                    layers.append(torch.nn.Linear(2 * dim_pe, 2 * dim_pe))\n",
    "                    layers.append(activation())\n",
    "                layers.append(torch.nn.Linear(2 * dim_pe, dim_pe))\n",
    "                layers.append(activation())\n",
    "            self.pe_encoder = torch.nn.Sequential(*layers)\n",
    "\n",
    "        self.post_mlp = None\n",
    "        if post_n_layers > 0:\n",
    "            # MLP to apply post pooling\n",
    "            layers = []\n",
    "            if post_n_layers == 1:\n",
    "                layers.append(torch.nn.Linear(dim_pe, dim_pe))\n",
    "                layers.append(activation())\n",
    "            else:\n",
    "                layers.append(torch.nn.Linear(dim_pe, 2 * dim_pe))\n",
    "                layers.append(activation())\n",
    "                for _ in range(post_n_layers - 2):\n",
    "                    layers.append(torch.nn.Linear(2 * dim_pe, 2 * dim_pe))\n",
    "                    layers.append(activation())\n",
    "                layers.append(torch.nn.Linear(2 * dim_pe, dim_pe))\n",
    "                layers.append(activation())\n",
    "            self.post_mlp = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, pestat):\n",
    "        EigVals = pestat[0]\n",
    "        EigVecs = pestat[1]\n",
    "\n",
    "        if self.training:\n",
    "            sign_flip = torch.rand(EigVecs.size(1), device=EigVecs.device)\n",
    "            sign_flip[sign_flip >= 0.5] = 1.0\n",
    "            sign_flip[sign_flip < 0.5] = -1.0\n",
    "            EigVecs = EigVecs * sign_flip.unsqueeze(0)\n",
    "\n",
    "        pos_enc = torch.cat((EigVecs.unsqueeze(2), EigVals), dim=2) # (Num nodes) x (Num Eigenvectors) x 2\n",
    "        empty_mask = torch.isnan(pos_enc)  # (Num nodes) x (Num Eigenvectors) x 2\n",
    "\n",
    "        pos_enc[empty_mask] = 0  # (Num nodes) x (Num Eigenvectors) x 2\n",
    "        if self.raw_norm:\n",
    "            pos_enc = self.raw_norm(pos_enc)\n",
    "        pos_enc = self.linear_A(pos_enc)  # (Num nodes) x (Num Eigenvectors) x dim_pe\n",
    "\n",
    "        # PE encoder: a Transformer or DeepSet model\n",
    "        if self.model_type == 'Transformer':\n",
    "            pos_enc = self.pe_encoder(src=pos_enc,\n",
    "                                      src_key_padding_mask=empty_mask[:, :, 0])\n",
    "        else:\n",
    "            pos_enc = self.pe_encoder(pos_enc)\n",
    "\n",
    "        # Remove masked sequences; must clone before overwriting masked elements\n",
    "        pos_enc = pos_enc.clone().masked_fill_(empty_mask[:, :, 0].unsqueeze(2), 0.)\n",
    "\n",
    "        # Sum pooling\n",
    "        pos_enc = torch.sum(pos_enc, 1, keepdim=False)  # (Num nodes) x dim_pe\n",
    "\n",
    "        # MLP post pooling\n",
    "        if self.post_mlp is not None:\n",
    "            pos_enc = self.post_mlp(pos_enc)  # (Num nodes) x dim_pe\n",
    "\n",
    "        # Expand node features if needed\n",
    "        if self.expand_x:\n",
    "            h = self.linear_x(x)\n",
    "        else:\n",
    "            h = x\n",
    "        # Concatenate final PEs to input embedding\n",
    "        x = torch.cat((h, pos_enc), 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e054d345",
   "metadata": {},
   "source": [
    "Again after defining some default parameter values, the Kernel-based Positional Embedding node encoder class `KernelPENodeEncoder`. In contrast to the `LapPENodeEncoder` which uses Laplace eigenvalues and eigenvectors to encode global graph-level structural information, this `KernelPENodeEncoder` relies on statistics such as Random Walk Structural Encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c4787",
   "metadata": {},
   "outputs": [],
   "source": [
    "KER_DIM_PE = 28\n",
    "NUM_RW_STEPS = 20\n",
    "KER_MODEL = 'Linear' # renamed from original code\n",
    "KER_LAYERS = 3 # renamed from original code\n",
    "KER_RAW_NORM_TYPE = 'BatchNorm' # renamed from original code\n",
    "KER_PASS_AS_VAR = False # renamed from original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734d5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelPENodeEncoder(torch.nn.Module):\n",
    "    \"\"\"Configurable kernel-based Positional Encoding node encoder.\n",
    "\n",
    "    The choice of which kernel-based statistics to use is configurable through\n",
    "    setting of `kernel_type`. Based on this, the appropriate config is selected,\n",
    "    and also the appropriate variable with precomputed kernel stats is then\n",
    "    selected from PyG Data graphs in `forward` function.\n",
    "    E.g., supported are 'RWSE', 'HKdiagSE', 'ElstaticSE'.\n",
    "\n",
    "    PE of size `dim_pe` will get appended to each node feature vector.\n",
    "    If `expand_x` set True, original node features will be first linearly\n",
    "    projected to (dim_emb - dim_pe) size and the concatenated with PE.\n",
    "\n",
    "    Args:\n",
    "        dim_emb: Size of final node embedding\n",
    "        expand_x: Expand node features `x` from dim_in to (dim_emb - dim_pe)\n",
    "    \"\"\"\n",
    "\n",
    "    kernel_type = None  # Instantiated type of the KernelPE, e.g. RWSE\n",
    "\n",
    "    def __init__(self, dim_in, dim_emb, expand_x=True):\n",
    "        super().__init__()\n",
    "        if self.kernel_type is None:\n",
    "            raise ValueError(f\"{self.__class__.__name__} has to be \"\n",
    "                             f\"preconfigured by setting 'kernel_type' class\"\n",
    "                             f\"variable before calling the constructor.\")\n",
    "\n",
    "        dim_pe = KER_DIM_PE  # Size of the kernel-based PE embedding\n",
    "        num_rw_steps = NUM_RW_STEPS\n",
    "        model_type = KER_MODEL.lower()  # Encoder NN model type for PEs\n",
    "        n_layers = KER_LAYERS  # Num. layers in PE encoder model\n",
    "        norm_type = KER_RAW_NORM_TYPE.lower()  # Raw PE normalization layer type\n",
    "        self.pass_as_var = KER_PASS_AS_VAR  # Pass PE also as a separate variable\n",
    "\n",
    "        if dim_emb - dim_pe < 0: # formerly 1, but you could have zero feature size\n",
    "            raise ValueError(f\"PE dim size {dim_pe} is too large for \"\n",
    "                             f\"desired embedding size of {dim_emb}.\")\n",
    "\n",
    "        if expand_x and dim_emb - dim_pe > 0:\n",
    "            self.linear_x = torch.nn.Linear(dim_in, dim_emb - dim_pe)\n",
    "        self.expand_x = expand_x and dim_emb - dim_pe > 0\n",
    "\n",
    "        if norm_type == 'batchnorm':\n",
    "            self.raw_norm = torch.nn.BatchNorm1d(num_rw_steps)\n",
    "        else:\n",
    "            self.raw_norm = None\n",
    "\n",
    "        activation = torch.nn.ReLU  # register.act_dict[cfg.gnn.act]\n",
    "        if model_type == 'mlp':\n",
    "            layers = []\n",
    "            if n_layers == 1:\n",
    "                layers.append(torch.nn.Linear(num_rw_steps, dim_pe))\n",
    "                layers.append(activation())\n",
    "            else:\n",
    "                layers.append(torch.nn.Linear(num_rw_steps, 2 * dim_pe))\n",
    "                layers.append(activation())\n",
    "                for _ in range(n_layers - 2):\n",
    "                    layers.append(torch.nn.Linear(2 * dim_pe, 2 * dim_pe))\n",
    "                    layers.append(activation())\n",
    "                layers.append(torch.nn.Linear(2 * dim_pe, dim_pe))\n",
    "                layers.append(activation())\n",
    "            self.pe_encoder = torch.nn.Sequential(*layers)\n",
    "        elif model_type == 'linear':\n",
    "            self.pe_encoder = torch.nn.Linear(num_rw_steps, dim_pe)\n",
    "        else:\n",
    "            raise ValueError(f\"{self.__class__.__name__}: Does not support \"\n",
    "                             f\"'{model_type}' encoder model.\")\n",
    "\n",
    "    def forward(self, x, pestat):\n",
    "        pos_enc = pestat  # (Num nodes) x (Num kernel times)\n",
    "        # pos_enc = batch.rw_landing  # (Num nodes) x (Num kernel times)\n",
    "        if self.raw_norm:\n",
    "            pos_enc = self.raw_norm(pos_enc)\n",
    "        pos_enc = self.pe_encoder(pos_enc)  # (Num nodes) x dim_pe\n",
    "\n",
    "        # Expand node features if needed\n",
    "        if self.expand_x:\n",
    "            h = self.linear_x(x)\n",
    "        else:\n",
    "            h = x\n",
    "        # Concatenate final PEs to input embedding\n",
    "        x = torch.cat((h, pos_enc), 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a5bd3a",
   "metadata": {},
   "source": [
    "The `RWSENodeEncoder` is then just an implementation of a specific type of `KernelPENodeEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acd5e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RWSENodeEncoder(KernelPENodeEncoder):\n",
    "    \"\"\"Random Walk Structural Encoding node encoder.\n",
    "    \"\"\"\n",
    "    kernel_type = 'RWSE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88fa623",
   "metadata": {},
   "source": [
    "The `PosEncoder` class provides an abstraction for the various encoder classes defined earlier. With it we can do the following:\n",
    "\n",
    "- `get()` returns an instance of the encoder.\n",
    "\n",
    "- `DIM_PE()` returns the dimensionality of the encoding vector.\n",
    "\n",
    "- `get_pe()` returns the positional encoding vector of each node in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07461d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEncoder(Enum):\n",
    "    \"\"\"\n",
    "        an object for the different encoders\n",
    "    \"\"\"\n",
    "    NONE = auto()\n",
    "    LAP = auto()\n",
    "    RWSE = auto()\n",
    "\n",
    "    @staticmethod\n",
    "    def from_string(s: str):\n",
    "        try:\n",
    "            return PosEncoder[s]\n",
    "        except KeyError:\n",
    "            raise ValueError()\n",
    "\n",
    "    def get(self, in_dim: int, emb_dim: int, expand_x: bool):\n",
    "        if self is PosEncoder.NONE:\n",
    "            return None\n",
    "        elif self is PosEncoder.LAP:\n",
    "            return LapPENodeEncoder(dim_in=in_dim, dim_emb=emb_dim, expand_x=expand_x)\n",
    "        elif self is PosEncoder.RWSE:\n",
    "            return RWSENodeEncoder(dim_in=in_dim, dim_emb=emb_dim, expand_x=expand_x)\n",
    "        else:\n",
    "            raise ValueError(f'DataSetEncoders {self.name} not supported')\n",
    "\n",
    "    def DIM_PE(self):\n",
    "        if self is PosEncoder.NONE:\n",
    "            return None\n",
    "        elif self is PosEncoder.LAP:\n",
    "            return LAP_DIM_PE\n",
    "        elif self is PosEncoder.RWSE:\n",
    "            return KER_DIM_PE\n",
    "        else:\n",
    "            raise ValueError(f'DataSetEncoders {self.name} not supported')\n",
    "\n",
    "    def get_pe(self, data: Data, device):\n",
    "        if self is PosEncoder.NONE:\n",
    "            return None\n",
    "        elif self is PosEncoder.LAP:\n",
    "            return [data.EigVals.to(device), data.EigVecs.to(device)]\n",
    "        elif self is PosEncoder.RWSE:\n",
    "            return data.pestat_RWSE.to(device)\n",
    "        else:\n",
    "            raise ValueError(f'DataSetEncoders {self.name} not supported')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc2f2d",
   "metadata": {},
   "source": [
    "The `EnvArgs` class provides a container for the architecture-related hyperparameters. Its `load_net()` function builds the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvArgs(NamedTuple):\n",
    "    model_type: ModelType\n",
    "    num_layers: int\n",
    "    env_dim: int\n",
    "\n",
    "    layer_norm: bool\n",
    "    skip: bool\n",
    "    batch_norm: bool\n",
    "    dropout: float\n",
    "    act_type: ActivationType\n",
    "    dec_num_layers: int\n",
    "    pos_enc: PosEncoder\n",
    "    dataset_encoders: DataSetEncoders\n",
    "\n",
    "    metric_type: MetricType\n",
    "    in_dim: int\n",
    "    out_dim: int\n",
    "\n",
    "    gin_mlp_func: Callable\n",
    "\n",
    "    def load_net(self) -> torch.nn.ModuleList:\n",
    "        if self.pos_enc is PosEncoder.NONE:\n",
    "            enc_list = [self.dataset_encoders.node_encoder(in_dim=self.in_dim, emb_dim=self.env_dim)]\n",
    "        else:\n",
    "            if self.dataset_encoders is DataSetEncoders.NONE:\n",
    "                enc_list = [self.pos_enc.get(in_dim=self.in_dim, emb_dim=self.env_dim)]\n",
    "            else:\n",
    "                enc_list = [Concat2NodeEncoder(enc1_cls=self.dataset_encoders.node_encoder,\n",
    "                                               enc2_cls=self.pos_enc.get,\n",
    "                                               in_dim=self.in_dim, emb_dim=self.env_dim,\n",
    "                                               enc2_dim_pe=self.pos_enc.DIM_PE())]\n",
    "\n",
    "        component_list = self.model_type.get_component_list(in_dim=self.env_dim, hidden_dim=self.env_dim,  out_dim=self.env_dim,\n",
    "                                               num_layers=self.num_layers, bias=True, edges_required=True,\n",
    "                                               gin_mlp_func=self.gin_mlp_func)\n",
    "\n",
    "        if self.dec_num_layers > 1:\n",
    "            mlp_list = (self.dec_num_layers - 1) * [torch.nn.Linear(self.env_dim, self.env_dim),\n",
    "                                                    torch.nn.Dropout(self.dropout), self.act_type.nn()]\n",
    "            mlp_list = mlp_list + [torch.nn.Linear(self.env_dim, self.out_dim)]\n",
    "            dec_list = [torch.nn.Sequential(*mlp_list)]\n",
    "        else:\n",
    "            dec_list = [torch.nn.Linear(self.env_dim, self.out_dim)]\n",
    "\n",
    "        return torch.nn.ModuleList(enc_list + component_list + dec_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def20b0f",
   "metadata": {},
   "source": [
    "The \"policy\" of the network, i.e. which state to choose for each node is the training goal of the Action Network. To this end, we construct the `ActionNetArgs` which, similarly to the `EnvArgs` class, contains its hyperparameters and the `load_net()` function builds the full Action Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fa9c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNetArgs(NamedTuple):\n",
    "    model_type: ModelType\n",
    "    num_layers: int\n",
    "    hidden_dim: int\n",
    "\n",
    "    dropout: float\n",
    "    act_type: ActivationType\n",
    "\n",
    "    env_dim: int\n",
    "    gin_mlp_func: Callable\n",
    "    \n",
    "    def load_net(self) -> torch.nn.ModuleList:\n",
    "        net = self.model_type.get_component_list(in_dim=self.env_dim, hidden_dim=self.hidden_dim, out_dim=2,\n",
    "                                                 num_layers=self.num_layers, bias=True, edges_required=False,\n",
    "                                                 gin_mlp_func=self.gin_mlp_func)\n",
    "        return torch.nn.ModuleList(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b33a2",
   "metadata": {},
   "source": [
    "Having defined the `ActionNetArgs`, we should now also implement the `ActionNet` class. It constructs the network according to the arguments where the network architecture is created via the `load_net` function from the arguments and dropout, as well as activation function, are chosen therefrom, too. The `forward` function simply computes the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNet(torch.nn.Module):\n",
    "    def __init__(self, action_args: ActionNetArgs):\n",
    "        \"\"\"\n",
    "        Create a model which represents the agent's policy.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_layers = action_args.num_layers\n",
    "        self.net = action_args.load_net()\n",
    "        self.dropout = torch.nn.Dropout(action_args.dropout)\n",
    "        self.act = action_args.act_type.get()\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj, env_edge_attr: OptTensor, act_edge_attr: OptTensor) -> Tensor:\n",
    "        edge_attrs = [env_edge_attr] + (self.num_layers - 1) * [act_edge_attr]\n",
    "        for idx, (edge_attr, layer) in enumerate(zip(edge_attrs[:-1], self.net[:-1])):\n",
    "            x = layer(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "            x = self.dropout(x)\n",
    "            x = self.act(x)\n",
    "        x = self.net[-1](x=x, edge_index=edge_index, edge_attr=edge_attrs[-1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc91bd84",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c94cc",
   "metadata": {},
   "source": [
    "The `DatasetBySplit` class is a simple container that holds the data in training, validation and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663e74d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetBySplit(NamedTuple):\n",
    "    train: Union[Data, List[Data]]\n",
    "    val: Union[Data, List[Data]]\n",
    "    test: Union[Data, List[Data]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a2ff48",
   "metadata": {},
   "source": [
    "The `DataSetFamily` class is a simple container grouping the different datasets into predefined categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2194f4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetFamily(Enum):\n",
    "    heterophilic = auto()\n",
    "    synthetic = auto()\n",
    "    social_networks = auto()\n",
    "    proteins = auto()\n",
    "    lrgb = auto()\n",
    "    homophilic = auto()\n",
    "\n",
    "    @staticmethod\n",
    "    def from_string(s: str):\n",
    "        try:\n",
    "            return DataSetFamily[s]\n",
    "        except KeyError:\n",
    "            raise ValueError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1050265",
   "metadata": {},
   "source": [
    "Before defining the final `DataSet` class, we need to implement two helper functions `get_cosine_schedule_with_warmup` and `cosine_with_warmup_scheduler`. The latter is a simple wrapper, whereas the former returns a different scheduler instance, depending on whether the current step is smaller than number of warmup steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06620f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_schedule_with_warmup(\n",
    "        optimizer: torch.optim.Optimizer, num_warmup_steps: int, num_training_steps: int,\n",
    "        num_cycles: float = 0.5, last_epoch: int = -1):\n",
    "    \"\"\"\n",
    "    Implementation by Huggingface:\n",
    "    https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/optimization.py\n",
    "\n",
    "    Create a schedule with a learning rate that decreases following the values\n",
    "    of the cosine function between the initial lr set in the optimizer to 0,\n",
    "    after a warmup period during which it increases linearly between 0 and the\n",
    "    initial lr set in the optimizer.\n",
    "    Args:\n",
    "        optimizer ([`~torch.optim.Optimizer`]):\n",
    "            The optimizer for which to schedule the learning rate.\n",
    "        num_warmup_steps (`int`):\n",
    "            The number of steps for the warmup phase.\n",
    "        num_training_steps (`int`):\n",
    "            The total number of training steps.\n",
    "        num_cycles (`float`, *optional*, defaults to 0.5):\n",
    "            The number of waves in the cosine schedule (the defaults is to just\n",
    "            decrease from the max value to 0 following a half-cosine).\n",
    "        last_epoch (`int`, *optional*, defaults to -1):\n",
    "            The index of the last epoch when resuming training.\n",
    "    Return:\n",
    "        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
    "    \"\"\"\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return max(1e-6, float(current_step) / float(max(1, num_warmup_steps)))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715595fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_with_warmup_scheduler(optimizer: torch.optim.Optimizer,\n",
    "                                 num_warmup_epochs: int, max_epoch: int):\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_epochs,\n",
    "        num_training_steps=max_epoch\n",
    "    )\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a63fccd",
   "metadata": {},
   "source": [
    "One final set of helper functions that we will need to set up is `apply_transform` and its subfunctions. We start with `pre_transform_in_memory`. It essentially applies a transformation function to a dataset and ensuring `None` values are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17cede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_transform_in_memory(dataset, transform_func, show_progress=False):\n",
    "    \"\"\"\n",
    "    Pre-transform already loaded PyG dataset object.\n",
    "\n",
    "    Apply transform function to a loaded PyG dataset object so that\n",
    "    the transformed result is persistent for the lifespan of the object.\n",
    "    This means the result is not saved to disk, as what PyG's `pre_transform`\n",
    "    would do, but also the transform is applied only once and not at each\n",
    "    data access as what PyG's `transform` hook does.\n",
    "\n",
    "    Implementation is based on torch_geometric.data.in_memory_dataset.copy\n",
    "\n",
    "    Args:\n",
    "        dataset: PyG dataset object to modify\n",
    "        transform_func: transformation function to apply to each data example\n",
    "        show_progress: show tqdm progress bar\n",
    "    \"\"\"\n",
    "    if transform_func is None:\n",
    "        return dataset\n",
    "\n",
    "    data_list = [transform_func(dataset.get(i))\n",
    "                 for i in tqdm(range(len(dataset)),\n",
    "                               disable=not show_progress,\n",
    "                               mininterval=10,\n",
    "                               miniters=len(dataset)//20)]\n",
    "    data_list = list(filter(None, data_list))\n",
    "\n",
    "    dataset._indices = None\n",
    "    dataset._data_list = data_list\n",
    "    dataset.data, dataset.slices = dataset.collate(data_list)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d36bf",
   "metadata": {},
   "source": [
    "Next up is the `eigvec_normalizer` function which allows for different methods of normalization of eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigvec_normalizer(EigVecs, EigVals, normalization=\"L2\", eps=1e-12):\n",
    "    \"\"\"\n",
    "    Implement different eigenvector normalizations.\n",
    "    \"\"\"\n",
    "\n",
    "    EigVals = EigVals.unsqueeze(0)\n",
    "\n",
    "    if normalization == \"L1\":\n",
    "        # L1 normalization: eigvec / sum(abs(eigvec))\n",
    "        denom = EigVecs.norm(p=1, dim=0, keepdim=True)\n",
    "\n",
    "    elif normalization == \"L2\":\n",
    "        # L2 normalization: eigvec / sqrt(sum(eigvec^2))\n",
    "        denom = EigVecs.norm(p=2, dim=0, keepdim=True)\n",
    "\n",
    "    elif normalization == \"abs-max\":\n",
    "        # AbsMax normalization: eigvec / max|eigvec|\n",
    "        denom = torch.max(EigVecs.abs(), dim=0, keepdim=True).values\n",
    "\n",
    "    elif normalization == \"wavelength\":\n",
    "        # AbsMax normalization, followed by wavelength multiplication:\n",
    "        # eigvec * pi / (2 * max|eigvec| * sqrt(eigval))\n",
    "        denom = torch.max(EigVecs.abs(), dim=0, keepdim=True).values\n",
    "        eigval_denom = torch.sqrt(EigVals)\n",
    "        eigval_denom[EigVals < eps] = 1  # Problem with eigval = 0\n",
    "        denom = denom * eigval_denom * 2 / np.pi\n",
    "\n",
    "    elif normalization == \"wavelength-asin\":\n",
    "        # AbsMax normalization, followed by arcsin and wavelength multiplication:\n",
    "        # arcsin(eigvec / max|eigvec|)  /  sqrt(eigval)\n",
    "        denom_temp = torch.max(EigVecs.abs(), dim=0, keepdim=True).values.clamp_min(eps).expand_as(EigVecs)\n",
    "        EigVecs = torch.asin(EigVecs / denom_temp)\n",
    "        eigval_denom = torch.sqrt(EigVals)\n",
    "        eigval_denom[EigVals < eps] = 1  # Problem with eigval = 0\n",
    "        denom = eigval_denom\n",
    "\n",
    "    elif normalization == \"wavelength-soft\":\n",
    "        # AbsSoftmax normalization, followed by wavelength multiplication:\n",
    "        # eigvec / (softmax|eigvec| * sqrt(eigval))\n",
    "        denom = (torch.nn.functional.softmax(EigVecs.abs(), dim=0) * EigVecs.abs()).sum(dim=0, keepdim=True)\n",
    "        eigval_denom = torch.sqrt(EigVals)\n",
    "        eigval_denom[EigVals < eps] = 1  # Problem with eigval = 0\n",
    "        denom = denom * eigval_denom\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported normalization `{normalization}`\")\n",
    "\n",
    "    denom = denom.clamp_min(eps).expand_as(EigVecs)\n",
    "    EigVecs = EigVecs / denom\n",
    "\n",
    "    return EigVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d560c8",
   "metadata": {},
   "source": [
    "Now, after defining some default constants, we implement the `get_lap_decomp_stats` function, which computes the laplacian eigen-decomposition-based positional encoding statistics of a given a given graph and correspondingly returns the eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1366b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAPLACIAN_NORM = 'none'\n",
    "POSENC_MAX_FREQS = 10\n",
    "EIGVEC_NORM = 'L2'\n",
    "#KERNEL = ''\n",
    "TIMES = list(range(1, 21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f92e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lap_decomp_stats(evals, evects, max_freqs, eigvec_norm='L2'):\n",
    "    \"\"\"Compute Laplacian eigen-decomposition-based PE stats of the given graph.\n",
    "\n",
    "    Args:\n",
    "        evals, evects: Precomputed eigen-decomposition\n",
    "        max_freqs: Maximum number of top smallest frequencies / eigenvecs to use\n",
    "        eigvec_norm: Normalization for the eigen vectors of the Laplacian\n",
    "    Returns:\n",
    "        Tensor (num_nodes, max_freqs, 1) eigenvalues repeated for each node\n",
    "        Tensor (num_nodes, max_freqs) of eigenvector values per node\n",
    "    \"\"\"\n",
    "    N = len(evals)  # Number of nodes, including disconnected nodes.\n",
    "\n",
    "    # Keep up to the maximum desired number of frequencies.\n",
    "    idx = evals.argsort()[:max_freqs]\n",
    "    evals, evects = evals[idx], np.real(evects[:, idx])\n",
    "    evals = torch.from_numpy(np.real(evals)).clamp_min(0)\n",
    "\n",
    "    # Normalize and pad eigen vectors.\n",
    "    evects = torch.from_numpy(evects).float()\n",
    "    evects = eigvec_normalizer(evects, evals, normalization=eigvec_norm)\n",
    "    if N < max_freqs:\n",
    "        EigVecs = torch.nn.functional.pad(evects, (0, max_freqs - N), value=float('nan'))\n",
    "    else:\n",
    "        EigVecs = evects\n",
    "\n",
    "    # Pad and save eigenvalues.\n",
    "    if N < max_freqs:\n",
    "        EigVals = torch.nn.functional.pad(evals, (0, max_freqs - N), value=float('nan')).unsqueeze(0)\n",
    "    else:\n",
    "        EigVals = evals.unsqueeze(0)\n",
    "    EigVals = EigVals.repeat(N, 1).unsqueeze(2)\n",
    "\n",
    "    return EigVals, EigVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1055a3c",
   "metadata": {},
   "source": [
    "The next thing we need to do is create a function `get_rw_landing_probs` which computes the random walk probabilities of returning to a node after a range of number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7eb302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rw_landing_probs(ksteps, edge_index, edge_weight=None,\n",
    "                         num_nodes=None, space_dim=0):\n",
    "    \"\"\"Compute Random Walk landing probabilities for given list of K steps.\n",
    "\n",
    "    Args:\n",
    "        ksteps: List of k-steps for which to compute the RW landings\n",
    "        edge_index: PyG sparse representation of the graph\n",
    "        edge_weight: (optional) Edge weights\n",
    "        num_nodes: (optional) Number of nodes in the graph\n",
    "        space_dim: (optional) Estimated dimensionality of the space. Used to\n",
    "            correct the random-walk diagonal by a factor `k^(space_dim/2)`.\n",
    "            In euclidean space, this correction means that the height of\n",
    "            the gaussian distribution stays almost constant across the number of\n",
    "            steps, if `space_dim` is the dimension of the euclidean space.\n",
    "\n",
    "    Returns:\n",
    "        2D Tensor with shape (num_nodes, len(ksteps)) with RW landing probs\n",
    "    \"\"\"\n",
    "    if edge_weight is None:\n",
    "        edge_weight = torch.ones(edge_index.size(1), device=edge_index.device)\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "    source, dest = edge_index[0], edge_index[1]\n",
    "    deg = scatter(edge_weight, source, dim=0, dim_size=num_nodes, reduce='sum')  # Out degrees.\n",
    "    deg_inv = deg.pow(-1.)\n",
    "    deg_inv.masked_fill_(deg_inv == float('inf'), 0)\n",
    "\n",
    "    if edge_index.numel() == 0:\n",
    "        P = edge_index.new_zeros((1, num_nodes, num_nodes))\n",
    "    else:\n",
    "        # P = D^-1 * A\n",
    "        P = torch.diag(deg_inv) @ to_dense_adj(edge_index, max_num_nodes=num_nodes)  # 1 x (Num nodes) x (Num nodes)\n",
    "    rws = []\n",
    "    if ksteps == list(range(min(ksteps), max(ksteps) + 1)):\n",
    "        # Efficient way if ksteps are a consecutive sequence (most of the time the case)\n",
    "        Pk = P.clone().detach().matrix_power(min(ksteps))\n",
    "        for k in range(min(ksteps), max(ksteps) + 1):\n",
    "            rws.append(torch.diagonal(Pk, dim1=-2, dim2=-1) * (k ** (space_dim / 2)))\n",
    "            Pk = Pk @ P\n",
    "    else:\n",
    "        # Explicitly raising P to power k for each k \\in ksteps.\n",
    "        for k in ksteps:\n",
    "            rws.append(torch.diagonal(P.matrix_power(k), dim1=-2, dim2=-1) * \\\n",
    "                       (k ** (space_dim / 2)))\n",
    "\n",
    "    rw_landing = torch.cat(rws, dim=0).transpose(0, 1)  # (Num nodes) x (K steps)\n",
    "\n",
    "    return rw_landing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d501313",
   "metadata": {},
   "source": [
    "The two functions we just implemented will now be useful when coding the `compute_posenc_stats` which enables us to compute the positional encoding statistics for a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef36e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posenc_stats(data, pos_encoder: PosEncoder, is_undirected):\n",
    "    \"\"\"Precompute positional encodings for the given graph.\n",
    "\n",
    "    Supported PE statistics to precompute, selected by `pe_types`:\n",
    "    'LapPE': Laplacian eigen-decomposition.\n",
    "    'RWSE': Random walk landing probabilities (diagonals of RW matrices).\n",
    "\n",
    "    Args:\n",
    "        data: PyG graph\n",
    "        is_undirected: True if the graph is expected to be undirected\n",
    "\n",
    "    Returns:\n",
    "        Extended PyG Data object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Basic preprocessing of the input graph.\n",
    "    if hasattr(data, 'num_nodes'):\n",
    "        N = data.num_nodes  # Explicitly given number of nodes, e.g. ogbg-ppa\n",
    "    else:\n",
    "        N = data.x.shape[0]  # Number of nodes, including disconnected nodes.\n",
    "    laplacian_norm_type = LAPLACIAN_NORM.lower()\n",
    "    if laplacian_norm_type == 'none':\n",
    "        laplacian_norm_type = None\n",
    "    if is_undirected:\n",
    "        undir_edge_index = data.edge_index\n",
    "    else:\n",
    "        undir_edge_index = to_undirected(data.edge_index)\n",
    "\n",
    "    # Eigen values and vectors.\n",
    "    evals, evects = None, None\n",
    "    if pos_encoder is PosEncoder.LAP:\n",
    "        # Eigen-decomposition with numpy, can be reused for Heat kernels.\n",
    "        L = to_scipy_sparse_matrix(\n",
    "            *get_laplacian(undir_edge_index, normalization=laplacian_norm_type,\n",
    "                           num_nodes=N)\n",
    "        )\n",
    "        evals, evects = np.linalg.eigh(L.toarray())\n",
    "        max_freqs = POSENC_MAX_FREQS\n",
    "        eigvec_norm = EIGVEC_NORM\n",
    "        data.EigVals, data.EigVecs = get_lap_decomp_stats(\n",
    "            evals=evals, evects=evects,\n",
    "            max_freqs=max_freqs,\n",
    "            eigvec_norm=eigvec_norm)\n",
    "    elif pos_encoder is PosEncoder.RWSE:\n",
    "        times = TIMES\n",
    "        if len(times) == 0:\n",
    "            raise ValueError(\"List of kernel times required for RWSE\")\n",
    "        rw_landing = get_rw_landing_probs(ksteps=times,\n",
    "                                          edge_index=data.edge_index,\n",
    "                                          num_nodes=N)\n",
    "        data.pestat_RWSE = rw_landing\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5737fff5",
   "metadata": {},
   "source": [
    "The constant `TASK` defines whether the task at hand is graph-level or node-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763eed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = 'graph'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b51a4b0",
   "metadata": {},
   "source": [
    "The `set_dataset_splits` function sets the training, validation and test splits for a given dataset object and depending on graph/node-level tasks, ensuring that there are no overlaps between the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6141d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dataset_splits(dataset, splits):\n",
    "    \"\"\"Set given splits to the dataset object.\n",
    "\n",
    "    Args:\n",
    "        dataset: PyG dataset object\n",
    "        splits: List of train/val/test split indices\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any pair of splits has intersecting indices\n",
    "    \"\"\"\n",
    "    # First check whether splits intersect and raise error if so.\n",
    "    for i in range(len(splits) - 1):\n",
    "        for j in range(i + 1, len(splits)):\n",
    "            n_intersect = len(set(splits[i]) & set(splits[j]))\n",
    "            if n_intersect != 0:\n",
    "                raise ValueError(\n",
    "                    f\"Splits must not have intersecting indices: \"\n",
    "                    f\"split #{i} (n = {len(splits[i])}) and \"\n",
    "                    f\"split #{j} (n = {len(splits[j])}) have \"\n",
    "                    f\"{n_intersect} intersecting indices\"\n",
    "                )\n",
    "\n",
    "    task_level = TASK\n",
    "    if task_level == 'node':\n",
    "        split_names = ['train_mask', 'val_mask', 'test_mask']\n",
    "        for split_name, split_index in zip(split_names, splits):\n",
    "            mask = index2mask(split_index, size=dataset.data.y.shape[0])\n",
    "            set_dataset_attr(dataset, split_name, mask, len(mask))\n",
    "\n",
    "    elif task_level == 'graph':\n",
    "        split_names = [\n",
    "            'train_graph_index', 'val_graph_index', 'test_graph_index'\n",
    "        ]\n",
    "        for split_name, split_index in zip(split_names, splits):\n",
    "            set_dataset_attr(dataset, split_name, split_index, len(split_index))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset task level: {task_level}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70892b49",
   "metadata": {},
   "source": [
    "Now finally we can implement the `apply_transform` function  which takes in a dataset and a positional encoder, which then gets applied to the dataset to compute node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e73941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transform(dataset, pos_encoder):\n",
    "    start = time.perf_counter()\n",
    "    logging.info(f\"Precomputing Positional Encoding statistics for all graphs... \")\n",
    "    # Estimate directedness based on 10 graphs to save time.\n",
    "    is_undirected = all(d.is_undirected() for d in dataset[:10])\n",
    "    logging.info(f\"  ...estimated to be undirected: {is_undirected}\")\n",
    "    dataset = pre_transform_in_memory(dataset, partial(compute_posenc_stats, pos_encoder=pos_encoder, is_undirected=is_undirected), show_progress=True)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    timestr = time.strftime('%H:%M:%S', time.gmtime(elapsed)) + f'{elapsed:.2f}'[-3:]\n",
    "    logging.info(f\"Done! Took {timestr}\")\n",
    "\n",
    "    # Set standard dataset train/val/test splits\n",
    "    if hasattr(dataset, 'split_idxs'):\n",
    "        set_dataset_splits(dataset, dataset.split_idxs)\n",
    "        delattr(dataset, 'split_idxs')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c54a9e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setting up the Dataset types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0001b9b",
   "metadata": {},
   "source": [
    "### Root Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac38118",
   "metadata": {},
   "source": [
    "We start by implementing the `RootNeighboursDataset`. It has the following methods:\n",
    "\n",
    "- `get()` returns the generated data.\n",
    "\n",
    "- `create_data()` returns the generated data, to be called at instantiation.\n",
    "\n",
    "- `mask_task()` returns the training, validation and testing masks that shall be used.\n",
    "\n",
    "- `generate_component()` returns one generated graph component, to be called in `create_data()`.\n",
    "\n",
    "- `initialize_constants()` returns a dictionary with various dataset parameters.\n",
    "\n",
    "- `generate_fold()` generates a single graph topology sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RootNeighboursDataset(object):\n",
    "\n",
    "    def __init__(self, seed: int, print_flag: bool = False):\n",
    "        super().__init__()\n",
    "        self.seed = seed\n",
    "        self.plot_flag = print_flag\n",
    "        self.generator = torch.Generator().manual_seed(seed)\n",
    "        self.constants_dict = self.initialize_constants()\n",
    "\n",
    "        self._data = self.create_data()\n",
    "\n",
    "    def get(self) -> Data:\n",
    "        return self._data\n",
    "\n",
    "    def create_data(self) -> Data:\n",
    "        # train, val, test\n",
    "        data_list = []\n",
    "        for num in range(self.constants_dict['NUM_COMPONENTS']):\n",
    "            data_list.append(self.generate_component())\n",
    "        return Batch.from_data_list(data_list)\n",
    "\n",
    "    def mask_task(self, num_nodes_per_fold: List[int]) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        num_nodes = sum(num_nodes_per_fold)\n",
    "        train_mask = torch.zeros(size=(num_nodes,), dtype=torch.bool)\n",
    "        val_mask = torch.zeros(size=(num_nodes,), dtype=torch.bool)\n",
    "        test_mask = torch.zeros(size=(num_nodes,), dtype=torch.bool)\n",
    "\n",
    "        train_mask[0] = True\n",
    "        val_mask[num_nodes_per_fold[0]] = True\n",
    "        test_mask[num_nodes_per_fold[0] + num_nodes_per_fold[1]] = True\n",
    "        return train_mask, val_mask, test_mask\n",
    "\n",
    "    def generate_component(self) -> Data:\n",
    "        data_per_fold, num_nodes_per_fold = [], []\n",
    "        for fold_idx in range(3):\n",
    "            data = self.generate_fold(eval=(fold_idx != 0))\n",
    "            num_nodes_per_fold.append(data.x.shape[0])\n",
    "            data_per_fold.append(data)\n",
    "\n",
    "        train_mask, val_mask, test_mask = self.mask_task(num_nodes_per_fold=num_nodes_per_fold)\n",
    "\n",
    "        batch = Batch.from_data_list(data_per_fold)\n",
    "        return Data(x=batch.x, edge_index=batch.edge_index, y=batch.y, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "    def initialize_constants(self) -> Dict[str, int]:\n",
    "        return {'NUM_COMPONENTS': 1000, 'MAX_HUBS': 3, 'MAX_1HOP_NEIGHBORS': 10, 'ADD_HUBS': 2, 'HUB_NEIGHBORS': 5, 'MAX_2HOP_NEIGHBORS': 3, 'NUM_FEATURES': 5}\n",
    "\n",
    "    def generate_fold(self, eval: bool) -> Data:\n",
    "        constant_dict = self.initialize_constants()\n",
    "        MAX_HUBS, MAX_1HOP_NEIGHBORS, ADD_HUBS, HUB_NEIGHBORS, MAX_2HOP_NEIGHBORS, NUM_FEATURES =\\\n",
    "            [constant_dict[key] for key in ['MAX_HUBS', 'MAX_1HOP_NEIGHBORS', 'ADD_HUBS', 'HUB_NEIGHBORS', 'MAX_2HOP_NEIGHBORS', 'NUM_FEATURES']]\n",
    "\n",
    "        assert MAX_HUBS + ADD_HUBS <= MAX_1HOP_NEIGHBORS\n",
    "        add_hubs = ADD_HUBS if eval else 0\n",
    "        num_hubs = torch.randint(1, MAX_HUBS + 1, size=(1,), generator=self.generator).item() + add_hubs\n",
    "        num_1hop_neighbors = torch.randint(MAX_HUBS + add_hubs, MAX_1HOP_NEIGHBORS + 1, size=(1,), generator=self.generator).item()\n",
    "        assert num_hubs <= num_1hop_neighbors\n",
    "\n",
    "        list_num_2hop_neighbors = torch.randint(1, MAX_2HOP_NEIGHBORS, size=(num_1hop_neighbors - num_hubs,), generator=self.generator).tolist()\n",
    "        list_num_2hop_neighbors = [HUB_NEIGHBORS] * num_hubs + list_num_2hop_neighbors\n",
    "\n",
    "        # 2 hop edge index\n",
    "        num_nodes = 1  # root node is 0\n",
    "        idx_1hop_neighbors = []\n",
    "        list_edge_index = []\n",
    "        for num_2hop_neighbors in list_num_2hop_neighbors:\n",
    "            idx_1hop_neighbors.append(num_nodes)\n",
    "            if num_2hop_neighbors > 0:\n",
    "                clique_edge_index = torch.tensor([[0] * num_2hop_neighbors, list(range(1, num_2hop_neighbors + 1))])\n",
    "                # clique_edge_index = torch.combinations(torch.arange(num_2hop_neighbors), r=2).T\n",
    "                list_edge_index.append(clique_edge_index + num_nodes)\n",
    "\n",
    "            num_nodes += num_2hop_neighbors + 1\n",
    "\n",
    "        # 1 hop edge index\n",
    "        idx_0hop = torch.tensor([0] * num_1hop_neighbors)\n",
    "        idx_1hop_neighbors = torch.tensor(idx_1hop_neighbors)\n",
    "        hubs = idx_1hop_neighbors[:num_hubs]\n",
    "        list_edge_index.append(torch.stack((idx_0hop, idx_1hop_neighbors), dim=0))\n",
    "        edge_index = torch.cat(list_edge_index, dim=1)\n",
    "\n",
    "        # undirect\n",
    "        edge_index_other_direction = torch.stack((edge_index[1], edge_index[0]), dim=0)\n",
    "        edge_index = torch.cat((edge_index_other_direction, edge_index), dim=1)\n",
    "\n",
    "        # features\n",
    "        x = 4 * torch.rand(size=(num_nodes, NUM_FEATURES), generator=self.generator) - 2\n",
    "\n",
    "        # labels\n",
    "        y = torch.zeros_like(x)\n",
    "        y[0] = torch.mean(x[hubs], dim=0)\n",
    "        return Data(x=x, edge_index=edge_index, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a84e9e9",
   "metadata": {},
   "source": [
    "### Cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b6fc46",
   "metadata": {},
   "source": [
    "In order to set up the `CyclesDataset`, we first need a function `make_undirected` which simply turns a directed graph into an undirected one by concatenating the inverse of the list of directed edges to itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f09190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_undirected(edge_index: Tensor) -> Tensor:\n",
    "    edge_index_other_direction = torch.stack((edge_index[1], edge_index[0]), dim=0)\n",
    "    edge_index = torch.cat((edge_index_other_direction, edge_index), dim=1)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dcd36c",
   "metadata": {},
   "source": [
    "Now, the other helper function we need is `create_cycle` which creates two cycles for each value of `cycle_size`. One is a standard 0 - ... - (n-1) - 0 cycle and the other additionally has a shortcut edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c65d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cycle(max_cycle: int) -> List[Data]:\n",
    "    data_list = []\n",
    "    for cycle_size in range(6, max_cycle + 1):\n",
    "        if cycle_size < (max_cycle + 1 - 6) / 3 + 6:\n",
    "            train_mask = torch.ones(size=(1,), dtype=torch.bool)\n",
    "            val_mask = torch.zeros(size=(1,), dtype=torch.bool)\n",
    "            test_mask = torch.zeros(size=(1,), dtype=torch.bool)\n",
    "        elif cycle_size < 2 * (max_cycle + 1 - 6) / 3 + 6:\n",
    "            train_mask = torch.zeros(size=(1,), dtype=torch.bool)\n",
    "            val_mask = torch.ones(size=(1,), dtype=torch.bool)\n",
    "            test_mask = torch.zeros(size=(1,), dtype=torch.bool)\n",
    "        else:\n",
    "            train_mask = torch.zeros(size=(1,), dtype=torch.bool)\n",
    "            val_mask = torch.zeros(size=(1,), dtype=torch.bool)\n",
    "            test_mask = torch.ones(size=(1,), dtype=torch.bool)\n",
    "\n",
    "        x = torch.ones(size=(cycle_size, 1))\n",
    "        edge_index1 = torch.tensor([list(range(cycle_size)),\n",
    "                                    list(range(1, cycle_size)) + [0]])\n",
    "        edge_index1 = make_undirected(edge_index=edge_index1)\n",
    "        edge_index2 = torch.tensor([[0, 1, 2] + list(range(3, cycle_size)),\n",
    "                                    [1, 2, 0] + list(range(4, cycle_size)) + [3]])\n",
    "        edge_index2 = make_undirected(edge_index=edge_index2)\n",
    "\n",
    "        data_list.append(Data(x=x, edge_index=edge_index1, y=torch.tensor([0], dtype=torch.long),\n",
    "                              train_mask=train_mask, val_mask=val_mask, test_mask=test_mask))\n",
    "        data_list.append(Data(x=x, edge_index=edge_index2, y=torch.tensor([1], dtype=torch.long),\n",
    "                              train_mask=train_mask, val_mask=val_mask, test_mask=test_mask))\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0488a3",
   "metadata": {},
   "source": [
    "The `CyclesDataset` class is then simply a function call to the `create_dataset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40882685",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclesDataset(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.data = create_cycle(max_cycle=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329429da",
   "metadata": {},
   "source": [
    "### Peptides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f02f6f0",
   "metadata": {},
   "source": [
    "The `PeptidesFunctionalDataset` deals with the classification of organic molecules' graphs as different classes of biological functions. It contains the following methods:\n",
    "\n",
    "- `_md5sum()` computes the md5 hash of the dataset to verify the file's integrity.\n",
    "\n",
    "- `download()` downloads the dataset from the internet.\n",
    "\n",
    "- `process()` converts each SMILE string (Simplified Molecular Input Line Entry System, a sort of text representation of a molecule's skeletal formula) of the dataset into a graph and applies an optional transformation.\n",
    "\n",
    "- `get_idx_split()` returns the pre-computed training, validation and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9521e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeptidesFunctionalDataset(InMemoryDataset):\n",
    "    def __init__(self, root='data', smiles2graph=smiles2graph,\n",
    "                 transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        PyG dataset of 15,535 peptides represented as their molecular graph\n",
    "        (SMILES) with 10-way multi-task binary classification of their\n",
    "        functional classes.\n",
    "        The goal is use the molecular representation of peptides instead\n",
    "        of amino acid sequence representation ('peptide_seq' field in the file,\n",
    "        provided for possible baseline benchmarking but not used here) to test\n",
    "        GNNs' representation capability.\n",
    "        The 10 classes represent the following functional classes (in order):\n",
    "            ['antifungal', 'cell_cell_communication', 'anticancer',\n",
    "            'drug_delivery_vehicle', 'antimicrobial', 'antiviral',\n",
    "            'antihypertensive', 'antibacterial', 'antiparasitic', 'toxic']\n",
    "        Args:\n",
    "            root (string): Root directory where the dataset should be saved.\n",
    "            smiles2graph (callable): A callable function that converts a SMILES\n",
    "                string into a graph object. We use the OGB featurization.\n",
    "                * The default smiles2graph requires rdkit to be installed *\n",
    "        \"\"\"\n",
    "\n",
    "        self.original_root = root\n",
    "        self.smiles2graph = smiles2graph\n",
    "        self.folder = join(root, 'peptides-functional')\n",
    "\n",
    "        self.url = 'https://www.dropbox.com/s/ol2v01usvaxbsr8/peptide_multi_class_dataset.csv.gz?dl=1'\n",
    "        self.version = '701eb743e899f4d793f0e13c8fa5a1b4'  # MD5 hash of the intended dataset file\n",
    "        self.url_stratified_split = 'https://www.dropbox.com/s/j4zcnx2eipuo0xz/splits_random_stratified_peptide.pickle?dl=1'\n",
    "        self.md5sum_stratified_split = '5a0114bdadc80b94fc7ae974f13ef061'\n",
    "\n",
    "        # Check version and update if necessary.\n",
    "        release_tag = join(self.folder, self.version)\n",
    "        if isdir(self.folder) and (not exists(release_tag)):\n",
    "            print(f\"{self.__class__.__name__} has been updated.\")\n",
    "            if input(\"Will you update the dataset now? (y/N)\\n\").lower() == 'y':\n",
    "                rmtree(self.folder)\n",
    "\n",
    "        super().__init__(self.folder, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return 'peptide_multi_class_dataset.csv.gz'\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'geometric_data_processed.pt'\n",
    "\n",
    "    def _md5sum(self, path):\n",
    "        hash_md5 = md5()\n",
    "        with open(path, 'rb') as f:\n",
    "            buffer = f.read()\n",
    "            hash_md5.update(buffer)\n",
    "        return hash_md5.hexdigest()\n",
    "\n",
    "    def download(self):\n",
    "        if decide_download(self.url):\n",
    "            path = download_url(self.url, self.raw_dir)\n",
    "            # Save to disk the MD5 hash of the downloaded file.\n",
    "            hash = self._md5sum(path)\n",
    "            if hash != self.version:\n",
    "                raise ValueError(\"Unexpected MD5 hash of the downloaded file\")\n",
    "            open(join(self.root, hash), 'w').close()\n",
    "            # Download train/val/test splits.\n",
    "            path_split1 = download_url(self.url_stratified_split, self.root)\n",
    "            assert self._md5sum(path_split1) == self.md5sum_stratified_split\n",
    "        else:\n",
    "            print('Stop download.')\n",
    "            exit(-1)\n",
    "\n",
    "    def process(self):\n",
    "        data_df = pd.read_csv(join(self.raw_dir,\n",
    "                                       'peptide_multi_class_dataset.csv.gz'))\n",
    "        smiles_list = data_df['smiles']\n",
    "\n",
    "        print('Converting SMILES strings into graphs...')\n",
    "        data_list = []\n",
    "        for i in tqdm(range(len(smiles_list))):\n",
    "            data = Data()\n",
    "\n",
    "            smiles = smiles_list[i]\n",
    "            graph = self.smiles2graph(smiles)\n",
    "\n",
    "            assert (len(graph['edge_feat']) == graph['edge_index'].shape[1])\n",
    "            assert (len(graph['node_feat']) == graph['num_nodes'])\n",
    "\n",
    "            data.__num_nodes__ = int(graph['num_nodes'])\n",
    "            data.edge_index = torch.from_numpy(graph['edge_index']).to(\n",
    "                torch.int64)\n",
    "            data.edge_attr = torch.from_numpy(graph['edge_feat']).to(\n",
    "                torch.int64)\n",
    "            data.x = torch.from_numpy(graph['node_feat']).to(torch.int64)\n",
    "            data.y = torch.Tensor([eval(data_df['labels'].iloc[i])])\n",
    "\n",
    "            data_list.append(data)\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            print('Applying pre_transform of graphs...')\n",
    "            data_list = [self.pre_transform(data) for data in tqdm(data_list)]\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "\n",
    "        print('Saving...')\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def get_idx_split(self):\n",
    "        \"\"\" Get dataset splits.\n",
    "        Returns:\n",
    "            Dict with 'train', 'val', 'test', splits indices.\n",
    "        \"\"\"\n",
    "        split_file = join(self.root,\n",
    "                              \"splits_random_stratified_peptide.pickle\")\n",
    "        with open(split_file, 'rb') as f:\n",
    "            splits = pickle.load(f)\n",
    "        split_dict = replace_numpy_with_torchtensor(splits)\n",
    "        return split_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05f18df",
   "metadata": {},
   "source": [
    "### Planetoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d74029",
   "metadata": {},
   "source": [
    "On the way to define the `Planetoid` dataset class, we start by coding `parse_index_file` which basically reads a file and returns a list of the integer values of each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17360982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_index_file(filename):\n",
    "    \"\"\"Code taken from https://github.com/Yujun-Yan/Heterophily_and_oversmoothing/blob/main/process.py#L18\"\"\n",
    "\n",
    "    Parse index file.\n",
    "    \"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf7f48",
   "metadata": {},
   "source": [
    "To load the dataset, we define the `full_load_citation` function which reads the files (features, labels, adjacency matrix, ...), storing the result in variables, and returning the dataset as a `Data` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401befb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_load_citation(dataset_str, raw_dir):\n",
    "    \"\"\"Code adapted from https://github.com/Yujun-Yan/Heterophily_and_oversmoothing/blob/main/process.py#L33\"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        path = join(raw_dir, \"ind.{}.{}\".format(dataset_str, names[i]))\n",
    "        with open(path, 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pickle.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pickle.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(join(raw_dir, \"ind.{}.test.index\".format(dataset_str)))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "    test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder) + 1)\n",
    "    tx_extended = scipy.sparse.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "    if len(test_idx_range_full) != len(test_idx_range):\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position, mark them\n",
    "        # Follow H2GCN code\n",
    "        tx_extended[test_idx_range - min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range - min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "        non_valid_samples = set(test_idx_range_full) - set(test_idx_range)\n",
    "    else:\n",
    "        non_valid_samples = set()\n",
    "    features = scipy.sparse.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = networkx.adjacency_matrix(networkx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "    non_valid_samples = list(non_valid_samples.union(set(list(np.where(labels.sum(1) == 0)[0]))))\n",
    "    labels = np.argmax(labels, axis=-1)\n",
    "\n",
    "    features = features.todense()\n",
    "\n",
    "    # Prepare in PyTorch Geometric Format\n",
    "    sparse_mx = scipy.sparse.coo_matrix(adj).astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    edge_index, _ = coalesce(indices, None, shape[0], shape[1])\n",
    "\n",
    "    # Remove self-loops\n",
    "    edge_index, _ = remove_self_loops(edge_index)\n",
    "    # Make the graph undirected\n",
    "    edge_index = to_undirected(edge_index)\n",
    "\n",
    "    assert (np.array_equal(np.unique(labels), np.arange(len(np.unique(labels)))))\n",
    "\n",
    "    features = torch.FloatTensor(features)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    non_valid_samples = torch.LongTensor(non_valid_samples)\n",
    "\n",
    "    return Data(x=features, edge_index=edge_index, y=labels, num_node_features=features.size(1),\n",
    "                non_valid_samples=non_valid_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da264523",
   "metadata": {},
   "source": [
    "Finally, the `Planetoid` dataset holds the data loaded at instantiation. Its `process` function can apply an optional transform to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec4ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Planetoid(InMemoryDataset):\n",
    "\n",
    "    def __init__(self, root: str, name: str,\n",
    "                 transform: Optional[Callable] = None,\n",
    "                 pre_transform: Optional[Callable] = None):\n",
    "        self.name = name\n",
    "\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "        data = self.get(0)\n",
    "        self.data, self.slices = self.collate([data])\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        return join(self.root, self.name, 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        return join(self.root, self.name, 'processed')\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        names = ['x', 'tx', 'allx', 'y', 'ty', 'ally', 'graph', 'test.index']\n",
    "        return [f'ind.{self.name.lower()}.{name}' for name in names]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        data = full_load_citation(self.name, self.raw_dir)\n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "        torch.save(self.collate([data]), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.name}()'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c68146d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Defining the DataSet class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9996f756",
   "metadata": {},
   "source": [
    "The `DataSet` class provides a container for the different datasets used in the paper. It has the following functionality:\n",
    "\n",
    "- `get_family()` tells us which of the types from the `DataSetFamily` class the DataSet object is.\n",
    "\n",
    "- `is_node_based()` returns `True` if the dataset is for node classification.\n",
    "\n",
    "- `not_synthetic()` returns `True` if the dataset is real, i.e. not synthetic.\n",
    "\n",
    "- `is_expressivity()` returns `True` if the dataset is for distinguishing graph structures, in our case that is only the Cycles dataset.\n",
    "\n",
    "- `clip_grad()` returns `True` if gradient clipping is required, in our case that is only the LRGB dataset.\n",
    "\n",
    "- `get_dataset_encoders()` tells us which encoder to use for the dataset, in our case only the LRGB dataset will get a special encoder.\n",
    "\n",
    "- `get_folds()` returns the fold indices to be used in n-fold training, i.e. how many different models shall be trained.\n",
    "\n",
    "- `load()` does the loading of the dataset from the data files and returns a list of Data-objects. Adjust the `root` and `name`, or `tu_dataset_name`, variable depending on the folder structure of where the data is located relative to this notebook.\n",
    "\n",
    "- `select_fold_and_split()` takes the data returned from `laod()` and splits it into the desired number of folds. It returns the folds within a `DatasetBySplit` object.\n",
    "\n",
    "- `get_metric_type()` tells us which from the `MetricType` to use as a metric for model performance.\n",
    "\n",
    "- `num_after_decimal()` returns the precision to use when displaying results.\n",
    "\n",
    "- `env_activation_type()` tells us which activation function to use in the model.\n",
    "\n",
    "- `gin_mlp_func()` returns an architecture for the MLP to be used for the model in the `WeightedGINConv` class.\n",
    "\n",
    "- `optimizer()` returns the optimizer to use when training the model.\n",
    "\n",
    "- `scheduler()` returns the scheduler to use when training the model.\n",
    "\n",
    "- `get_split_mask()` returns the training / validation / test masks (of nodes) to use for node classification tasks, or the entire graph for graph classification tasks.\n",
    "\n",
    "- `get_edge_ratio_node_mask()` is similar to `get_split_mask()` but only for node-level tasks.\n",
    "\n",
    "- `asserts()` checks that various assertions hold on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a170ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Enum):\n",
    "    \"\"\"\n",
    "        an object for the different datasets\n",
    "    \"\"\"\n",
    "    # heterophilic\n",
    "    roman_empire = auto()\n",
    "    amazon_ratings = auto()\n",
    "    minesweeper = auto()\n",
    "    tolokers = auto()\n",
    "    questions = auto()\n",
    "\n",
    "    # synthetic\n",
    "    root_neighbours = auto()\n",
    "    cycles = auto()\n",
    "\n",
    "    # social networks\n",
    "    imdb_binary = auto()\n",
    "    imdb_multi = auto()\n",
    "    reddit_binary = auto()\n",
    "    reddit_multi = auto()\n",
    "    \n",
    "    # proteins\n",
    "    enzymes = auto()\n",
    "    proteins = auto()\n",
    "    nci1 = auto()\n",
    "\n",
    "    # lrgb\n",
    "    func = auto()\n",
    "\n",
    "    # homophilic\n",
    "    cora = auto()\n",
    "    pubmed = auto()\n",
    "\n",
    "    @staticmethod\n",
    "    def from_string(s: str):\n",
    "        try:\n",
    "            return DataSet[s]\n",
    "        except KeyError:\n",
    "            raise ValueError()\n",
    "        \n",
    "    def get_family(self) -> DataSetFamily:\n",
    "        if self in [DataSet.roman_empire, DataSet.amazon_ratings, DataSet.minesweeper,\n",
    "                    DataSet.tolokers, DataSet.questions]:\n",
    "            return DataSetFamily.heterophilic\n",
    "        elif self in [DataSet.root_neighbours, DataSet.cycles]:\n",
    "            return DataSetFamily.synthetic\n",
    "        elif self in [DataSet.imdb_binary, DataSet.imdb_multi, DataSet.reddit_binary, DataSet.reddit_multi]:\n",
    "            return DataSetFamily.social_networks\n",
    "        elif self in [DataSet.enzymes, DataSet.proteins, DataSet.nci1]:\n",
    "            return DataSetFamily.proteins\n",
    "        elif self is DataSet.func:\n",
    "            return DataSetFamily.lrgb\n",
    "        elif self in [DataSet.cora, DataSet.pubmed]:\n",
    "            return DataSetFamily.homophilic\n",
    "        else:\n",
    "            raise ValueError(f'DataSet {self.name} not supported in dataloader')\n",
    "\n",
    "    def is_node_based(self) -> bool:\n",
    "        return self.get_family() in [DataSetFamily.heterophilic, DataSetFamily.homophilic] or self is DataSet.root_neighbours\n",
    "\n",
    "    def not_synthetic(self) -> bool:\n",
    "        return self.get_family() is not DataSetFamily.synthetic\n",
    "\n",
    "    def is_expressivity(self) -> bool:\n",
    "        return self is DataSet.cycles\n",
    "\n",
    "    def clip_grad(self) -> bool:\n",
    "        return self.get_family() is DataSetFamily.lrgb\n",
    "\n",
    "    def get_dataset_encoders(self):\n",
    "        if self.get_family() in [DataSetFamily.heterophilic, DataSetFamily.synthetic, DataSetFamily.social_networks,\n",
    "                                 DataSetFamily.proteins, DataSetFamily.homophilic]:\n",
    "            return DataSetEncoders.NONE\n",
    "        elif self is DataSet.func:\n",
    "            return DataSetEncoders.MOL\n",
    "        else:\n",
    "            raise ValueError(f'DataSet {self.name} not supported in get_dataset_encoders')\n",
    "\n",
    "    def get_folds(self, fold: int) -> List[int]:\n",
    "        if self.get_family() in [DataSetFamily.synthetic, DataSetFamily.lrgb]:\n",
    "            return list(range(1))\n",
    "        elif self.get_family() in [DataSetFamily.heterophilic, DataSetFamily.homophilic]:\n",
    "            return list(range(10))\n",
    "        elif self.get_family() in [DataSetFamily.social_networks, DataSetFamily.proteins]:\n",
    "            return [fold]\n",
    "        else:\n",
    "            raise ValueError(f'DataSet {self.name} not supported in dataloader')\n",
    "    \n",
    "    def load(self, seed: int, pos_enc: PosEncoder) -> List[Data]:\n",
    "        root = join(ROOT_DIR, 'datasets')\n",
    "        if self.get_family() is DataSetFamily.heterophilic:\n",
    "            name = self.name.replace('_', '-').capitalize()\n",
    "            dataset = [HeterophilousGraphDataset(root=root, name=name, transform=T.ToUndirected())[0]]\n",
    "        elif self.get_family() in [DataSetFamily.social_networks, DataSetFamily.proteins]:\n",
    "            tu_dataset_name = self.name.upper().replace('_', '-')\n",
    "            root = join(ROOT_DIR, 'datasets', tu_dataset_name)\n",
    "            dataset = torch.load(root + '.pt')\n",
    "        elif self is DataSet.root_neighbours:\n",
    "            dataset = [RootNeighboursDataset(seed=seed).get()]\n",
    "        elif self is DataSet.cycles:\n",
    "            dataset = CyclesDataset().data\n",
    "        elif self is DataSet.func:\n",
    "            dataset = PeptidesFunctionalDataset(root=root)\n",
    "            dataset = apply_transform(dataset=dataset, pos_encoder=pos_enc)\n",
    "        elif self.get_family() is DataSetFamily.homophilic:\n",
    "            dataset = [Planetoid(root=root, name=self.name, transform=T.NormalizeFeatures())[0]]\n",
    "        else:\n",
    "            raise ValueError(f'DataSet {self.name} not supported in dataloader')\n",
    "        return dataset\n",
    "\n",
    "    def select_fold_and_split(self, dataset: List[Data], num_fold: int) -> DatasetBySplit:\n",
    "        if self.get_family() is DataSetFamily.heterophilic:\n",
    "            dataset_copy = copy.deepcopy(dataset)\n",
    "            dataset_copy[0].train_mask = dataset_copy[0].train_mask[:, num_fold]\n",
    "            dataset_copy[0].val_mask = dataset_copy[0].val_mask[:, num_fold]\n",
    "            dataset_copy[0].test_mask = dataset_copy[0].test_mask[:, num_fold]\n",
    "            return DatasetBySplit(train=dataset_copy, val=dataset_copy, test=dataset_copy)\n",
    "        elif self.get_family() is DataSetFamily.synthetic:\n",
    "            return DatasetBySplit(train=dataset, val=dataset, test=dataset)\n",
    "        elif self.get_family() in [DataSetFamily.social_networks, DataSetFamily.proteins]:\n",
    "            tu_dataset_name = self.name.upper().replace('_', '-')\n",
    "            original_fold_dict = json.load(open(f'folds/{tu_dataset_name}_splits.json', \"r\"))[num_fold]\n",
    "            model_selection_dict = original_fold_dict['model_selection'][0]\n",
    "            split_dict = {'train': model_selection_dict['train'], 'val': model_selection_dict['validation'],\n",
    "                          'test': original_fold_dict['test']}\n",
    "            dataset_by_splits = [[dataset[idx] for idx in split_dict[split]] for split in DatasetBySplit._fields]\n",
    "            return DatasetBySplit(*dataset_by_splits)\n",
    "        elif self is DataSet.func:\n",
    "            split_idx = dataset.get_idx_split()\n",
    "            dataset_by_splits = [[dataset[idx] for idx in split_idx[split]] for split in DatasetBySplit._fields]\n",
    "            return DatasetBySplit(*dataset_by_splits)\n",
    "        elif self.get_family() is DataSetFamily.homophilic:\n",
    "            device = dataset[0].x.device\n",
    "            with np.load(f'folds/{self.name}_split_0.6_0.2_{num_fold}.npz') as folds_file:\n",
    "                train_mask = torch.tensor(folds_file['train_mask'], dtype=torch.bool, device=device)\n",
    "                val_mask = torch.tensor(folds_file['val_mask'], dtype=torch.bool, device=device)\n",
    "                test_mask = torch.tensor(folds_file['test_mask'], dtype=torch.bool, device=device)\n",
    "\n",
    "            setattr(dataset[0], 'train_mask', train_mask)\n",
    "            setattr(dataset[0], 'val_mask', val_mask)\n",
    "            setattr(dataset[0], 'test_mask', test_mask)\n",
    "\n",
    "            dataset[0].train_mask[dataset[0].non_valid_samples] = False\n",
    "            dataset[0].test_mask[dataset[0].non_valid_samples] = False\n",
    "            dataset[0].val_mask[dataset[0].non_valid_samples] = False\n",
    "            return DatasetBySplit(train=dataset, val=dataset, test=dataset)\n",
    "        else:\n",
    "            raise ValueError(f'NotImplemented')\n",
    "\n",
    "    def get_metric_type(self) -> MetricType:\n",
    "        if self.get_family() in [DataSetFamily.social_networks, DataSetFamily.proteins, DataSetFamily.homophilic] or self in [DataSet.roman_empire, DataSet.amazon_ratings, DataSet.cycles]:\n",
    "            return MetricType.ACCURACY\n",
    "        elif self in [DataSet.minesweeper, DataSet.tolokers, DataSet.questions]:\n",
    "            return MetricType.AUC_ROC\n",
    "        elif self is DataSet.root_neighbours:\n",
    "            return MetricType.MSE_MAE\n",
    "        elif self is DataSet.func:\n",
    "            return MetricType.MULTI_LABEL_AP\n",
    "        else:\n",
    "            raise ValueError(f'DataSet {self.name} not supported in dataloader')\n",
    "\n",
    "    def num_after_decimal(self) -> int:\n",
    "        return 4 if self.get_family() is DataSetFamily.lrgb else 2\n",
    "\n",
    "    def env_activation_type(self) -> ActivationType:\n",
    "        if self.get_family() in [DataSetFamily.heterophilic, DataSetFamily.lrgb]:\n",
    "            return ActivationType.GELU\n",
    "        else:\n",
    "            return ActivationType.RELU\n",
    "\n",
    "    def gin_mlp_func(self) -> Callable:\n",
    "        if self is DataSet.func:\n",
    "            def mlp_func(in_channels: int, out_channels: int, bias: bool):\n",
    "                return torch.nn.Sequential(torch.nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                                           torch.nn.ReLU(), torch.nn.Linear(out_channels, out_channels, bias=bias))\n",
    "        elif self.get_family() in [DataSetFamily.social_networks, DataSetFamily.proteins]:\n",
    "            def mlp_func(in_channels: int, out_channels: int, bias: bool):\n",
    "                return torch.nn.Sequential(torch.nn.Linear(in_channels, 2 * in_channels, bias=bias),\n",
    "                                           torch.nn.ReLU(), torch.nn.Linear(2 * in_channels, out_channels, bias=bias))\n",
    "        else:\n",
    "            def mlp_func(in_channels: int, out_channels: int, bias: bool):\n",
    "                return torch.nn.Sequential(torch.nn.Linear(in_channels, 2 * in_channels, bias=bias),\n",
    "                                           torch.nn.BatchNorm1d(2 * in_channels),\n",
    "                                           torch.nn.ReLU(), torch.nn.Linear(2 * in_channels, out_channels, bias=bias))\n",
    "        return mlp_func\n",
    "\n",
    "    def optimizer(self, model, lr: float, weight_decay: float):\n",
    "        if self.get_family() in [DataSetFamily.heterophilic, DataSetFamily.synthetic, DataSetFamily.social_networks,\n",
    "                                 DataSetFamily.proteins, DataSetFamily.homophilic]:\n",
    "            return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif self.get_family() is DataSetFamily.lrgb:\n",
    "            return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise ValueError(f'DataSet {self.name} not supported in dataloader')\n",
    "\n",
    "    def scheduler(self, optimizer, step_size: Optional[int], gamma: Optional[float], num_warmup_epochs: Optional[int],\n",
    "                  max_epochs: int):\n",
    "        if self.get_family() is DataSetFamily.lrgb:\n",
    "            assert num_warmup_epochs is not None, 'cosine_with_warmup_scheduler\\'s num_warmup_epochs is None'\n",
    "            assert max_epochs is not None, 'cosine_with_warmup_scheduler\\'s max_epochs is None'\n",
    "            return cosine_with_warmup_scheduler(optimizer=optimizer, num_warmup_epochs=num_warmup_epochs,\n",
    "                                                max_epoch=max_epochs)\n",
    "        elif self.get_family() in [DataSetFamily.social_networks, DataSetFamily.proteins]:\n",
    "            assert step_size is not None, 'StepLR\\'s step_size is None'\n",
    "            assert gamma is not None, 'StepLR\\'s gamma is None'\n",
    "            return torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=step_size, gamma=gamma)\n",
    "        elif self.get_family() in [DataSetFamily.heterophilic, DataSetFamily.synthetic, DataSetFamily.homophilic]:\n",
    "            return None\n",
    "        else:\n",
    "            raise ValueError(f'DataSet {self.name} not supported in dataloader')\n",
    "\n",
    "    def get_split_mask(self, data: Data, batch_size: int, split_mask_name: str) -> Tensor:\n",
    "        if hasattr(data, split_mask_name):\n",
    "            return getattr(data, split_mask_name)\n",
    "        elif self.is_node_based():\n",
    "            return torch.ones(size=(data.x.shape[0],), dtype=torch.bool)\n",
    "        else:\n",
    "            return torch.ones(size=(batch_size,), dtype=torch.bool)\n",
    "\n",
    "    def get_edge_ratio_node_mask(self, data: Data, split_mask_name: str) -> Tensor:\n",
    "        if hasattr(data, split_mask_name):\n",
    "            return getattr(data, split_mask_name)\n",
    "        else:\n",
    "            return torch.ones(size=(data.x.shape[0],), dtype=torch.bool)\n",
    "\n",
    "    def asserts(self, args):\n",
    "        # model\n",
    "        assert not(self.is_node_based()) or args['pool'] is Pool.NONE, \"Node based datasets have no pooling\"\n",
    "        assert not(self.is_node_based()) or args['batch_norm'] is False, \"Node based dataset cannot have batch norm\"\n",
    "        assert not(not(self.is_node_based()) and args['pool'] is Pool.NONE), \"Graph based datasets need pooling\"\n",
    "        assert args['env_model_type'] is not ModelType.LIN, \"The environment net can't be linear\"\n",
    "\n",
    "        # dataset dependant parameters\n",
    "        assert self.get_family() in [DataSetFamily.social_networks, DataSetFamily.proteins] or args['fold'] is None, 'social networks and protein datasets are the only ones to use fold'\n",
    "        assert self.get_family() not in [DataSetFamily.social_networks, DataSetFamily.proteins] or args['fold'] is not None, 'social networks and protein datasets must specify fold'\n",
    "        assert self.get_family() is DataSetFamily.proteins or self.get_family() is DataSetFamily.social_networks or (args['step_size'] is None and args['gamma'] is None), 'proteins datasets are the only ones to use step_size and gamma'\n",
    "        assert self.get_family() is DataSetFamily.lrgb or (args['num_warmup_epochs'] is None), 'lrgb datasets are the only ones to use num_warmup_epochs'\n",
    "        # encoders\n",
    "        assert self.get_family() is DataSetFamily.lrgb or (args['pos_enc'] is PosEncoder.NONE), 'lrgb datasets are the only ones to use pos_enc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57ce46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setting up the experiment parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46689965",
   "metadata": {},
   "source": [
    "Since we're running the code inside a jupyter notebook, it is easier to just use a dictionary instead of an argument parser. It is implemented as a function so that we can define default values in the parameters of `create_args`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e117bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_args(\n",
    "    dataset: DataSet.from_string=DataSet.roman_empire,\n",
    "    pool: Pool.from_string=Pool.NONE,\n",
    "\n",
    "    # gumbel\n",
    "    learn_temp=False,\n",
    "    temp_model_type: ModelType.from_string=ModelType.LIN,\n",
    "    tau0: float=0.5,\n",
    "    temp: float=0.01,\n",
    "\n",
    "    # optimization\n",
    "    max_epochs: int=3000,\n",
    "    batch_size: int=32,\n",
    "    lr: float=1e-3,\n",
    "    dropout: float=0.2,\n",
    "\n",
    "    # env cls parameters\n",
    "    env_model_type: ModelType.from_string=ModelType.MEAN_GNN,\n",
    "    env_num_layers: int=3,\n",
    "    env_dim: int=128,\n",
    "    skip=False,\n",
    "    batch_norm=False,\n",
    "    layer_norm=False,\n",
    "    dec_num_layers: int=1,\n",
    "    pos_enc: PosEncoder.from_string=PosEncoder.NONE,\n",
    "\n",
    "    # policy cls parameters\n",
    "    act_model_type: ModelType.from_string=ModelType.MEAN_GNN,\n",
    "    act_num_layers: int=1,\n",
    "    act_dim: int=16,\n",
    "\n",
    "    # reproduce\n",
    "    seed: int=0,\n",
    "    gpu: int=None, # In the original argument parser, there is no default value defined here\n",
    "\n",
    "    # dataset dependant parameters\n",
    "    fold: int=None,\n",
    "\n",
    "    # optimizer and scheduler\n",
    "    weight_decay: float=0.0,\n",
    "    ## for steplr scheduler only\n",
    "    step_size: int=None,\n",
    "    gamma: float=None,\n",
    "    ## for cosine with warmup scheduler only\n",
    "    num_warmup_epochs: int=None\n",
    "):\n",
    "    return {\n",
    "        'dataset' : dataset,\n",
    "        'pool' : pool,\n",
    "        'learn_temp' : learn_temp,\n",
    "        'temp_model_type' : temp_model_type,\n",
    "        'tau0' : tau0,\n",
    "        'temp' : temp,\n",
    "        'max_epochs' : max_epochs,\n",
    "        'batch_size' : batch_size,\n",
    "        'lr' : lr,\n",
    "        'dropout' : dropout,\n",
    "        'env_model_type' : env_model_type,\n",
    "        'env_num_layers' : env_num_layers,\n",
    "        'env_dim' : env_dim,\n",
    "        'skip' : skip,\n",
    "        'batch_norm' : batch_norm,\n",
    "        'layer_norm' : layer_norm,\n",
    "        'dec_num_layers' : dec_num_layers,\n",
    "        'pos_enc' : pos_enc,\n",
    "        'act_model_type' : act_model_type,\n",
    "        'act_num_layers' : act_num_layers,\n",
    "        'act_dim' : act_dim,\n",
    "        'seed' : seed,\n",
    "        'gpu' : gpu,\n",
    "        'fold' : fold,\n",
    "        'weight_decay' : weight_decay,\n",
    "        'step_size' : step_size,\n",
    "        'gamma' : gamma,\n",
    "        'num_warmup_epochs' : num_warmup_epochs\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2e0259",
   "metadata": {},
   "source": [
    "Now we just create this dictionary as `ARGS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb5370",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGS = create_args(dataset=DataSet.root_neighbours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04092f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setting up the CoGNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b5d0fe",
   "metadata": {},
   "source": [
    "Before being able to implement the `CoGNN`, we first need to implement this class which will be an optional functionality. It is essentially a model that is constructed according to a `GumbelArgs` instance and, for a given graph computes the temperature dynamically while ensuring that it never reaches infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bca721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempSoftPlus(torch.nn.Module):\n",
    "    def __init__(self, gumbel_args: GumbelArgs, env_dim: int):\n",
    "        super(TempSoftPlus, self).__init__()\n",
    "        model_list = gumbel_args.temp_model_type.get_component_list(in_dim=env_dim, hidden_dim=env_dim, out_dim=1, num_layers=1,\n",
    "                                                                    bias=False, edges_required=False,\n",
    "                                                                    gin_mlp_func=gumbel_args.gin_mlp_func)\n",
    "        self.linear_model = torch.nn.ModuleList(model_list)\n",
    "        self.softplus = torch.nn.Softplus(beta=1)\n",
    "        self.tau0 = gumbel_args.tau0\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj, edge_attr: Tensor):\n",
    "        x = self.linear_model[0](x=x, edge_index=edge_index,edge_attr = edge_attr)\n",
    "        x = self.softplus(x) + self.tau0\n",
    "        temp = x.pow_(-1)\n",
    "        return temp.masked_fill_(temp == float('inf'), 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e62401",
   "metadata": {},
   "source": [
    "Now we are ready to implement the `CoGNN` architecture. Something that is worth highlighting is the fact that we have two `ActionNet`, i.e. policies, namely `self.in_act_net` and `self.out_act_net` which decide whether to receive and broadcast, respectively. In other words, we do not have a mapping to the 4 possible states `Standard`, `Broadcast`, `Receive`, and `Isolate` but rather each of the decision to receive and broadcast are independent. Moreover, there are separate encoders of the edges for the environment and the action networks: `self.env_bond_encoder`  and `self.act_bond_encoder`.\n",
    "\n",
    "Overall, the `CoGNN` class follows the typical design pattern of a `torch.nn.Module` model, i.e. its `forward` function computes the model's output. In particular, it can track edge ratio statistics, then it encodes the edges using the two encoder mentioned before. Afterwards, it encodes the node (in case no particular encoder shall be used it applies dropout and computes the activation) and applies layer normalization to each node's features.\n",
    "\n",
    "Then the message passing begins with the number of message passing rounds defined by `env_args.num_layers`: The two policies compute the logit-distribution tensors (receive, no receive) and (broadcast, no broadcast) for each node from which, for a given (learned) temperature, we compute the actions using a hard Gumbel Softmax (the variable names `in_probs` and `out_probs` are somewhat misleading as the value for each edge is either keep or discard, no in between values). The call to `create_edge_weight` now constructs the graph as determined by the policy for that message passing-layer.\n",
    "\n",
    "The next step is to perform the actual message passing, which is handled by the environment network `self.env_net` and thus in extension by the model chosen in `env_args.load_net()`, to whose output we apply dropout and an activation function. Depending on whether we chose to track the edge ratio, the information is now appended to the statistics, and thereafter we can optionally (hyperparameter) enable a residual connection to keep more of a node's original features.\n",
    "\n",
    "The final result is then obtained by performing a layer normalization, pooling and running the output through a decoder (usually a linear classifier) and adding this message passing layer's output to the previous message passing layer's output. Then again some statistics can be collected and the forward pass returns the resulting features and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoGNN(torch.nn.Module):\n",
    "    def __init__(self, gumbel_args: GumbelArgs, env_args: EnvArgs, action_args: ActionNetArgs, pool: Pool):\n",
    "        super(CoGNN, self).__init__()\n",
    "        self.env_args = env_args\n",
    "        self.learn_temp = gumbel_args.learn_temp\n",
    "        if gumbel_args.learn_temp:\n",
    "            self.temp_model = TempSoftPlus(gumbel_args=gumbel_args, env_dim=env_args.env_dim)\n",
    "        self.temp = gumbel_args.temp\n",
    "\n",
    "        self.num_layers = env_args.num_layers\n",
    "        self.env_net = env_args.load_net()\n",
    "        self.use_encoders = env_args.dataset_encoders.use_encoders()\n",
    "\n",
    "        layer_norm_cls = torch.nn.LayerNorm if env_args.layer_norm else torch.nn.Identity\n",
    "        self.hidden_layer_norm = layer_norm_cls(env_args.env_dim)\n",
    "        self.skip = env_args.skip\n",
    "        self.dropout = torch.nn.Dropout(p=env_args.dropout)\n",
    "        self.drop_ratio = env_args.dropout\n",
    "        self.act = env_args.act_type.get()\n",
    "        self.in_act_net = ActionNet(action_args=action_args)\n",
    "        self.out_act_net = ActionNet(action_args=action_args)\n",
    "\n",
    "        # Encoder types\n",
    "        self.dataset_encoder = env_args.dataset_encoders\n",
    "        self.env_bond_encoder = self.dataset_encoder.edge_encoder(emb_dim=env_args.env_dim, model_type=env_args.model_type)\n",
    "        self.act_bond_encoder = self.dataset_encoder.edge_encoder(emb_dim=action_args.hidden_dim, model_type=action_args.model_type)\n",
    "\n",
    "        # Pooling function to generate whole-graph embeddings\n",
    "        self.pooling = pool.get()\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj, pestat, edge_attr: OptTensor = None, batch: OptTensor = None,\n",
    "                edge_ratio_node_mask: OptTensor = None) -> Tuple[Tensor, Tensor]:\n",
    "        result = 0\n",
    "\n",
    "        calc_stats = edge_ratio_node_mask is not None\n",
    "        if calc_stats:\n",
    "            edge_ratio_edge_mask = edge_ratio_node_mask[edge_index[0]] & edge_ratio_node_mask[edge_index[1]]\n",
    "            edge_ratio_list = []\n",
    "\n",
    "        # bond encode\n",
    "        if edge_attr is None or self.env_bond_encoder is None:\n",
    "            env_edge_embedding = None\n",
    "        else:\n",
    "            env_edge_embedding = self.env_bond_encoder(edge_attr)\n",
    "        if edge_attr is None or self.act_bond_encoder is None:\n",
    "            act_edge_embedding = None\n",
    "        else:\n",
    "            act_edge_embedding = self.act_bond_encoder(edge_attr)\n",
    "\n",
    "        # node encode  \n",
    "        x = self.env_net[0](x, pestat)  # (N, F) encoder\n",
    "        if not self.use_encoders:\n",
    "            x = self.dropout(x)\n",
    "            x = self.act(x)\n",
    "\n",
    "        for gnn_idx in range(self.num_layers):\n",
    "            x = self.hidden_layer_norm(x)\n",
    "\n",
    "            # action\n",
    "            in_logits = self.in_act_net(x=x, edge_index=edge_index, env_edge_attr=env_edge_embedding, act_edge_attr=act_edge_embedding)  # (N, 2)\n",
    "            out_logits = self.out_act_net(x=x, edge_index=edge_index, env_edge_attr=env_edge_embedding, act_edge_attr=act_edge_embedding)  # (N, 2)\n",
    "\n",
    "            temp = self.temp_model(x=x, edge_index=edge_index, edge_attr=env_edge_embedding) if self.learn_temp else self.temp\n",
    "            in_probs = torch.nn.functional.gumbel_softmax(logits=in_logits, tau=temp, hard=True)\n",
    "            out_probs = torch.nn.functional.gumbel_softmax(logits=out_logits, tau=temp, hard=True)\n",
    "            edge_weight = self.create_edge_weight(edge_index=edge_index, keep_in_prob=in_probs[:, 0], keep_out_prob=out_probs[:, 0])\n",
    "\n",
    "            # environment\n",
    "            out = self.env_net[1 + gnn_idx](x=x, edge_index=edge_index, edge_weight=edge_weight, edge_attr=env_edge_embedding)\n",
    "            out = self.dropout(out)\n",
    "            out = self.act(out)\n",
    "\n",
    "            if calc_stats:\n",
    "                edge_ratio = edge_weight[edge_ratio_edge_mask].sum() / edge_weight[edge_ratio_edge_mask].shape[0]\n",
    "                edge_ratio_list.append(edge_ratio.item())\n",
    "\n",
    "            if self.skip:\n",
    "                x = x + out\n",
    "            else:\n",
    "                x = out\n",
    "\n",
    "        x = self.hidden_layer_norm(x)\n",
    "        x = self.pooling(x, batch=batch)\n",
    "        x = self.env_net[-1](x)  # decoder\n",
    "        result = result + x\n",
    "\n",
    "        if calc_stats:\n",
    "            edge_ratio_tensor = torch.tensor(edge_ratio_list, device=x.device)\n",
    "        else:\n",
    "            edge_ratio_tensor = -1 * torch.ones(size=(self.num_layers,), device=x.device)\n",
    "        return result, edge_ratio_tensor\n",
    "\n",
    "    def create_edge_weight(self, edge_index: Adj, keep_in_prob: Tensor, keep_out_prob: Tensor) -> Tensor:\n",
    "        u, v = edge_index\n",
    "        edge_in_prob = keep_in_prob[v]\n",
    "        edge_out_prob = keep_out_prob[u]\n",
    "        return edge_in_prob * edge_out_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a688e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setting up the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f05732",
   "metadata": {},
   "source": [
    "The `Experiment` class is designed to handle all the functionalities required for the experiment. It contains the training and testing of our model as defined by the arguments `ARGS`. Compared to the authors' code, we have changed only that we are not using an argument parser but instead a dictionary. \n",
    "\n",
    "During instantiation of an `Experiment` object, we set all the attributes drived from the `ARGS` and define the device that the experiment shall be run upon. Moreover, we define a performance metric, the decimal precision and loss function to be used. There are also some assertations verified.\n",
    "\n",
    "The `Experiment` class implements several methods:\n",
    "\n",
    "- `single_fold()` instantiates a `CoGNN` as well as the optimizer and scheduler, then it calls `train_and_test()` to perform training and testing of the CoGNN. Thereafter it returns the performance metrics.\n",
    "\n",
    "- `train_and_test()` essentially contains the training and testing structure. It sets up the dataloaders and instantiates the metric class. The training loop operates by the following logic per epoch: (1) `train()` on the training set with evaluation on the training set, (2) `test()` on the training set, (3) `test()` on the validation set and on the test set, (4) store the metrics, optionally perform a scheduler step. At the very end for non-synthetic datasets we `test()` it again on the test dataset.\n",
    "\n",
    "- `train()` performs a batch-based training step with backpropagation and an optimizer step.\n",
    "\n",
    "- `test()` evaluates the model by computing the outputs of a dataset and applying the loss function to determine the losses and performance metric scores which it then returns.\n",
    "\n",
    "- `run()` at a high level view is the core script of the experiment. It loads the dataset into memory, splits them into folds, contains an n-fold training loop which makes a call to `single_fold()` where the model training and evaluation happens, then computes and outputs the experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a55ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(object):\n",
    "    def __init__(self, args: dict): # args type used to be Namespace\n",
    "        super().__init__()\n",
    "        for key, value in args.items(): # Adjusted loop\n",
    "            setattr(self, key, value)\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        set_seed(seed=self.seed)\n",
    "\n",
    "        # parameters\n",
    "        self.metric_type = self.dataset.get_metric_type()\n",
    "        self.decimal = self.dataset.num_after_decimal()\n",
    "        self.task_loss = self.metric_type.get_task_loss()\n",
    "\n",
    "        # asserts\n",
    "        self.dataset.asserts(args)\n",
    "\n",
    "    def run(self) -> Tuple[Tensor, Tensor]:\n",
    "        dataset = self.dataset.load(seed=self.seed, pos_enc=self.pos_enc)\n",
    "        if self.metric_type.is_multilabel():\n",
    "            dataset.data.y = dataset.data.y.to(dtype=torch.float)\n",
    "\n",
    "        folds = self.dataset.get_folds(fold=self.fold)\n",
    "\n",
    "        # locally used parameters\n",
    "        out_dim = self.metric_type.get_out_dim(dataset=dataset)\n",
    "        gin_mlp_func = self.dataset.gin_mlp_func()\n",
    "        env_act_type = self.dataset.env_activation_type()\n",
    "\n",
    "        # named tuples\n",
    "        gumbel_args = GumbelArgs(learn_temp=self.learn_temp, temp_model_type=self.temp_model_type, tau0=self.tau0,\n",
    "                                 temp=self.temp, gin_mlp_func=gin_mlp_func)\n",
    "        env_args =  EnvArgs(model_type=self.env_model_type, num_layers=self.env_num_layers, env_dim=self.env_dim,\n",
    "                            layer_norm=self.layer_norm, skip=self.skip, batch_norm=self.batch_norm, dropout=self.dropout,\n",
    "                            act_type=env_act_type, metric_type=self.metric_type, in_dim=dataset[0].x.shape[1], out_dim=out_dim,\n",
    "                            gin_mlp_func=gin_mlp_func, dec_num_layers=self.dec_num_layers, pos_enc=self.pos_enc,\n",
    "                            dataset_encoders=self.dataset.get_dataset_encoders())\n",
    "        action_args = ActionNetArgs(model_type=self.act_model_type, num_layers=self.act_num_layers,\n",
    "                                    hidden_dim=self.act_dim, dropout=self.dropout, act_type=ActivationType.RELU,\n",
    "                                    env_dim=self.env_dim, gin_mlp_func=gin_mlp_func)\n",
    "\n",
    "        # folds\n",
    "        metrics_list = []\n",
    "        edge_ratios_list = []\n",
    "        for num_fold in folds:\n",
    "            set_seed(seed=self.seed)\n",
    "            dataset_by_split = self.dataset.select_fold_and_split(num_fold=num_fold, dataset=dataset)\n",
    "            best_losses_n_metrics, edge_ratios =\\\n",
    "                self.single_fold(dataset_by_split=dataset_by_split, gumbel_args=gumbel_args, env_args=env_args,\n",
    "                                 action_args=action_args, num_fold=num_fold)\n",
    "\n",
    "            # print final\n",
    "            print_str = f'Fold {num_fold}/{len(folds)}'\n",
    "            for name in best_losses_n_metrics._fields:\n",
    "                print_str += f\",{name}={round(getattr(best_losses_n_metrics, name), self.decimal)}\"\n",
    "            metrics_list.append(best_losses_n_metrics.get_fold_metrics())\n",
    "\n",
    "            if edge_ratios is not None:\n",
    "                edge_ratios_list.append(edge_ratios)\n",
    "\n",
    "        metrics_matrix = torch.stack(metrics_list, dim=0)  # (F, 3)\n",
    "        metrics_mean = torch.mean(metrics_matrix, dim=0).tolist()  # (3,)\n",
    "        if len(edge_ratios_list) > 0:\n",
    "            edge_ratios = torch.mean(torch.stack(edge_ratios_list, dim=0), dim=0)\n",
    "        else:\n",
    "            edge_ratios = None\n",
    "\n",
    "        # prints\n",
    "        print(f'Final Rewired train={round(metrics_mean[0], self.decimal)},'\n",
    "              f'val={round(metrics_mean[1], self.decimal)},'\n",
    "              f'test={round(metrics_mean[2], self.decimal)}')\n",
    "        if len(folds) > 1:\n",
    "            metrics_std = torch.std(metrics_matrix, dim=0).tolist()  # (3,)\n",
    "            print(f'Final Rewired train={round(metrics_mean[0], self.decimal)}+-{round(metrics_std[0], self.decimal)},'\n",
    "                  f'val={round(metrics_mean[1], self.decimal)}+-{round(metrics_std[1], self.decimal)},'\n",
    "                  f'test={round(metrics_mean[2], self.decimal)}+-{round(metrics_std[2], self.decimal)}')\n",
    "    \n",
    "        return metrics_mean, edge_ratios\n",
    "            \n",
    "    def single_fold(self, dataset_by_split: DatasetBySplit, gumbel_args: GumbelArgs, env_args: EnvArgs,\n",
    "                    action_args: ActionNetArgs, num_fold: int) -> Tuple[LossesAndMetrics, OptTensor]:\n",
    "        model = CoGNN(gumbel_args=gumbel_args, env_args=env_args, action_args=action_args,\n",
    "                      pool=self.pool).to(device=self.device)\n",
    "\n",
    "        optimizer = self.dataset.optimizer(model=model, lr=self.lr, weight_decay=self.weight_decay)\n",
    "        scheduler = self.dataset.scheduler(optimizer=optimizer, step_size=self.step_size, gamma=self.gamma,\n",
    "                                           num_warmup_epochs=self.num_warmup_epochs, max_epochs=self.max_epochs)\n",
    "\n",
    "        with tqdm.tqdm(total=self.max_epochs, file=sys.stdout) as pbar:\n",
    "            best_losses_n_metrics, edge_ratios = self.train_and_test(dataset_by_split=dataset_by_split, model=model, optimizer=optimizer,\n",
    "                                                                     scheduler=scheduler, pbar=pbar, num_fold=num_fold)\n",
    "        return best_losses_n_metrics, edge_ratios\n",
    "\n",
    "    def train_and_test(self, dataset_by_split: DatasetBySplit, model, optimizer, scheduler, pbar, num_fold: int)\\\n",
    "            -> Tuple[LossesAndMetrics, OptTensor]:\n",
    "        train_loader = DataLoader(dataset_by_split.train, batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(dataset_by_split.val, batch_size=self.batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(dataset_by_split.test, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        best_losses_n_metrics = self.metric_type.get_worst_losses_n_metrics()\n",
    "        for epoch in range(self.max_epochs):\n",
    "            self.train(train_loader=train_loader, model=model, optimizer=optimizer)\n",
    "            train_loss, train_metric, _ = self.test(loader=train_loader, model=model, split_mask_name='train_mask', calc_edge_ratio=False)\n",
    "            if self.dataset.is_expressivity():\n",
    "                val_loss, val_metric = train_loss, train_metric\n",
    "                test_loss, test_metric = train_loss, train_metric\n",
    "            else:\n",
    "                val_loss, val_metric, _ = self.test(loader=val_loader, model=model, split_mask_name='val_mask', calc_edge_ratio=False)\n",
    "                test_loss, test_metric, _ = self.test(loader=test_loader, model=model, split_mask_name='test_mask', calc_edge_ratio=False)\n",
    "\n",
    "            losses_n_metrics = LossesAndMetrics(train_loss=train_loss, val_loss=val_loss, test_loss=test_loss,\n",
    "                                                train_metric=train_metric, val_metric=val_metric, test_metric=test_metric)\n",
    "            if scheduler is not None:\n",
    "                scheduler.step(losses_n_metrics.val_metric)\n",
    "\n",
    "            # best metrics\n",
    "            if self.metric_type.src_better_than_other(src=losses_n_metrics.val_metric, other=best_losses_n_metrics.val_metric):\n",
    "                best_losses_n_metrics = losses_n_metrics\n",
    "\n",
    "            # prints\n",
    "            log_str = f'Split: {num_fold}, epoch: {epoch}'\n",
    "            for name in losses_n_metrics._fields:\n",
    "                log_str += f\",{name}={round(getattr(losses_n_metrics, name), self.decimal)}\"\n",
    "            log_str += f\"({round(best_losses_n_metrics.test_metric, self.decimal)})\"\n",
    "            pbar.set_description(log_str)\n",
    "            pbar.update(n=1)\n",
    "\n",
    "        edge_ratios = None\n",
    "        if self.dataset.not_synthetic():\n",
    "            _, _, edge_ratios = self.test(loader=test_loader, model=model, split_mask_name='test_mask', calc_edge_ratio=True)\n",
    "\n",
    "        return best_losses_n_metrics, edge_ratios\n",
    "\n",
    "    def train(self, train_loader, model, optimizer):\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:\n",
    "            if self.batch_norm and (data.x.shape[0] == 1 or data.num_graphs == 1):\n",
    "                continue\n",
    "            optimizer.zero_grad()\n",
    "            node_mask = self.dataset.get_split_mask(data=data, batch_size=data.num_graphs,\n",
    "                                                    split_mask_name='train_mask').to(self.device)\n",
    "            edge_attr = data.edge_attr\n",
    "            if data.edge_attr is not None:\n",
    "                edge_attr = edge_attr.to(device=self.device)\n",
    "\n",
    "            # forward\n",
    "            scores, _ =\\\n",
    "                model(data.x.to(device=self.device), edge_index=data.edge_index.to(device=self.device),\n",
    "                      batch=data.batch.to(device=self.device), edge_attr=edge_attr, edge_ratio_node_mask=None,\n",
    "                      pestat=self.pos_enc.get_pe(data=data, device=self.device))\n",
    "            train_loss = self.task_loss(scores[node_mask], data.y.to(device=self.device)[node_mask])\n",
    "\n",
    "            # backward\n",
    "            train_loss.backward()\n",
    "            if self.dataset.clip_grad():\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "\n",
    "    def test(self, loader, model, split_mask_name: str, calc_edge_ratio: bool)\\\n",
    "            -> Tuple[float, Any, Tensor]:\n",
    "        model.eval()\n",
    "\n",
    "        total_loss, total_metric, total_edge_ratios = 0, 0, 0\n",
    "        total_scores = np.empty(shape=(0, model.env_args.out_dim))\n",
    "        total_y = None\n",
    "        for data in loader:\n",
    "            if self.batch_norm and (data.x.shape[0] == 1 or data.num_graphs == 1):\n",
    "                continue\n",
    "            node_mask = self.dataset.get_split_mask(data=data, batch_size=data.num_graphs, split_mask_name=split_mask_name).to(device=self.device)\n",
    "            if calc_edge_ratio:\n",
    "                edge_ratio_node_mask = self.dataset.get_edge_ratio_node_mask(data=data, split_mask_name=split_mask_name).to(device=self.device)\n",
    "            else:\n",
    "                edge_ratio_node_mask = None\n",
    "            edge_attr = data.edge_attr\n",
    "            if data.edge_attr is not None:\n",
    "                edge_attr = edge_attr.to(device=self.device)\n",
    "\n",
    "            # forward\n",
    "            scores, edge_ratios =\\\n",
    "                model(data.x.to(device=self.device), edge_index=data.edge_index.to(device=self.device),\n",
    "                      edge_attr=edge_attr, batch=data.batch.to(device=self.device),\n",
    "                      edge_ratio_node_mask=edge_ratio_node_mask,\n",
    "                      pestat=self.pos_enc.get_pe(data=data, device=self.device))\n",
    "            \n",
    "            eval_loss = self.task_loss(scores, data.y.to(device=self.device))\n",
    "\n",
    "            # analytics\n",
    "            total_scores = np.concatenate((total_scores, scores[node_mask].detach().cpu().numpy()))\n",
    "            if total_y is None:\n",
    "                total_y = data.y.to(device=self.device)[node_mask].detach().cpu().numpy()\n",
    "            else:\n",
    "                total_y = np.concatenate((total_y, data.y.to(device=self.device)[node_mask].detach().cpu().numpy()))\n",
    "\n",
    "            total_loss += eval_loss.item() * data.num_graphs\n",
    "            total_edge_ratios += edge_ratios * data.num_graphs\n",
    "\n",
    "        metric = self.metric_type.apply_metric(scores=total_scores, target=total_y)\n",
    "\n",
    "        loss = total_loss / len(loader.dataset)\n",
    "        edge_ratios = total_edge_ratios / len(loader.dataset)\n",
    "        return loss, metric, edge_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de8f122",
   "metadata": {},
   "source": [
    "The last thing we have to do now is execute the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b041c5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if ARGS['gpu'] is not None:\n",
    "#    set_device(ARGS['gpu'])\n",
    "\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "if GPU_AVAILABLE:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "Experiment(args=ARGS).run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AdvancedTopicsInMachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
