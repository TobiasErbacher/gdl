{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cooperative Graph Neural Networks ([CoGNN](https://doi.org/10.48550/arXiv.2310.01267))\n",
    "\n",
    "This part was adapted by Tobias Erbacher from the [authors' github](https://github.com/benfinkelshtein/CoGNN/tree/main). We recommend to run this on a GPU service like [Google Colab](https://colab.research.google.com/). The goal is to modify the original authors' code to work with our codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "To ensure we are using the same modules as the authors, we need to install the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch==2.0.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#%pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
    "#%pip install torch-geometric==2.3.0\n",
    "#%pip install torchmetrics ogb rdkit\n",
    "#%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute this notebook, we will assume that the datasets are already installed. If you cloned our github repository, you will be able to find them in [this folder](https://github.com/TobiasErbacher/gdl/tree/main/replication/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this notebook less crowded, some class definitions have been moved to other files. The overview here is to get an idea of the library dependencies. The file `architectures.py` will perform the following imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from `torch` import `Tensor, cat`\n",
    "- from `torch.nn` import `Linear, Parameter`\n",
    "- from `typing` import `Callable, List`\n",
    "- from `torch_geometric.typing` import `OptTensor, Adj`\n",
    "- from `torch_geometric.nn.conv` import `MessagePassing`\n",
    "- from `torch_geometric.nn.conv.gcn_conv` import `gcn_norm`\n",
    "- from `torch_geometric.utils` import `remove_self_loops, add_remaining_self_loops`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the type classes will be bundled in the file `type_classes.py` and perform the following imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from `torch` import `from_numpy, tensor`\n",
    "- from `torch.nn` import `Module, ReLU, GELU, CrossEntropyLoss`\n",
    "- import `torch.nn.functional` as `F`\n",
    "- from `torchmetrics` import `Accuracy`\n",
    "- from `torch_geometric.data` import `Data`\n",
    "- from `torch_geometric.nn.pool` import `global_mean_pool, global_add_pool`\n",
    "- import `numpy` as `np`\n",
    "- from `math` import `inf`\n",
    "- from `enum` import `Enum, auto`\n",
    "- from `typing` import `Callable, List, NamedTuple`\n",
    "- from `architectures` import `WeightedGCNConv, WeightedGINConv, WeightedGNNConv, GraphLinear, BatchIdentity`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the import from `architectures` is from the file `architectures.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, the encoders will be bundled in the file `encoder_classes.py` and perform the following imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from `enum` import `Enum, auto`\n",
    "- from `torch` import `cat, rand, isnan, sum, Tensor`\n",
    "- from `torch.nn` import `Module, ModuleList, Linear, Sequential, BatchNorm1d, ReLU, TransformerEncoder, TransformerEncoderLayer, Embedding`\n",
    "- from `torch.nn.init` import `xavier_uniform_`\n",
    "- from `ogb.utils.features` import `get_atom_feature_dims, get_bond_feature_dims`\n",
    "- from `torch_geometric.data` import `Data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Custom Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from type_classes import ModelType, LossesAndMetrics, Pool, ActivationType, Metric\n",
    "from encoder_classes import PosEncoder, DataSetEncoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Official Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.typing import OptTensor, Adj\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Union, List, Tuple, NamedTuple, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first check whether a GPU is available and if so then set it as the default device:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define the root directory `ROOT_DIR`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # If running as a .py\n",
    "    ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except:\n",
    "    # If running as a .ipynb\n",
    "    from os import getcwd\n",
    "    ROOT_DIR = os.path.dirname(os.path.abspath(getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure reproducibility, we define the `set_seed()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The device to be used for training (GPU, ...) will be set automatically in the `Experiment` class (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.npz` files stored in the `data` folder all have the following keys or similar ones (see the list in the code):\n",
    "\n",
    "- `adj_data`\n",
    "- `adj_indices`\n",
    "- `adj_indptr`\n",
    "- `adj_shape`\n",
    "- `attr_data`\n",
    "- `attr_indices`\n",
    "- `attr_indptr`\n",
    "- `attr_shape`\n",
    "- `labels`\n",
    "- `class_names`\n",
    "\n",
    "As an example, the dataset `A.Computer` contains the following class names:\n",
    "\n",
    "- `Desktops`\n",
    "- `Data Storage`\n",
    "- `Laptops`\n",
    "- `Monitors`\n",
    "- `Computer Components`\n",
    "- `Video Projectors`\n",
    "- `Routers`\n",
    "- `Tablets`\n",
    "- `Networking Products`\n",
    "- `Webcams`\n",
    "\n",
    "They are represented in the data labels `labels` as numbers in `[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]`.\n",
    "\n",
    "In this dataset we have `13752` nodes as can be inferred from the `adj_shape` key giving us the dimensions `[13752 13752]`. To each node an element of `labels` provides the corresponding label. The `adj_*` keys let us reconstruct the adjacency matrix and the `attr_*` keys let us reconstruct the node features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the dataset from `.npz` files with the `load_dataset` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(\n",
    "    dataset_name : str\n",
    ") -> Data:\n",
    "    \"\"\"\n",
    "        Loads the datasets from .npz files into a PyTorch_Geometric Data-object where\n",
    "        -   x:              Feature matrix,\n",
    "        -   y:              Labels,\n",
    "        -   edge_index:     List of edges,\n",
    "        -   edge_weight:    List of edge weights.\n",
    "\n",
    "        The function is currently implemented for the following datasets:\n",
    "        -   A.Computer\n",
    "        -   A.Photo\n",
    "        -   Citeseer\n",
    "        -   Cora-ML\n",
    "        -   MS-Academic\n",
    "        -   PubMed\n",
    "    \"\"\"\n",
    "    path = os.path.join(ROOT_DIR, \"replication/data/\" + dataset_name + \".npz\")\n",
    "    npz = np.load(path)\n",
    "    assert isinstance(npz, np.lib.npyio.NpzFile)\n",
    "\n",
    "    variant1 = [(key in [\"adj_data\",\n",
    "                         \"adj_indices\",\n",
    "                         \"adj_indptr\",\n",
    "                         \"adj_shape\",\n",
    "                         \"attr_data\",\n",
    "                         \"attr_indices\",\n",
    "                         \"attr_indptr\",\n",
    "                         \"attr_shape\",\n",
    "                         \"labels\",\n",
    "                         \"class_names\"])\n",
    "                for key in npz.keys()]\n",
    "    isvar1 = False not in variant1\n",
    "    \n",
    "    variant2 = [(key in [\"adj_data\",\n",
    "                         \"adj_indices\",\n",
    "                         \"adj_indptr\",\n",
    "                         \"adj_shape\",\n",
    "                         \"attr_data\",\n",
    "                         \"attr_indices\",\n",
    "                         \"attr_indptr\",\n",
    "                         \"attr_shape\",\n",
    "                         \"labels\",\n",
    "                         \"node_names\",\n",
    "                         \"attr_names\",\n",
    "                         \"class_names\"])\n",
    "                for key in npz.keys()]\n",
    "    isvar2 = False not in variant2\n",
    "    \n",
    "    variant3 = [(key in [\"adj_matrix.data\",\n",
    "                         \"adj_matrix.indices\",\n",
    "                         \"adj_matrix.indptr\",\n",
    "                         \"adj_matrix.shape\",\n",
    "                         \"attr_matrix.data\",\n",
    "                         \"attr_matrix.indices\",\n",
    "                         \"attr_matrix.indptr\",\n",
    "                         \"attr_matrix.shape\",\n",
    "                         \"edge_attr_matrix\",\n",
    "                         \"labels\",\n",
    "                         \"node_names\",\n",
    "                         \"attr_names\",\n",
    "                         \"edge_attr_names\",\n",
    "                         \"class_names\",\n",
    "                         \"metadata\"])\n",
    "                for key in npz.keys()]\n",
    "    isvar3 = False not in variant3\n",
    "\n",
    "    match dataset_name:\n",
    "        case name if name in [\"A.Computer\", \"A.Photo\", \"MS-Academic\"]:\n",
    "            assert isvar1 or isvar2\n",
    "            features_csr = csr_matrix((npz[\"attr_data\"], npz[\"attr_indices\"], npz[\"attr_indptr\"]), shape=npz[\"attr_shape\"])\n",
    "            adjacency_csr = csr_matrix((npz[\"adj_data\"], npz[\"adj_indices\"], npz[\"adj_indptr\"]), shape=npz[\"adj_shape\"])\n",
    "        case name if name in name in [\"Citeseer\", \"Cora-ML\", \"PubMed\"]:\n",
    "            assert isvar3\n",
    "            features_csr = csr_matrix((npz[\"attr_matrix.data\"], npz[\"attr_matrix.indices\"], npz[\"attr_matrix.indptr\"]), shape=npz[\"attr_matrix.shape\"])\n",
    "            adjacency_csr = csr_matrix((npz[\"adj_matrix.data\"], npz[\"adj_matrix.indices\"], npz[\"adj_matrix.indptr\"]), shape=npz[\"adj_matrix.shape\"])\n",
    "        case _:\n",
    "            raise NotImplementedError(f\"The dataset cannot be loaded as it contains unexpected keys. The keys obtained are \\n{list(npz.keys())}\")\n",
    "    \n",
    "    x = torch.FloatTensor(features_csr.toarray())\n",
    "    edge_index, edge_weight = from_scipy_sparse_matrix(adjacency_csr)\n",
    "    y = torch.LongTensor(npz[\"labels\"])\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=y)\n",
    "    if len(np.unique(x)) > 2:\n",
    "        transform = NormalizeFeatures()\n",
    "        data = transform(data)\n",
    "\n",
    "    num_classes = len(set(npz[\"labels\"]))\n",
    "    \n",
    "    return data, num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will define the parameters to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config(\n",
    "    dataset: str,                                                                           # Only the dataset file name, without the .npz ending.\n",
    "    device : torch.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),     # Can override this manually as torch.device object.\n",
    "    pool: Pool = Pool.NONE,\n",
    "\n",
    "    # gumbel\n",
    "    learn_temp=False,\n",
    "    temp_model_type: ModelType.from_string=ModelType.LIN,\n",
    "    tau0: float=0.5,\n",
    "    temp: float=0.01,\n",
    "\n",
    "    # optimization\n",
    "    num_epochs: int=1000,\n",
    "    batch_size: int=32,\n",
    "    lr: float=1e-3,\n",
    "    dropout: float=0.2,\n",
    "\n",
    "    # env cls parameters\n",
    "    env_model_type: ModelType.from_string=ModelType.MEAN_GNN,\n",
    "    env_num_layers: int=3,\n",
    "    env_dim: int=128,\n",
    "    skip=False,\n",
    "    batch_norm=False,\n",
    "    layer_norm=False,\n",
    "    dec_num_layers: int=1,\n",
    "    pos_enc: PosEncoder.from_string=PosEncoder.NONE,\n",
    "\n",
    "    # policy cls parameters\n",
    "    act_model_type: ModelType=ModelType.MEAN_GNN,\n",
    "    act_num_layers: int=1,\n",
    "    act_dim: int=16,\n",
    "\n",
    "    # reproduce\n",
    "    seed: int=0,\n",
    "    gpu: int=None, # In the original argument parser, there is no default value defined here\n",
    "\n",
    "    # dataset dependant parameters\n",
    "    fold: int=None,\n",
    "\n",
    "    # optimizer and scheduler\n",
    "    weight_decay: float=0.0,\n",
    "    ## for steplr scheduler only\n",
    "    step_size: int=None,\n",
    "    gamma: float=None,\n",
    "    ## for cosine with warmup scheduler only\n",
    "    num_warmup_epochs: int=None,\n",
    "\n",
    "    decimal: int=2\n",
    "):\n",
    "    return {\n",
    "        'dataset' : dataset,\n",
    "        'device' : device,\n",
    "        'pool' : pool,\n",
    "        'learn_temp' : learn_temp,\n",
    "        'temp_model_type' : temp_model_type,\n",
    "        'tau0' : tau0,\n",
    "        'temp' : temp,\n",
    "        'num_epochs' : num_epochs,\n",
    "        'batch_size' : batch_size,\n",
    "        'lr' : lr,\n",
    "        'dropout' : dropout,\n",
    "        'env_model_type' : env_model_type,\n",
    "        'env_num_layers' : env_num_layers,\n",
    "        'env_dim' : env_dim,\n",
    "        'skip' : skip,\n",
    "        'batch_norm' : batch_norm,\n",
    "        'layer_norm' : layer_norm,\n",
    "        'dec_num_layers' : dec_num_layers,\n",
    "        'pos_enc' : pos_enc,\n",
    "        'act_model_type' : act_model_type,\n",
    "        'act_num_layers' : act_num_layers,\n",
    "        'act_dim' : act_dim,\n",
    "        'seed' : seed,\n",
    "        'gpu' : gpu,\n",
    "        'fold' : fold,\n",
    "        'weight_decay' : weight_decay,\n",
    "        'step_size' : step_size,\n",
    "        'gamma' : gamma,\n",
    "        'num_warmup_epochs' : num_warmup_epochs,\n",
    "        \"decimal\" : decimal\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetBySplit(NamedTuple):\n",
    "    train: Union[Data, List[Data]]\n",
    "    val: Union[Data, List[Data]]\n",
    "    test: Union[Data, List[Data]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self, data: Data, train_mask: np.ndarray, val_mask: np.ndarray, test_mask: np.ndarray):\n",
    "        self.data = data\n",
    "        self.train_mask = train_mask\n",
    "        self.val_mask = val_mask\n",
    "        self.test_mask = test_mask\n",
    "        self.family = True\n",
    "        self.is_node_based = True\n",
    "        self.not_synthetic = True\n",
    "        self.is_expressivity = False\n",
    "        self.clip_grad = False\n",
    "        self.dataset_encoders = DataSetEncoders.NONE\n",
    "        self.num_after_decimal = 2\n",
    "        self.env_activation_type = ActivationType.RELU\n",
    "    \n",
    "    def select_fold_and_split(self, dataset: Data) -> DatasetBySplit:\n",
    "        device = dataset.x.device\n",
    "        train_mask = torch.tensor(self.train_mask, dtype=torch.bool, device=device)\n",
    "        val_mask = torch.tensor(self.val_mask, dtype=torch.bool, device=device)\n",
    "        test_mask = torch.tensor(self.test_mask, dtype=torch.bool, device=device)\n",
    "\n",
    "        setattr(dataset, \"train_mask\", train_mask)\n",
    "        setattr(dataset, \"val_mask\", val_mask)\n",
    "        setattr(dataset, \"test_mask\", test_mask)\n",
    "\n",
    "        dataset.train_mask[dataset.non_valid_samples] = False\n",
    "        dataset.test_mask[dataset.non_valid_samples] = False\n",
    "        dataset.val_mask[dataset.non_valid_samples] = False\n",
    "        \n",
    "        return DatasetBySplit(train=dataset, val=dataset, test=dataset)\n",
    "\n",
    "    def gin_mlp_func(self) -> Callable:\n",
    "        def mlp_func(in_channels: int, out_channels: int, bias: bool):\n",
    "            return torch.nn.Sequential(\n",
    "                torch.nn.Linear(in_channels, 2 * in_channels, bias=bias),\n",
    "                torch.nn.BatchNorm1d(2 * in_channels),\n",
    "                torch.nn.ReLU(), torch.nn.Linear(2 * in_channels, out_channels, bias=bias)\n",
    "            )\n",
    "        return mlp_func\n",
    "    \n",
    "    def get_split_mask(self, data: Data, batch_size: int, split_mask_name: str) -> Tensor:\n",
    "        if hasattr(data, split_mask_name):\n",
    "            return getattr(data, split_mask_name)\n",
    "        elif self.is_node_based():\n",
    "            return torch.ones(size=(data.x.shape[0],), dtype=torch.bool)\n",
    "        else:\n",
    "            return torch.ones(size=(batch_size,), dtype=torch.bool)\n",
    "    \n",
    "    def get_edge_ratio_node_mask(self, data: Data, split_mask_name: str) -> Tensor:\n",
    "        if hasattr(data, split_mask_name):\n",
    "            return getattr(data, split_mask_name)\n",
    "        else:\n",
    "            return torch.ones(size=(data.x.shape[0],), dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Action Network (Policy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_act_net(action_args : dict) -> torch.nn.ModuleList:\n",
    "    model_type = action_args[\"model_type\"]\n",
    "    env_dim = action_args[\"env_dim\"]\n",
    "    hidden_dim = action_args[\"hidden_dim\"]\n",
    "    num_layers = action_args[\"num_layers\"]\n",
    "    gin_mlp_func = action_args[\"gin_mlp_func\"]\n",
    "\n",
    "    net = model_type.get_component_list(in_dim=env_dim,\n",
    "                                        hidden_dim=hidden_dim,\n",
    "                                        out_dim=2,\n",
    "                                        num_layers=num_layers,\n",
    "                                        bias=True,\n",
    "                                        edges_required=False,\n",
    "                                        gin_mlp_func=gin_mlp_func)\n",
    "\n",
    "    return torch.nn.ModuleList(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNet(torch.nn.Module):\n",
    "    def __init__(self, action_args: dict):\n",
    "        \"\"\"\n",
    "        Create a model which represents the agent's policy.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_layers = action_args[\"num_layers\"]\n",
    "        self.net = load_act_net(action_args=action_args)\n",
    "        self.dropout = torch.nn.Dropout(action_args[\"dropout\"])\n",
    "        self.act = action_args[\"act_type\"].get()\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj, env_edge_attr: OptTensor, act_edge_attr: OptTensor) -> Tensor:\n",
    "        edge_attrs = [env_edge_attr] + (self.num_layers - 1) * [act_edge_attr]\n",
    "        for idx, (edge_attr, layer) in enumerate(zip(edge_attrs[:-1], self.net[:-1])):\n",
    "            x = layer(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "            x = self.dropout(x)\n",
    "            x = self.act(x)\n",
    "        x = self.net[-1](x=x, edge_index=edge_index, edge_attr=edge_attrs[-1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional dynamic temperature calculation for the Gumbel Softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempSoftPlus(torch.nn.Module):\n",
    "    def __init__(self, gumbel_args: dict, env_dim: int):\n",
    "        super(TempSoftPlus, self).__init__()\n",
    "        model_list = gumbel_args[\"temp_model_type\"].get_component_list(in_dim=env_dim, hidden_dim=env_dim, out_dim=1, num_layers=1,\n",
    "                                                                       bias=False, edges_required=False,\n",
    "                                                                       gin_mlp_func=gumbel_args[\"gin_mlp_func\"])\n",
    "        self.linear_model = torch.nn.ModuleList(model_list)\n",
    "        self.softplus = torch.nn.Softplus(beta=1)\n",
    "        self.tau0 = gumbel_args[\"tau0\"]\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj, edge_attr: Tensor):\n",
    "        x = self.linear_model[0](x=x, edge_index=edge_index,edge_attr = edge_attr)\n",
    "        x = self.softplus(x) + self.tau0\n",
    "        temp = x.pow_(-1)\n",
    "        return temp.masked_fill_(temp == float(\"inf\"), 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoGNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will implement the CoGNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_env_net(env_args : dict) -> torch.nn.ModuleList:\n",
    "    in_dim = env_args[\"in_dim\"]\n",
    "    env_dim = env_args[\"env_dim\"]\n",
    "    num_layers = env_args[\"num_layers\"]\n",
    "    gin_mlp_func = env_args[\"gin_mlp_func\"]\n",
    "    dec_num_layers = env_args[\"dec_num_layers\"]\n",
    "    dropout = env_args[\"dropout\"]\n",
    "    act_type = env_args[\"act_type\"]\n",
    "    out_dim = env_args[\"out_dim\"]\n",
    "\n",
    "    enc_list = [env_args[\"dataset_encoders\"].node_encoder(in_dim=in_dim, emb_dim=env_dim)]\n",
    "\n",
    "    component_list = env_args[\"model_type\"].get_component_list(in_dim=in_dim, hidden_dim=env_dim, out_dim=env_dim,\n",
    "                                                               num_layers=num_layers, bias=True, edges_required=True,\n",
    "                                                               gin_mlp_func=gin_mlp_func)\n",
    "\n",
    "    if dec_num_layers > 1:\n",
    "        mlp_list = (dec_num_layers - 1) * [torch.nn.Linear(env_dim, env_dim), torch.nn.Dropout(dropout), act_type.nn()]\n",
    "        mlp_list = mlp_list + [torch.nn.Linear(env_dim, out_dim)]\n",
    "        dec_list = [torch.nn.Sequential(*mlp_list)]\n",
    "    else:\n",
    "        dec_list = [torch.nn.Linear(env_dim, out_dim)]\n",
    "\n",
    "    return torch.nn.ModuleList(enc_list + component_list + dec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoGNN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gumbel_args: dict,\n",
    "        env_args: dict,\n",
    "        action_args: dict,\n",
    "        pool: Pool\n",
    "    ):\n",
    "        super(CoGNN, self).__init__()\n",
    "        self.env_args = env_args\n",
    "        self.learn_temp = gumbel_args[\"learn_temp\"]\n",
    "        if self.learn_temp:\n",
    "            self.temp_model = TempSoftPlus(gumbel_args=gumbel_args, env_dim=env_args[\"env_dim\"])\n",
    "        self.temp = gumbel_args[\"temp\"]\n",
    "\n",
    "        self.num_layers = env_args[\"num_layers\"]\n",
    "        self.env_net = load_env_net(env_args=env_args)\n",
    "        self.use_encoders = env_args[\"dataset_encoders\"].use_encoders()\n",
    "\n",
    "        layer_norm_cls = torch.nn.LayerNorm if env_args[\"layer_norm\"] else torch.nn.Identity\n",
    "        self.hidden_layer_norm = layer_norm_cls(env_args[\"env_dim\"])\n",
    "        self.skip = env_args[\"skip\"]\n",
    "        self.drop_ratio = env_args[\"dropout\"]\n",
    "        self.dropout = torch.nn.Dropout(p=self.drop_ratio)\n",
    "        self.act = env_args[\"act_type\"].get()\n",
    "        self.in_act_net = ActionNet(action_args=action_args)\n",
    "        self.out_act_net = ActionNet(action_args=action_args)\n",
    "\n",
    "        # Encoder types\n",
    "        self.dataset_encoder = env_args[\"dataset_encoders\"]\n",
    "        self.env_bond_encoder = self.dataset_encoder.edge_encoder(emb_dim=env_args[\"env_dim\"], model_type=env_args[\"model_type\"])\n",
    "        self.act_bond_encoder = self.dataset_encoder.edge_encoder(emb_dim=action_args[\"hidden_dim\"], model_type=action_args[\"model_type\"])\n",
    "\n",
    "        # Pooling function to generate whole-graph embeddings\n",
    "        self.pooling = pool.get()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        edge_index: Adj,\n",
    "        pestat,\n",
    "        edge_attr: OptTensor = None,\n",
    "        batch: OptTensor = None,\n",
    "        edge_ratio_node_mask: OptTensor = None\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        result = 0\n",
    "\n",
    "        calc_stats = edge_ratio_node_mask is not None\n",
    "        if calc_stats:\n",
    "            edge_ratio_edge_mask = edge_ratio_node_mask[edge_index[0]] & edge_ratio_node_mask[edge_index[1]]\n",
    "            edge_ratio_list = []\n",
    "\n",
    "        # bond encode\n",
    "        if edge_attr is None or self.env_bond_encoder is None:\n",
    "            env_edge_embedding = None\n",
    "        else:\n",
    "            env_edge_embedding = self.env_bond_encoder(edge_attr)\n",
    "        if edge_attr is None or self.act_bond_encoder is None:\n",
    "            act_edge_embedding = None\n",
    "        else:\n",
    "            act_edge_embedding = self.act_bond_encoder(edge_attr)\n",
    "\n",
    "        # node encode  \n",
    "        x = self.env_net[0](x, pestat) # (N, F) encoder\n",
    "        if not self.use_encoders:\n",
    "            x = self.dropout(x)\n",
    "            x = self.act(x)\n",
    "\n",
    "        for gnn_idx in range(self.num_layers):\n",
    "            x = self.hidden_layer_norm(x)\n",
    "\n",
    "            # action\n",
    "            in_logits = self.in_act_net(x=x, edge_index=edge_index, env_edge_attr=env_edge_embedding, act_edge_attr=act_edge_embedding) # (N, 2)\n",
    "            out_logits = self.out_act_net(x=x, edge_index=edge_index, env_edge_attr=env_edge_embedding, act_edge_attr=act_edge_embedding) # (N, 2)\n",
    "\n",
    "            temp = self.temp_model(x=x, edge_index=edge_index, edge_attr=env_edge_embedding) if self.learn_temp else self.temp\n",
    "            in_probs = torch.nn.functional.gumbel_softmax(logits=in_logits, tau=temp, hard=True)\n",
    "            out_probs = torch.nn.functional.gumbel_softmax(logits=out_logits, tau=temp, hard=True)\n",
    "            edge_weight = self.create_edge_weight(edge_index=edge_index, keep_in_prob=in_probs[:, 0], keep_out_prob=out_probs[:, 0])\n",
    "\n",
    "            # environment\n",
    "            out = self.env_net[1 + gnn_idx](x=x, edge_index=edge_index, edge_weight=edge_weight, edge_attr=env_edge_embedding)\n",
    "            out = self.dropout(out)\n",
    "            out = self.act(out)\n",
    "\n",
    "            if calc_stats:\n",
    "                edge_ratio = edge_weight[edge_ratio_edge_mask].sum() / edge_weight[edge_ratio_edge_mask].shape[0]\n",
    "                edge_ratio_list.append(edge_ratio.item())\n",
    "\n",
    "            if self.skip:\n",
    "                x = x + out\n",
    "            else:\n",
    "                x = out\n",
    "\n",
    "        x = self.hidden_layer_norm(x)\n",
    "        x = self.pooling(x, batch=batch)\n",
    "        x = self.env_net[-1](x) # decoder\n",
    "        result = result + x\n",
    "\n",
    "        if calc_stats:\n",
    "            edge_ratio_tensor = torch.tensor(edge_ratio_list, device=x.device)\n",
    "        else:\n",
    "            edge_ratio_tensor = -1 * torch.ones(size=(self.num_layers,), device=x.device)\n",
    "        return result, edge_ratio_tensor\n",
    "\n",
    "    def create_edge_weight(\n",
    "        self,\n",
    "        edge_index: Adj,\n",
    "        keep_in_prob: Tensor,\n",
    "        keep_out_prob: Tensor\n",
    "    ) -> Tensor:\n",
    "        u, v = edge_index\n",
    "        edge_in_prob = keep_in_prob[v]\n",
    "        edge_out_prob = keep_out_prob[u]\n",
    "        return edge_in_prob * edge_out_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Experiment` class contains the procedure to load a dataset, run the training and evaluation, as well as return the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CoGNN(\n",
    "    model : torch.nn.Module,\n",
    "    task_loss : torch.nn.CrossEntropyLoss,\n",
    "    dataset : Data,\n",
    "    optimizer : torch.optim.Optimizer,\n",
    "    train_loader : DataLoader,\n",
    "    config : dict\n",
    "):\n",
    "    \"\"\"\n",
    "        Training loop for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    for batch_data in train_loader:\n",
    "        if config[\"batch_norm\"] and (batch_data.x.shape[0] == 1 or batch_data.num_graphs == 1):\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        node_mask = dataset.get_split_mask(data=batch_data, batch_size=batch_data.num_graphs, split_mask_name='train_mask').to(config[\"device\"])\n",
    "        edge_attr = batch_data.edge_attr\n",
    "        if batch_data.edge_attr is not None:\n",
    "            edge_attr = edge_attr.to(device=config[\"device\"])\n",
    "\n",
    "        predictions, edge_ratio_tensor = model(x=batch_data.x.to(device=config[\"device\"]),\n",
    "                                               edge_index=batch_data.edge_index.to(device=config[\"device\"]),\n",
    "                                               batch=batch_data.batch.to(device=config[\"device\"]),\n",
    "                                               edge_attr=edge_attr,\n",
    "                                               edge_ratio_node_mask=None,\n",
    "                                               pestat=config[\"pos_enc\"].get_pe(data=batch_data, device=config[\"device\"]))\n",
    "\n",
    "        train_loss = task_loss(predictions[node_mask], batch_data.y.to(device=config[\"device\"])[node_mask])\n",
    "        train_loss.backward()\n",
    "\n",
    "        if dataset.clip_grad():\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, train_loss.item(), edge_ratio_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_CoGNN(\n",
    "    model : torch.nn.Module,\n",
    "    dataset : Data,\n",
    "    config,\n",
    "    task_loss,\n",
    "    metric_type,\n",
    "    split_mask_name=None,\n",
    "    calc_edge_ratio=True\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_metric, total_edge_ratios = 0, 0, 0\n",
    "    total_scores = np.empty(shape=(0, model.env_args[\"out_dim\"]))\n",
    "    total_y = None\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    for batch_data in loader:\n",
    "        if config[\"batch_norm\"] and (batch_data.x.shape[0] == 1 or batch_data.num_graphs == 1):\n",
    "            continue\n",
    "        node_mask = dataset.get_split_mask(data=batch_data, batch_size=batch_data.num_graphs, split_mask_name=split_mask_name).to(device=config[\"device\"])\n",
    "        if calc_edge_ratio:\n",
    "            edge_ratio_node_mask = dataset.get_edge_ratio_node_mask(data=batch_data, split_mask_name=split_mask_name).to(device=config[\"device\"])\n",
    "        else:\n",
    "            edge_ratio_node_mask = None\n",
    "        edge_attr = batch_data.edge_attr\n",
    "        if batch_data.edge_attr is not None:\n",
    "            edge_attr = edge_attr.to(device=config[\"device\"])\n",
    "        predictions, edge_ratio_tensor = model(batch_data.x.to(device=config[\"device\"]),\n",
    "                                               edge_index=batch_data.edge_index.to(device=config[\"device\"]),\n",
    "                                               edge_attr=edge_attr, batch=batch_data.batch.to(device=config[\"device\"]),\n",
    "                                               edge_ratio_node_mask=edge_ratio_node_mask,\n",
    "                                               pestat=config[\"pos_enc\"].get_pe(data=batch_data, device=config[\"device\"]))\n",
    "\n",
    "        eval_loss = task_loss(predictions, batch_data.y.to(device=config[\"device\"]))\n",
    "\n",
    "        total_scores = np.concatenate((total_scores, predictions[node_mask].detach().cpu().numpy()))\n",
    "        if total_y is None:\n",
    "            total_y = batch_data.y.to(device=config[\"device\"])[node_mask].detach().cpu().numpy()\n",
    "        else:\n",
    "            total_y = np.concatenate((total_y, batch_data.y.to(device=config[\"device\"])[node_mask].detach().cpu().numpy()))\n",
    "        total_loss += eval_loss.item() * batch_data.num_graphs\n",
    "        total_edge_ratios += edge_ratio_tensor * batch_data.num_graphs\n",
    "    \n",
    "    accuracy = metric_type.apply_metric(scores=total_scores, target=total_y)\n",
    "    loss = total_loss / len(loader.dataset)\n",
    "    edge_ratios = total_edge_ratios / len(loader.dataset)\n",
    "\n",
    "    return accuracy, loss, edge_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold_CoGNN(\n",
    "    config : dict,\n",
    "    dataset_by_split: DatasetBySplit,\n",
    "    model : torch.nn.Module,\n",
    "    optimizer : torch.optim.Optimizer,\n",
    "    pbar : tqdm.std.tqdm,\n",
    "    num_fold: int\n",
    ") -> Tuple[LossesAndMetrics, OptTensor]:\n",
    "    \"\"\"\n",
    "        Training loop for one fold.\n",
    "    \"\"\"\n",
    "    train_loader = DataLoader(dataset_by_split.train, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(dataset_by_split.val, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    test_loader = DataLoader(dataset_by_split.test, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    task_loss = config[\"metric\"].task_loss\n",
    "\n",
    "    best_losses_n_metrics = config[\"metric\"].get_worst_losses_n_metrics\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        model, epoch_loss, epoch_edge_ratio_tensor = train_CoGNN(model=model,\n",
    "                                                                 task_loss=task_loss,\n",
    "                                                                 dataset=dataset_by_split.train,\n",
    "                                                                 optimizer=optimizer,\n",
    "                                                                 train_loader=train_loader,\n",
    "                                                                 config=config)\n",
    "\n",
    "        epoch_train_accuracy, epoch_train_loss, epoch_edge_ratios = evaluate_CoGNN(loader=train_loader,\n",
    "                                                                                   model=model,\n",
    "                                                                                   split_mask_name=\"train_mask\",\n",
    "                                                                                   calc_edge_ratio=False)\n",
    "        \n",
    "        val_loss, val_metric, _ = evaluate_CoGNN(loader=val_loader, model=model, split_mask_name='val_mask', calc_edge_ratio=False)\n",
    "        test_loss, test_metric, _ = evaluate_CoGNN(loader=test_loader, model=model, split_mask_name='test_mask', calc_edge_ratio=False)\n",
    "\n",
    "        losses_n_metrics = LossesAndMetrics(train_loss=epoch_loss,\n",
    "                                            val_loss=val_loss,\n",
    "                                            test_loss=test_loss,\n",
    "                                            train_metric=epoch_score,\n",
    "                                            val_metric=val_metric,\n",
    "                                            test_metric=test_metric)\n",
    "        \n",
    "        if metric_type.src_better_than_other(src=losses_n_metrics.val_metric,\n",
    "                                             other=best_losses_n_metrics.val_metric):\n",
    "            best_losses_n_metrics = losses_n_metrics\n",
    "\n",
    "        log_str = f\"Split: {num_fold}, epoch: {epoch}\"\n",
    "        for name in losses_n_metrics._fields:\n",
    "            log_str += f\",{name}={round(getattr(losses_n_metrics, name), config[\"decimal\"])}\"\n",
    "        log_str += f\"({round(best_losses_n_metrics.test_metric, config[\"decimal\"])})\"\n",
    "        pbar.set_description(log_str)\n",
    "        pbar.update(n=1)\n",
    "    \n",
    "    edge_ratios = None\n",
    "    _, _, edge_ratios = evaluate_CoGNN(loader=test_loader, model=model, split_mask_name='test_mask', calc_edge_ratio=True)\n",
    "    \n",
    "    return best_losses_n_metrics, edge_ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_CoGNN(\n",
    "    data : Data,\n",
    "    config : dict,\n",
    "    num_folds : int\n",
    ") -> Tuple[torch.nn.Module, dict]:\n",
    "    \"\"\"\n",
    "        Training loop for n-fold with multiple epochs.\n",
    "    \"\"\"\n",
    "    #task_loss = config[\"metric\"].task_loss\n",
    "    decimal = config[\"decimal\"]\n",
    "    seeds = [config[\"seed\"] + n for n in range(num_folds)]\n",
    "\n",
    "    dataset = DataSet(data=data, train_mask=None, val_mask=None, test_mask=None)\n",
    "\n",
    "    gin_mlp_func = dataset.gin_mlp_func()\n",
    "    env_act_type = dataset.env_activation_type\n",
    "    dataset_encoder = dataset.dataset_encoders\n",
    "    out_dim = config[\"metric\"].get_out_dim(data=data)\n",
    "\n",
    "    gumbel = [\"learn_temp\", \"temp_model_type\", \"tau0\", \"temp\"]\n",
    "    gumbel_add = {\"gin_mlp_func\" :  gin_mlp_func}\n",
    "    gumbel_args = {key: config.get(key) for key in gumbel}\n",
    "    gumbel_args.update(gumbel_add)\n",
    "\n",
    "    env = [\"env_dim\", \"layer_norm\", \"skip\", \"batch_norm\", \"dropout\", \"metric\", \"dec_num_layers\", \"pos_enc\"]\n",
    "    env_add = {\"model_type\" :       config[\"env_model_type\"],\n",
    "               \"num_layers\" :       config[\"env_num_layers\"],\n",
    "               \"act_type\" :         env_act_type,\n",
    "               \"in_dim\" :           data.x.shape[1],\n",
    "               \"out_dim\" :          out_dim,\n",
    "               \"gin_mlp_func\" :     gin_mlp_func,\n",
    "               \"dataset_encoders\" : dataset_encoder}\n",
    "    env_args = {key: config.get(key) for key in env}\n",
    "    env_args.update(env_add)\n",
    "\n",
    "    action = [\"dropout\", \"env_dim\"]\n",
    "    action_add = {\"model_type\" :    config[\"act_model_type\"],\n",
    "                  \"num_layers\" :    config[\"act_num_layers\"],\n",
    "                  \"hidden_dim\" :    config[\"act_dim\"],\n",
    "                  \"act_type\" :      ActivationType.RELU,\n",
    "                  \"gin_mlp_func\" :  gin_mlp_func}\n",
    "    action_args = {key: config.get(key) for key in action}\n",
    "    action_args.update(action_add)\n",
    "\n",
    "    metrics_list = []\n",
    "    edge_ratios_list = []\n",
    "    for n in range(num_folds):\n",
    "        set_seed(seed=seeds[n])\n",
    "        dataset_by_split = dataset.select_fold_and_split(dataset=dataset)\n",
    "\n",
    "        model = CoGNN(gumbel_args=gumbel_args, env_args=env_args, action_args=action_args, pool=config[\"pool\"]).to(device=config[\"device\"])\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "        with tqdm.tqdm(total=config[\"num_epochs\"], file=sys.stdout) as pbar:\n",
    "            best_losses_n_metrics, edge_ratios = train_fold_CoGNN(config=config,\n",
    "                                                                  dataset_by_split=dataset_by_split,\n",
    "                                                                  model=model,\n",
    "                                                                  optimizer=optimizer,\n",
    "                                                                  pbar=pbar,\n",
    "                                                                  num_fold=n)\n",
    "        \n",
    "        print_str = f\"Fold {n}/{num_folds}\"\n",
    "        for name in best_losses_n_metrics._fields:\n",
    "            print_str += f\",{name}={round(getattr(best_losses_n_metrics, name), decimal)}\"\n",
    "        print(print_str)\n",
    "        print()\n",
    "        metrics_list.append(best_losses_n_metrics.get_fold_metrics())\n",
    "\n",
    "        if edge_ratios is not None:\n",
    "                edge_ratios_list.append(edge_ratios)\n",
    "    \n",
    "    metrics_matrix = torch.stack(metrics_list, dim=0)  # (F, 3)\n",
    "    metrics_mean = torch.mean(metrics_matrix, dim=0).tolist()  # (3,)\n",
    "    if len(edge_ratios_list) > 0:\n",
    "        edge_ratios = torch.mean(torch.stack(edge_ratios_list, dim=0), dim=0)\n",
    "    else:\n",
    "        edge_ratios = None\n",
    "    \n",
    "    print(f\"Final Rewired train={round(metrics_mean[0], decimal)},\"\n",
    "          f\"val={round(metrics_mean[1], decimal)},\"\n",
    "          f\"test={round(metrics_mean[2], decimal)}\")\n",
    "    \n",
    "    if num_folds > 1:\n",
    "        metrics_std = torch.std(metrics_matrix, dim=0).tolist()  # (3,)\n",
    "        print(f\"Final Rewired train={round(metrics_mean[0], decimal)}+-{round(metrics_std[0], decimal)},\"\n",
    "              f\"val={round(metrics_mean[1], decimal)}+-{round(metrics_std[1], decimal)},\"\n",
    "              f\"test={round(metrics_mean[2], decimal)}+-{round(metrics_std[2], decimal)}\")\n",
    "\n",
    "    # return metrics_mean, edge_ratios\n",
    "    history = None\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_CoGNN(\n",
    "    config : dict,\n",
    "    num_folds : int=10,\n",
    "    run_grid_search : bool=False\n",
    ") -> Tuple[torch.nn.Module, Data, dict]:\n",
    "    \"\"\"\n",
    "        Main function with optional grid search.\n",
    "    \"\"\"\n",
    "    device = config[\"device\"]\n",
    "\n",
    "    print(f\"Loading {config[\"dataset\"]} dataset...\")\n",
    "    data, num_classes = load_dataset(config[\"dataset\"])\n",
    "    config[\"metric\"] = Metric(task=\"multiclass\", num_classes=num_classes)\n",
    "    data = data.to(device)\n",
    "\n",
    "    if run_grid_search:\n",
    "        print(\"Running grid search...\")\n",
    "        raise NotImplementedError(\"Grid search hasn't been implemented yet!\")\n",
    "    \n",
    "        # List of the parameters and the values to be searched across\n",
    "        param_list_1 = []\n",
    "        param_list_2 = []\n",
    "        # ...\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # Adjust the nested loops for the various parameters\n",
    "        for param_1 in param_list_1:\n",
    "            for param_2 in param_list_2:\n",
    "                # ...\n",
    "                config_copy = config.copy()\n",
    "                config_copy[\"param_1_name\"] = param_1\n",
    "                config_copy[\"param_2_name\"] = param_2\n",
    "\n",
    "                print(f\"\\nTrying param_1_name={param_1}, param_2_name={param_2} ...\")\n",
    "                model = CoGNN(dataset, config_copy).to(device)\n",
    "                model, _ = train_model_CoGNN(model, data, config_copy)\n",
    "\n",
    "                test_acc, test_steps, _, _ = evaluate_CoGNN(model, data, data.test_mask)\n",
    "\n",
    "                results.append({\n",
    "                    \"param_1_name\": param_1,\n",
    "                    \"param_2_name\": param_2,\n",
    "                    # ...\n",
    "                    \"test_acc\": test_acc,\n",
    "                    \"test_steps\": test_steps\n",
    "                })\n",
    "\n",
    "                print(f\"Result: Test Acc: {test_acc:.4f}, Test Steps: {test_steps:.2f}\")\n",
    "\n",
    "        best_parameters = max(results, key=lambda x: x[\"test_acc\"])\n",
    "\n",
    "        print(\"\\nGrid Search Results:\")\n",
    "        for r in results:\n",
    "            print(f\"param_1_name={r[\"param_1_name\"]}, param_2_name={r[\"param_2_name\"]}: \" # ...\n",
    "                  f\"Acc={r[\"test_acc\"]:.4f}, Steps={r[\"test_steps\"]:.2f}\")\n",
    "\n",
    "        print(f\"\\nBest parameters: param_1_name={best_result[\"param_1_name\"]}, \" # ...\n",
    "              f\"param_2_name={best_result[\"param_2_name\"]}, \"\n",
    "              f\"Accuracy={best_result[\"test_acc\"]:.4f}\")\n",
    "\n",
    "        # config with best parameters\n",
    "        config[\"param_1_name\"] = best_result[\"param_1_name\"]\n",
    "        config[\"param_2_name\"] = best_result[\"param_2_name\"]\n",
    "        # ...    \n",
    "\n",
    "    # Instantiating the model with obtained config\n",
    "    print(\"\\nTraining final model...\")\n",
    "    #model = CoGNN(gumbel_args, env_args, action_args, config[\"pool\"]).to(device)\n",
    "\n",
    "    # Training the model\n",
    "    model, history = train_model_CoGNN(data=data, config=config, num_folds=num_folds)\n",
    "\n",
    "    # test eval\n",
    "    test_acc, test_steps, _, _ = evaluate_CoGNN(model, data, data.test_mask)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}, Average Steps: {test_steps:.2f}\")\n",
    "\n",
    "    # visualizations\n",
    "    # TO-DO!\n",
    "\n",
    "    # optional: impact of various parameters\n",
    "\n",
    "    return model, data, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading A.Computer dataset...\n",
      "\n",
      "Training final model...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataSet' object has no attribute 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     CONFIG \u001b[38;5;241m=\u001b[39m create_config(dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA.Computer\u001b[39m\u001b[38;5;124m\"\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     model, data, history \u001b[38;5;241m=\u001b[39m \u001b[43mmain_CoGNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_grid_search\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 72\u001b[0m, in \u001b[0;36mmain_CoGNN\u001b[0;34m(config, num_folds, run_grid_search)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining final model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m#model = CoGNN(gumbel_args, env_args, action_args, config[\"pool\"]).to(device)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_CoGNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_folds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# test eval\u001b[39;00m\n\u001b[1;32m     75\u001b[0m test_acc, test_steps, _, _ \u001b[38;5;241m=\u001b[39m evaluate_CoGNN(model, data, data\u001b[38;5;241m.\u001b[39mtest_mask)\n",
      "Cell \u001b[0;32mIn[18], line 49\u001b[0m, in \u001b[0;36mtrain_model_CoGNN\u001b[0;34m(data, config, num_folds)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_folds):\n\u001b[1;32m     48\u001b[0m     set_seed(seed\u001b[38;5;241m=\u001b[39mseeds[n])\n\u001b[0;32m---> 49\u001b[0m     dataset_by_split \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_fold_and_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     model \u001b[38;5;241m=\u001b[39m CoGNN(gumbel_args\u001b[38;5;241m=\u001b[39mgumbel_args, env_args\u001b[38;5;241m=\u001b[39menv_args, action_args\u001b[38;5;241m=\u001b[39maction_args, pool\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpool\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     52\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m], weight_decay\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m, in \u001b[0;36mDataSet.select_fold_and_split\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_fold_and_split\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: Data) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DatasetBySplit:\n\u001b[0;32m---> 17\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     18\u001b[0m     train_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_mask, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     19\u001b[0m     val_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_mask, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataSet' object has no attribute 'x'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    CONFIG = create_config(dataset=\"A.Computer\", seed=42)\n",
    "    model, data, history = main_CoGNN(config=CONFIG, num_folds=10, run_grid_search=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textcolor{red}{\\texttt{TO - DO!}}$\n",
    "\n",
    "- grid search in `main` function. What parameters to search across and what should the values be?\n",
    "- Check that all quotations are \" not '\n",
    "- Check whether the dataset classification (homophilic etc.) is okay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AdvancedTopicsInMachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
