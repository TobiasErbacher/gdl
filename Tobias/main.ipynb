{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cooperative Graph Neural Networks ([CoGNN](https://doi.org/10.48550/arXiv.2310.01267))\n",
    "\n",
    "This part was adapted by Tobias Erbacher from the [authors' github](https://github.com/benfinkelshtein/CoGNN/tree/main). We recommend to run this on a GPU service like [Google Colab](https://colab.research.google.com/). The goal is to modify the original authors' code to be integrable with our codebase.\n",
    "\n",
    "Wherever possible, the original code was left untouched while code sections not required for our dataset types were removed to enhance readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure we are using the same modules as the authors, we need to install the following modules as specified on the authors' github (uncomment before cell execution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch==2.0.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#%pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
    "#%pip install torch-geometric==2.3.0\n",
    "#%pip install torchmetrics ogb rdkit\n",
    "#%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute this notebook, we will assume that the datasets are already installed. If you cloned our github repository, you will be able to find them in [this folder](https://github.com/TobiasErbacher/gdl/tree/main/replication/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this notebook less crowded, some class definitions have been moved to other files. The overview here is to get an idea of the library dependencies. The file `architectures.py` contains the available model architectures and will perform the following imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from `torch.nn` import `Linear, Parameter, Module`\n",
    "- from `torch` import `Tensor, cat`\n",
    "- from `typing` import `Callable, Any`\n",
    "- from `torch_geometric.typing` import `OptTensor, Adj`\n",
    "- from `torch_geometric.nn.conv` import `MessagePassing`\n",
    "- from `torch_geometric.nn.conv.gcn_conv` import `gcn_norm`\n",
    "- from `torch_geometric.utils` import `remove_self_loops, add_remaining_self_loops`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the type classes will be bundled in the file `type_classes.py` and perform the following imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from `torch` import `from_numpy, tensor`\n",
    "- from `torch.nn` import `Module, ReLU, GELU, CrossEntropyLoss`\n",
    "- import `torch.nn.functional` as `F`\n",
    "- from `torchmetrics` import `Accuracy`\n",
    "- from `torch_geometric.data` import `Data`\n",
    "- import `numpy` as `np`\n",
    "- from `math` import `inf`\n",
    "- from `enum` import `Enum, auto`\n",
    "- from `typing` import `Callable, List, NamedTuple`\n",
    "- from `architectures` import `WeightedGCNConv, WeightedGINConv, WeightedGNNConv, GraphLinear`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the import from `architectures` is from the file `architectures.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, the encoders will be bundled in the file `encoder_classes.py` and perform the following imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from `enum` import `Enum, auto`\n",
    "- from `torch` import `cat, rand, isnan, sum, Tensor`\n",
    "- from `torch.nn` import `Module, ModuleList, Linear, Sequential, BatchNorm1d, ReLU, TransformerEncoder, TransformerEncoderLayer, Embedding`\n",
    "- from `torch.nn.init` import `xavier_uniform_`\n",
    "- from `ogb.utils.features` import `get_atom_feature_dims, get_bond_feature_dims`\n",
    "- from `torch_geometric.data` import `Data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Custom Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import our custom files, we can simply use the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from type_classes import ModelType, LossesAndMetrics, ActivationType, Metric\n",
    "from encoder_classes import PosEncoder, DataSetEncoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Official Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import the \"official\" libraries that are used in this notebook, execute the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "from torch_geometric.typing import OptTensor, Adj\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import pandas\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Union, List, Tuple, Callable\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define the root directory `ROOT_DIR` so that we will be able to load the dataset correctly later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # If running as a .py\n",
    "    ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except:\n",
    "    # If running as a .ipynb\n",
    "    from os import getcwd\n",
    "    ROOT_DIR = os.path.dirname(os.path.abspath(getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure reproducibility, we define the `set_seed()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The device to be used for training (GPU, ...) will be set automatically in the `config` where it checks for GPU availability (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains the data loading functions that were copied from [this](https://github.com/TobiasErbacher/gdl/tree/main/replication/data_loading) folder from our github. It is only needed when running this notebook by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spla\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "import warnings\n",
    "\n",
    "data_dir = os.path.join(Path(getcwd()).parent, \"replication/data\")\n",
    "development_seed = 4143496719\n",
    "\n",
    "sparse_graph_properties = [\n",
    "        'adj_matrix', 'attr_matrix', 'labels',\n",
    "        'node_names', 'attr_names', 'class_names',\n",
    "        'metadata']\n",
    "\n",
    "class SparseGraph:\n",
    "    \"\"\"Attributed labeled graph stored in sparse matrix form.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adj_matrix\n",
    "        Adjacency matrix in CSR format. Shape [num_nodes, num_nodes]\n",
    "    attr_matrix\n",
    "        Attribute matrix in CSR or numpy format. Shape [num_nodes, num_attr]\n",
    "    labels\n",
    "        Array, where each entry represents respective node's label(s). Shape [num_nodes]\n",
    "        Alternatively, CSR matrix with labels in one-hot format. Shape [num_nodes, num_classes]\n",
    "    node_names\n",
    "        Names of nodes (as strings). Shape [num_nodes]\n",
    "    attr_names\n",
    "        Names of the attributes (as strings). Shape [num_attr]\n",
    "    class_names\n",
    "        Names of the class labels (as strings). Shape [num_classes]\n",
    "    metadata\n",
    "        Additional metadata such as text.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, adj_matrix: sp.spmatrix,\n",
    "            attr_matrix: Union[np.ndarray, sp.spmatrix] = None,\n",
    "            labels: Union[np.ndarray, sp.spmatrix] = None,\n",
    "            node_names: np.ndarray = None,\n",
    "            attr_names: np.ndarray = None,\n",
    "            class_names: np.ndarray = None,\n",
    "            metadata: Any = None):\n",
    "        # Make sure that the dimensions of matrices / arrays all agree\n",
    "        if sp.isspmatrix(adj_matrix):\n",
    "            adj_matrix = adj_matrix.tocsr().astype(np.float32)\n",
    "        else:\n",
    "            raise ValueError(\"Adjacency matrix must be in sparse format (got {0} instead).\"\n",
    "                             .format(type(adj_matrix)))\n",
    "\n",
    "        if adj_matrix.shape[0] != adj_matrix.shape[1]:\n",
    "            raise ValueError(\"Dimensions of the adjacency matrix don't agree.\")\n",
    "\n",
    "        if attr_matrix is not None:\n",
    "            if sp.isspmatrix(attr_matrix):\n",
    "                attr_matrix = attr_matrix.tocsr().astype(np.float32)\n",
    "            elif isinstance(attr_matrix, np.ndarray):\n",
    "                attr_matrix = attr_matrix.astype(np.float32)\n",
    "            else:\n",
    "                raise ValueError(\"Attribute matrix must be a sp.spmatrix or a np.ndarray (got {0} instead).\"\n",
    "                                 .format(type(attr_matrix)))\n",
    "\n",
    "            if attr_matrix.shape[0] != adj_matrix.shape[0]:\n",
    "                raise ValueError(\"Dimensions of the adjacency and attribute matrices don't agree.\")\n",
    "\n",
    "        if labels is not None:\n",
    "            if labels.shape[0] != adj_matrix.shape[0]:\n",
    "                raise ValueError(\"Dimensions of the adjacency matrix and the label vector don't agree.\")\n",
    "\n",
    "        if node_names is not None:\n",
    "            if len(node_names) != adj_matrix.shape[0]:\n",
    "                raise ValueError(\"Dimensions of the adjacency matrix and the node names don't agree.\")\n",
    "\n",
    "        if attr_names is not None:\n",
    "            if len(attr_names) != attr_matrix.shape[1]:\n",
    "                raise ValueError(\"Dimensions of the attribute matrix and the attribute names don't agree.\")\n",
    "\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.attr_matrix = attr_matrix\n",
    "        self.labels = labels\n",
    "        self.node_names = node_names\n",
    "        self.attr_names = attr_names\n",
    "        self.class_names = class_names\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def num_nodes(self) -> int:\n",
    "        \"\"\"Get the number of nodes in the graph.\n",
    "        \"\"\"\n",
    "        return self.adj_matrix.shape[0]\n",
    "\n",
    "    def num_edges(self) -> int:\n",
    "        \"\"\"Get the number of edges in the graph.\n",
    "\n",
    "        For undirected graphs, (i, j) and (j, i) are counted as _two_ edges.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.adj_matrix.nnz\n",
    "\n",
    "    def get_neighbors(self, idx: int) -> np.ndarray:\n",
    "        \"\"\"Get the indices of neighbors of a given node.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx\n",
    "            Index of the node whose neighbors are of interest.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.adj_matrix[idx].indices\n",
    "\n",
    "    def get_edgeid_to_idx_array(self) -> np.ndarray:\n",
    "        \"\"\"Return a Numpy Array that maps edgeids to the indices in the adjacency matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The i'th entry contains the x- and y-coordinates of edge i in the adjacency matrix.\n",
    "            Shape [num_edges, 2]\n",
    "\n",
    "        \"\"\"\n",
    "        return np.transpose(self.adj_matrix.nonzero())\n",
    "\n",
    "    def is_directed(self) -> bool:\n",
    "        \"\"\"Check if the graph is directed (adjacency matrix is not symmetric).\n",
    "        \"\"\"\n",
    "        return (self.adj_matrix != self.adj_matrix.T).sum() != 0\n",
    "\n",
    "    def to_undirected(self) -> 'SparseGraph':\n",
    "        \"\"\"Convert to an undirected graph (make adjacency matrix symmetric).\n",
    "        \"\"\"\n",
    "        idx = self.get_edgeid_to_idx_array().T\n",
    "        ridx = np.ravel_multi_index(idx, self.adj_matrix.shape)\n",
    "        ridx_rev = np.ravel_multi_index(idx[::-1], self.adj_matrix.shape)\n",
    "\n",
    "        # Get duplicate edges (self-loops and opposing edges)\n",
    "        dup_ridx = ridx[np.isin(ridx, ridx_rev)]\n",
    "        dup_idx = np.unravel_index(dup_ridx, self.adj_matrix.shape)\n",
    "\n",
    "        # Check if the adjacency matrix weights are symmetric (if nonzero)\n",
    "        if len(dup_ridx) > 0 and not np.allclose(self.adj_matrix[dup_idx], self.adj_matrix[dup_idx[::-1]]):\n",
    "            raise ValueError(\"Adjacency matrix weights of opposing edges differ.\")\n",
    "\n",
    "        # Create symmetric matrix\n",
    "        new_adj_matrix = self.adj_matrix + self.adj_matrix.T\n",
    "        if len(dup_ridx) > 0:\n",
    "            new_adj_matrix[dup_idx] = (new_adj_matrix[dup_idx] - self.adj_matrix[dup_idx]).A1\n",
    "\n",
    "        self.adj_matrix = new_adj_matrix\n",
    "        return self\n",
    "\n",
    "    def is_weighted(self) -> bool:\n",
    "        \"\"\"Check if the graph is weighted (edge weights other than 1).\n",
    "        \"\"\"\n",
    "        return np.any(np.unique(self.adj_matrix[self.adj_matrix.nonzero()].A1) != 1)\n",
    "\n",
    "    def to_unweighted(self) -> 'SparseGraph':\n",
    "        \"\"\"Convert to an unweighted graph (set all edge weights to 1).\n",
    "        \"\"\"\n",
    "        self.adj_matrix.data = np.ones_like(self.adj_matrix.data)\n",
    "        return self\n",
    "\n",
    "    def is_connected(self) -> bool:\n",
    "        \"\"\"Check if the graph is connected.\n",
    "        \"\"\"\n",
    "        return sp.csgraph.connected_components(self.adj_matrix, return_labels=False) == 1\n",
    "\n",
    "    def has_self_loops(self) -> bool:\n",
    "        \"\"\"Check if the graph has self-loops.\n",
    "        \"\"\"\n",
    "        return not np.allclose(self.adj_matrix.diagonal(), 0)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        props = []\n",
    "        for prop_name in sparse_graph_properties:\n",
    "            prop = getattr(self, prop_name)\n",
    "            if prop is not None:\n",
    "                if prop_name == 'metadata':\n",
    "                    props.append(prop_name)\n",
    "                else:\n",
    "                    shape_string = 'x'.join([str(x) for x in prop.shape])\n",
    "                    props.append(\"{} ({})\".format(prop_name, shape_string))\n",
    "        dir_string = 'Directed' if self.is_directed() else 'Undirected'\n",
    "        weight_string = 'weighted' if self.is_weighted() else 'unweighted'\n",
    "        conn_string = 'connected' if self.is_connected() else 'disconnected'\n",
    "        loop_string = 'has self-loops' if self.has_self_loops() else 'no self-loops'\n",
    "        return (\"<{}, {} and {} SparseGraph with {} edges ({}). Data: {}>\"\n",
    "                .format(dir_string, weight_string, conn_string,\n",
    "                        self.num_edges(), loop_string,\n",
    "                        ', '.join(props)))\n",
    "\n",
    "    # Quality of life (shortcuts)\n",
    "    def standardize(\n",
    "            self, make_unweighted: bool = True,\n",
    "            make_undirected: bool = True,\n",
    "            no_self_loops: bool = True,\n",
    "            select_lcc: bool = True\n",
    "            ) -> 'SparseGraph':\n",
    "        \"\"\"Perform common preprocessing steps: remove self-loops, make unweighted/undirected, select LCC.\n",
    "\n",
    "        All changes are done inplace.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        make_unweighted\n",
    "            Whether to set all edge weights to 1.\n",
    "        make_undirected\n",
    "            Whether to make the adjacency matrix symmetric. Can only be used if make_unweighted is True.\n",
    "        no_self_loops\n",
    "            Whether to remove self loops.\n",
    "        select_lcc\n",
    "            Whether to select the largest connected component of the graph.\n",
    "\n",
    "        \"\"\"\n",
    "        G = self\n",
    "        if make_unweighted and G.is_weighted():\n",
    "            G = G.to_unweighted()\n",
    "        if make_undirected and G.is_directed():\n",
    "            G = G.to_undirected()\n",
    "        if no_self_loops and G.has_self_loops():\n",
    "            G = remove_self_loops(G)\n",
    "        if select_lcc and not G.is_connected():\n",
    "            G = largest_connected_components(G, 1)\n",
    "        return G\n",
    "\n",
    "    def unpack(self) -> Tuple[sp.csr_matrix,\n",
    "                              Union[np.ndarray, sp.csr_matrix],\n",
    "                              Union[np.ndarray, sp.csr_matrix]]:\n",
    "        \"\"\"Return the (A, X, E, z) quadruplet.\n",
    "        \"\"\"\n",
    "        return self.adj_matrix, self.attr_matrix, self.labels\n",
    "\n",
    "    def to_flat_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return flat dictionary containing all SparseGraph properties.\n",
    "        \"\"\"\n",
    "        data_dict = {}\n",
    "        for key in sparse_graph_properties:\n",
    "            val = getattr(self, key)\n",
    "            if sp.isspmatrix(val):\n",
    "                data_dict['{}.data'.format(key)] = val.data\n",
    "                data_dict['{}.indices'.format(key)] = val.indices\n",
    "                data_dict['{}.indptr'.format(key)] = val.indptr\n",
    "                data_dict['{}.shape'.format(key)] = val.shape\n",
    "            else:\n",
    "                data_dict[key] = val\n",
    "        return data_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def from_flat_dict(data_dict: Dict[str, Any]) -> 'SparseGraph':\n",
    "        \"\"\"Initialize SparseGraph from a flat dictionary.\n",
    "        \"\"\"\n",
    "        init_dict = {}\n",
    "        del_entries = []\n",
    "\n",
    "        # Construct sparse matrices\n",
    "        for key in data_dict.keys():\n",
    "            if key.endswith('_data') or key.endswith('.data'):\n",
    "                if key.endswith('_data'):\n",
    "                    sep = '_'\n",
    "                    warnings.warn(\n",
    "                            \"The separator used for sparse matrices during export (for .npz files) \"\n",
    "                            \"is now '.' instead of '_'. Please update (re-save) your stored graphs.\",\n",
    "                            DeprecationWarning, stacklevel=2)\n",
    "                else:\n",
    "                    sep = '.'\n",
    "                matrix_name = key[:-5]\n",
    "                mat_data = key\n",
    "                mat_indices = '{}{}indices'.format(matrix_name, sep)\n",
    "                mat_indptr = '{}{}indptr'.format(matrix_name, sep)\n",
    "                mat_shape = '{}{}shape'.format(matrix_name, sep)\n",
    "                if matrix_name == 'adj' or matrix_name == 'attr':\n",
    "                    warnings.warn(\n",
    "                            \"Matrices are exported (for .npz files) with full names now. \"\n",
    "                            \"Please update (re-save) your stored graphs.\",\n",
    "                            DeprecationWarning, stacklevel=2)\n",
    "                    matrix_name += '_matrix'\n",
    "                init_dict[matrix_name] = sp.csr_matrix(\n",
    "                        (data_dict[mat_data],\n",
    "                         data_dict[mat_indices],\n",
    "                         data_dict[mat_indptr]),\n",
    "                        shape=data_dict[mat_shape])\n",
    "                del_entries.extend([mat_data, mat_indices, mat_indptr, mat_shape])\n",
    "\n",
    "        # Delete sparse matrix entries\n",
    "        for del_entry in del_entries:\n",
    "            del data_dict[del_entry]\n",
    "\n",
    "        # Load everything else\n",
    "        for key, val in data_dict.items():\n",
    "            if ((val is not None) and (None not in val)):\n",
    "                init_dict[key] = val\n",
    "\n",
    "        # Check if the dictionary contains only entries in sparse_graph_properties\n",
    "        unknown_keys = [key for key in init_dict.keys() if key not in sparse_graph_properties]\n",
    "        if len(unknown_keys) > 0:\n",
    "            raise ValueError(\"Input dictionary contains keys that are not SparseGraph properties ({}).\"\n",
    "                             .format(unknown_keys))\n",
    "\n",
    "        return SparseGraph(**init_dict)\n",
    "\n",
    "def create_subgraph(\n",
    "        sparse_graph: SparseGraph,\n",
    "        _sentinel: None = None,\n",
    "        nodes_to_remove: np.ndarray = None,\n",
    "        nodes_to_keep: np.ndarray = None\n",
    "        ) -> SparseGraph:\n",
    "    \"\"\"Create a graph with the specified subset of nodes.\n",
    "\n",
    "    Exactly one of (nodes_to_remove, nodes_to_keep) should be provided, while the other stays None.\n",
    "    Note that to avoid confusion, it is required to pass node indices as named arguments to this function.\n",
    "\n",
    "    The subgraph partially points to the old graph's data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sparse_graph\n",
    "        Input graph.\n",
    "    _sentinel\n",
    "        Internal, to prevent passing positional arguments. Do not use.\n",
    "    nodes_to_remove\n",
    "        Indices of nodes that have to removed.\n",
    "    nodes_to_keep\n",
    "        Indices of nodes that have to be kept.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    SparseGraph\n",
    "        Graph with specified nodes removed.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check that arguments are passed correctly\n",
    "    if _sentinel is not None:\n",
    "        raise ValueError(\"Only call `create_subgraph` with named arguments',\"\n",
    "                         \" (nodes_to_remove=...) or (nodes_to_keep=...).\")\n",
    "    if nodes_to_remove is None and nodes_to_keep is None:\n",
    "        raise ValueError(\"Either nodes_to_remove or nodes_to_keep must be provided.\")\n",
    "    elif nodes_to_remove is not None and nodes_to_keep is not None:\n",
    "        raise ValueError(\"Only one of nodes_to_remove or nodes_to_keep must be provided.\")\n",
    "    elif nodes_to_remove is not None:\n",
    "        nodes_to_keep = [i for i in range(sparse_graph.num_nodes()) if i not in nodes_to_remove]\n",
    "    elif nodes_to_keep is not None:\n",
    "        nodes_to_keep = sorted(nodes_to_keep)\n",
    "    else:\n",
    "        raise RuntimeError(\"This should never happen.\")\n",
    "\n",
    "    sparse_graph.adj_matrix = sparse_graph.adj_matrix[nodes_to_keep][:, nodes_to_keep]\n",
    "    if sparse_graph.attr_matrix is not None:\n",
    "        sparse_graph.attr_matrix = sparse_graph.attr_matrix[nodes_to_keep]\n",
    "    if sparse_graph.labels is not None:\n",
    "        sparse_graph.labels = sparse_graph.labels[nodes_to_keep]\n",
    "    if sparse_graph.node_names is not None:\n",
    "        sparse_graph.node_names = sparse_graph.node_names[nodes_to_keep]\n",
    "    return sparse_graph\n",
    "\n",
    "\n",
    "def largest_connected_components(sparse_graph: SparseGraph, n_components: int = 1) -> SparseGraph:\n",
    "    \"\"\"Select the largest connected components in the graph.\n",
    "\n",
    "    Changes are returned in a partially new SparseGraph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sparse_graph\n",
    "        Input graph.\n",
    "    n_components\n",
    "        Number of largest connected components to keep.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    SparseGraph\n",
    "        Subgraph of the input graph where only the nodes in largest n_components are kept.\n",
    "\n",
    "    \"\"\"\n",
    "    _, component_indices = sp.csgraph.connected_components(sparse_graph.adj_matrix)\n",
    "    component_sizes = np.bincount(component_indices)\n",
    "    components_to_keep = np.argsort(component_sizes)[::-1][:n_components]  # reverse order to sort descending\n",
    "    nodes_to_keep = [\n",
    "        idx for (idx, component) in enumerate(component_indices) if component in components_to_keep\n",
    "    ]\n",
    "    return create_subgraph(sparse_graph, nodes_to_keep=nodes_to_keep)\n",
    "\n",
    "\n",
    "def remove_self_loops(sparse_graph: SparseGraph) -> SparseGraph:\n",
    "    \"\"\"Remove self loops (diagonal entries in the adjacency matrix).\n",
    "\n",
    "    Changes are returned in a partially new SparseGraph.\n",
    "\n",
    "    \"\"\"\n",
    "    num_self_loops = (~np.isclose(sparse_graph.adj_matrix.diagonal(), 0)).sum()\n",
    "    if num_self_loops > 0:\n",
    "        sparse_graph.adj_matrix = sparse_graph.adj_matrix.tolil()\n",
    "        sparse_graph.adj_matrix.setdiag(0)\n",
    "        sparse_graph.adj_matrix = sparse_graph.adj_matrix.tocsr()\n",
    "        warnings.warn(\"{0} self loops removed\".format(num_self_loops))\n",
    "\n",
    "    return sparse_graph\n",
    "\n",
    "def load_from_npz(file_name: str) -> SparseGraph:\n",
    "    \"\"\"Load a SparseGraph from a Numpy binary file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name\n",
    "        Name of the file to load.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    SparseGraph\n",
    "        Graph in sparse matrix format.\n",
    "\n",
    "    \"\"\"\n",
    "    with np.load(file_name, allow_pickle=True) as loader:\n",
    "        loader = dict(loader)\n",
    "        dataset = SparseGraph.from_flat_dict(loader)\n",
    "    return dataset\n",
    "\n",
    "def load_dataset(name: str,\n",
    "                 directory: Union[Path, str] = data_dir\n",
    "                 ) -> SparseGraph:\n",
    "    \"\"\"Load a dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name\n",
    "        Name of the dataset to load.\n",
    "    directory\n",
    "        Path to the directory where the datasets are stored.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    SparseGraph\n",
    "        The requested dataset in sparse format.\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(directory, str):\n",
    "        directory = Path(directory)\n",
    "    if not name.endswith('.npz'):\n",
    "        name += '.npz'\n",
    "    path_to_file = directory / name\n",
    "    if path_to_file.exists():\n",
    "        return load_from_npz(path_to_file)\n",
    "    else:\n",
    "        raise ValueError(\"{} doesn't exist.\".format(path_to_file))\n",
    "\n",
    "def normalize_attributes(attr_matrix):\n",
    "    epsilon = 1e-12\n",
    "    if isinstance(attr_matrix, sp.csr_matrix):\n",
    "        attr_norms = spla.norm(attr_matrix, ord=1, axis=1)\n",
    "        attr_invnorms = 1 / np.maximum(attr_norms, epsilon)\n",
    "        attr_mat_norm = attr_matrix.multiply(attr_invnorms[:, np.newaxis])\n",
    "    else:\n",
    "        attr_norms = np.linalg.norm(attr_matrix, ord=1, axis=1)\n",
    "        attr_invnorms = 1 / np.maximum(attr_norms, epsilon)\n",
    "        attr_mat_norm = attr_matrix * attr_invnorms[:, np.newaxis]\n",
    "    return attr_mat_norm\n",
    "\n",
    "def get_dataset(name: str, use_lcc: bool = True) -> InMemoryDataset:\n",
    "    \"\"\"\n",
    "    :param name: The name of the dataset\n",
    "    :param use_lcc: Largest Connected Component\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dataset = InMemoryDataset\n",
    "    graph = load_dataset(name)\n",
    "    graph.standardize(select_lcc=use_lcc)  # This was changed from =True to use_lcc by JK\n",
    "    new_y = torch.LongTensor(graph.labels)\n",
    "    data = Data(\n",
    "        x=torch.FloatTensor(normalize_attributes(graph.attr_matrix).toarray()),\n",
    "        edge_index=torch.LongTensor(graph.get_edgeid_to_idx_array().T),\n",
    "        y=new_y,\n",
    "        train_mask=torch.zeros(new_y.size(0), dtype=torch.bool),\n",
    "        test_mask=torch.zeros(new_y.size(0), dtype=torch.bool),\n",
    "        val_mask=torch.zeros(new_y.size(0), dtype=torch.bool)\n",
    "    )\n",
    "    dataset.data = data\n",
    "    dataset.name = name\n",
    "    dataset.num_classes = len(np.unique(new_y))\n",
    "    return dataset\n",
    "\n",
    "def set_train_val_test_split(\n",
    "        seed: int,\n",
    "        data: Data,\n",
    "        num_development: int = 1500,\n",
    "        num_per_class: int = 20) -> Data:\n",
    "    rnd_state = np.random.RandomState(development_seed)\n",
    "    num_nodes = data.y.shape[0]\n",
    "    development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
    "    test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
    "\n",
    "    train_idx = []\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    for c in range(data.y.max() + 1):\n",
    "        class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
    "        train_idx.extend(rnd_state.choice(class_idx, num_per_class, replace=False))\n",
    "\n",
    "    val_idx_tmp = [i for i in development_idx if i not in train_idx]\n",
    "\n",
    "    val_idx = rnd_state.choice(val_idx_tmp, 500, replace=False)\n",
    "\n",
    "    def get_mask(idx):\n",
    "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        mask[idx] = 1\n",
    "        return mask\n",
    "\n",
    "    data.train_mask = get_mask(train_idx)\n",
    "    data.val_mask = get_mask(val_idx)\n",
    "    data.test_mask = get_mask(test_idx)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will define the parameters that one can modify to create the `config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config(\n",
    "    dataset: str,                                                                           # Only the dataset file name, without the .npz ending.\n",
    "    device : torch.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),     # Can override this manually as torch.device object.\n",
    "    \n",
    "    # Gumbel Parameters\n",
    "    learn_temp=False,                                                                       # Whether to use a temperature network\n",
    "    temp_model_type: ModelType.from_string=ModelType.LIN,                                   # Which temperature network architecture to use\n",
    "    tau0: float=0.5,                                                                        # Which temperature value to start at\n",
    "    temp: float=0.01,                                                                       # Which temperature to use if learn_temp == False\n",
    "\n",
    "    # Optimization Parameters\n",
    "    max_epochs: int=1000,                                                                   # Max numbers of epochs for early stopping\n",
    "    patience: int=100,                                                                      # NEW: Early Stopping\n",
    "    lr: float=1e-2,                                                                         # Learning rate for the optimizer\n",
    "    dropout: float=0.2,                                                                     # Dropout probability\n",
    "    weight_decay: float=0.0,                                                                # Weight decay for the optimizer\n",
    "\n",
    "    # Graph (Environment) Network Parameters\n",
    "    env_model_type: ModelType.from_string=ModelType.MEAN_GNN,                               # Environment architecture for message passing\n",
    "    env_num_layers: int=3,                                                                  # Number of message passing layers\n",
    "    env_dim: int=128,                                                                       # Feature size per node\n",
    "    skip=False,                                                                             # Whether to introduce a residual connection in the environment network\n",
    "    layer_norm=False,                                                                       # Whether to use layer normalization\n",
    "    dec_num_layers: int=1,                                                                  # Number of graph decoder layers\n",
    "    pos_enc: PosEncoder.from_string=PosEncoder.NONE,                                        # Which positional encoder to use, fixed for our experiments\n",
    "\n",
    "    # Policy Network Parameters\n",
    "    act_model_type: ModelType=ModelType.MEAN_GNN,                                           # Which architecture to use for the policy network\n",
    "    act_num_layers: int=1,                                                                  # Number of hidden layers of the policy network\n",
    "    act_dim: int=16,                                                                        # Hidden dimension of the policy network\n",
    "\n",
    "    # Reproduction\n",
    "    seed: int=0,                                                                            # Which starting seed to use for the experiments\n",
    "    num_folds: int=5,                                                                        # How many folds to use for training\n",
    "\n",
    "    # Precision\n",
    "    decimal: int=2                                                                          # Numerical precision to display (value still stored at full precision)\n",
    "):\n",
    "    return {\n",
    "        \"dataset\" : dataset,\n",
    "        \"device\" : device,\n",
    "        \"learn_temp\" : learn_temp,\n",
    "        \"temp_model_type\" : temp_model_type,\n",
    "        \"tau0\" : tau0,\n",
    "        \"temp\" : temp,\n",
    "        \"max_epochs\" : max_epochs,\n",
    "        \"patience\" : patience,\n",
    "        \"lr\" : lr,\n",
    "        \"dropout\" : dropout,\n",
    "        \"env_model_type\" : env_model_type,\n",
    "        \"env_num_layers\" : env_num_layers,\n",
    "        \"env_dim\" : env_dim,\n",
    "        \"skip\" : skip,\n",
    "        \"layer_norm\" : layer_norm,\n",
    "        \"dec_num_layers\" : dec_num_layers,\n",
    "        \"pos_enc\" : pos_enc,\n",
    "        \"act_model_type\" : act_model_type,\n",
    "        \"act_num_layers\" : act_num_layers,\n",
    "        \"act_dim\" : act_dim,\n",
    "        \"seed\" : seed,\n",
    "        \"num_folds\" : num_folds,\n",
    "        \"weight_decay\" : weight_decay,\n",
    "        \"decimal\" : decimal\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the `DataSet` class which contains the data and some other information similar to what the authors' code would have given as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    \"\"\"\n",
    "        Class to keep the data and various methods similar to the original author's code.\n",
    "    \"\"\"\n",
    "    def __init__(self, data: Data):\n",
    "        self.data = data\n",
    "        # For our datasets the following values are all fixed with respect to the author's code.\n",
    "        self.family = True\n",
    "        self.is_node_based = True\n",
    "        self.not_synthetic = True\n",
    "        self.is_expressivity = False\n",
    "        self.clip_grad = False\n",
    "        self.dataset_encoders = DataSetEncoders.NONE\n",
    "        self.num_after_decimal = 2\n",
    "        self.env_activation_type = ActivationType.RELU\n",
    "\n",
    "    def gin_mlp_func(self) -> Callable:\n",
    "        def mlp_func(in_channels: int, out_channels: int, bias: bool):\n",
    "            return torch.nn.Sequential(\n",
    "                torch.nn.Linear(in_channels, 2 * in_channels, bias=bias),\n",
    "                torch.nn.BatchNorm1d(2 * in_channels),\n",
    "                torch.nn.ReLU(), torch.nn.Linear(2 * in_channels, out_channels, bias=bias)\n",
    "            )\n",
    "        return mlp_func\n",
    "    \n",
    "    def get_split_mask(self, split_mask_name: str) -> Tensor:\n",
    "        if hasattr(self.data, split_mask_name):\n",
    "            return getattr(self.data, split_mask_name)\n",
    "        else:\n",
    "            return torch.ones(size=(self.data.x.shape[0],), dtype=torch.bool)\n",
    "    \n",
    "    def get_edge_ratio_node_mask(self, split_mask_name: str) -> Tensor:\n",
    "        if hasattr(self.data, split_mask_name):\n",
    "            return getattr(self.data, split_mask_name)\n",
    "        else:\n",
    "            return torch.ones(size=(self.data.x.shape[0],), dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Action Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action network represents the policy that maps to logits (keep, not keep) for each edge. Its components are instantiated from the `ModelType.get_component_list` function according to the parameters set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_act_net(action_args : dict) -> torch.nn.ModuleList:\n",
    "    model_type = action_args[\"model_type\"]\n",
    "    env_dim = action_args[\"env_dim\"]\n",
    "    hidden_dim = action_args[\"hidden_dim\"]\n",
    "    num_layers = action_args[\"num_layers\"]\n",
    "    gin_mlp_func = action_args[\"gin_mlp_func\"]\n",
    "\n",
    "    net = model_type.get_component_list(in_dim=env_dim,\n",
    "                                        hidden_dim=hidden_dim,\n",
    "                                        out_dim=2, # 2 because we can sample (edge, no edge)\n",
    "                                        num_layers=num_layers,\n",
    "                                        bias=True,\n",
    "                                        edges_required=False,\n",
    "                                        gin_mlp_func=gin_mlp_func)\n",
    "\n",
    "    return torch.nn.ModuleList(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ActionNet` class is an extension of a PyTorch Module class and contains the architecture as given by `load_act_net` and the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNet(torch.nn.Module):\n",
    "    def __init__(self, action_args: dict):\n",
    "        \"\"\"\n",
    "            Create a model which represents the agent's policy.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_layers = action_args[\"num_layers\"]\n",
    "        self.net = load_act_net(action_args=action_args)\n",
    "        self.dropout = torch.nn.Dropout(action_args[\"dropout\"])\n",
    "        self.act = action_args[\"act_type\"].get()\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj, env_edge_attr: OptTensor, act_edge_attr: OptTensor) -> Tensor:\n",
    "        edge_attrs = [env_edge_attr] + (self.num_layers - 1) * [act_edge_attr]\n",
    "        for (edge_attr, layer) in zip(edge_attrs[:-1], self.net[:-1]):\n",
    "            x = layer(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "            x = self.dropout(x)\n",
    "            x = self.act(x)\n",
    "        x = self.net[-1](x=x, edge_index=edge_index, edge_attr=edge_attrs[-1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Environment Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to `load_act_net`, the `load_env_net` function will be used in the `CoGNN` class (see below) to instantiate the environment network's layers. It contains a node encoder, message passing layers and a classifier (decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_env_net(env_args : dict) -> torch.nn.ModuleList:\n",
    "    in_dim = env_args[\"in_dim\"]\n",
    "    env_dim = env_args[\"env_dim\"]\n",
    "    num_layers = env_args[\"num_layers\"]\n",
    "    gin_mlp_func = env_args[\"gin_mlp_func\"]\n",
    "    dec_num_layers = env_args[\"dec_num_layers\"]\n",
    "    dropout = env_args[\"dropout\"]\n",
    "    act_type = env_args[\"act_type\"]\n",
    "    out_dim = env_args[\"out_dim\"]\n",
    "\n",
    "    enc_list = [env_args[\"dataset_encoders\"].node_encoder(in_dim=in_dim, emb_dim=env_dim)]\n",
    "\n",
    "    component_list = env_args[\"model_type\"].get_component_list(in_dim=env_dim,\n",
    "                                                               hidden_dim=env_dim,\n",
    "                                                               out_dim=env_dim,\n",
    "                                                               num_layers=num_layers,\n",
    "                                                               bias=True,\n",
    "                                                               edges_required=True,\n",
    "                                                               gin_mlp_func=gin_mlp_func)\n",
    "\n",
    "    if dec_num_layers > 1:\n",
    "        mlp_list = (dec_num_layers - 1) * [torch.nn.Linear(env_dim, env_dim), torch.nn.Dropout(dropout), act_type.nn()]\n",
    "        mlp_list = mlp_list + [torch.nn.Linear(env_dim, out_dim)]\n",
    "        dec_list = [torch.nn.Sequential(*mlp_list)]\n",
    "    else:\n",
    "        dec_list = [torch.nn.Linear(env_dim, out_dim)]\n",
    "\n",
    "    return torch.nn.ModuleList(enc_list + component_list + dec_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## CoGNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally we can enable dynamic temperature calculation for the Gumbel Softmax by using a temperature network `TempSoftPlus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempSoftPlus(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Network for dynamic temperature.\n",
    "    \"\"\"\n",
    "    def __init__(self, gumbel_args: dict, env_dim: int):\n",
    "        super(TempSoftPlus, self).__init__()\n",
    "        model_list = gumbel_args[\"temp_model_type\"].get_component_list(in_dim=env_dim,\n",
    "                                                                       hidden_dim=env_dim,\n",
    "                                                                       out_dim=1,\n",
    "                                                                       num_layers=1,\n",
    "                                                                       bias=False,\n",
    "                                                                       edges_required=False,\n",
    "                                                                       gin_mlp_func=gumbel_args[\"gin_mlp_func\"])\n",
    "        self.linear_model = torch.nn.ModuleList(model_list)\n",
    "        self.softplus = torch.nn.Softplus(beta=1)\n",
    "        self.tau0 = gumbel_args[\"tau0\"]\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj, edge_attr: Tensor):\n",
    "        x = self.linear_model[0](x=x, edge_index=edge_index,edge_attr = edge_attr)\n",
    "        x = self.softplus(x) + self.tau0\n",
    "        temp = x.pow_(-1)\n",
    "        return temp.masked_fill_(temp == float(\"inf\"), 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will implement the `CoGNN` architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoGNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        CoGNN model class.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        gumbel_args: dict,\n",
    "        env_args: dict,\n",
    "        action_args: dict\n",
    "    ):\n",
    "        super(CoGNN, self).__init__()\n",
    "        self.env_args = env_args\n",
    "        self.learn_temp = gumbel_args[\"learn_temp\"]\n",
    "        if self.learn_temp:\n",
    "            self.temp_model = TempSoftPlus(gumbel_args=gumbel_args, env_dim=env_args[\"env_dim\"])\n",
    "        self.temp = gumbel_args[\"temp\"]\n",
    "\n",
    "        self.num_layers = env_args[\"num_layers\"]\n",
    "        self.env_net = load_env_net(env_args=env_args)\n",
    "        self.use_encoders = env_args[\"dataset_encoders\"].use_encoders()\n",
    "\n",
    "        layer_norm_cls = torch.nn.LayerNorm if env_args[\"layer_norm\"] else torch.nn.Identity\n",
    "        self.hidden_layer_norm = layer_norm_cls(env_args[\"env_dim\"])\n",
    "        self.skip = env_args[\"skip\"]\n",
    "        self.drop_ratio = env_args[\"dropout\"]\n",
    "        self.dropout = torch.nn.Dropout(p=self.drop_ratio)\n",
    "        self.act = env_args[\"act_type\"].get()\n",
    "        self.in_act_net = ActionNet(action_args=action_args)\n",
    "        self.out_act_net = ActionNet(action_args=action_args)\n",
    "\n",
    "        # Encoder types\n",
    "        self.dataset_encoder = env_args[\"dataset_encoders\"]\n",
    "        self.env_bond_encoder = self.dataset_encoder.edge_encoder(emb_dim=env_args[\"env_dim\"], model_type=env_args[\"model_type\"])\n",
    "        self.act_bond_encoder = self.dataset_encoder.edge_encoder(emb_dim=action_args[\"hidden_dim\"], model_type=action_args[\"model_type\"])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        edge_index: Adj,\n",
    "        pestat,\n",
    "        edge_attr: OptTensor = None,\n",
    "        edge_ratio_node_mask: OptTensor = None\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        result = 0\n",
    "        node_states = []\n",
    "\n",
    "        calc_stats = edge_ratio_node_mask is not None\n",
    "        if calc_stats:\n",
    "            edge_ratio_edge_mask = edge_ratio_node_mask[edge_index[0]] & edge_ratio_node_mask[edge_index[1]]\n",
    "            edge_ratio_list = []\n",
    "\n",
    "        # bond encode\n",
    "        if edge_attr is None or self.env_bond_encoder is None:\n",
    "            env_edge_embedding = None\n",
    "        else:\n",
    "            env_edge_embedding = self.env_bond_encoder(edge_attr)\n",
    "        if edge_attr is None or self.act_bond_encoder is None:\n",
    "            act_edge_embedding = None\n",
    "        else:\n",
    "            act_edge_embedding = self.act_bond_encoder(edge_attr)\n",
    "\n",
    "        x = self.env_net[0](x, pestat) # (N, F) encoder\n",
    "        if not self.use_encoders:\n",
    "            x = self.dropout(x)\n",
    "            x = self.act(x)\n",
    "\n",
    "        for gnn_idx in range(self.num_layers):\n",
    "            x = self.hidden_layer_norm(x)\n",
    "\n",
    "            # action\n",
    "            in_logits = self.in_act_net(x=x, edge_index=edge_index, env_edge_attr=env_edge_embedding, act_edge_attr=act_edge_embedding) # (N, 2)\n",
    "            out_logits = self.out_act_net(x=x, edge_index=edge_index, env_edge_attr=env_edge_embedding, act_edge_attr=act_edge_embedding) # (N, 2)\n",
    "\n",
    "            temp = self.temp_model(x=x, edge_index=edge_index, edge_attr=env_edge_embedding) if self.learn_temp else self.temp\n",
    "            in_probs = torch.nn.functional.gumbel_softmax(logits=in_logits, tau=temp, hard=True)\n",
    "            out_probs = torch.nn.functional.gumbel_softmax(logits=out_logits, tau=temp, hard=True)\n",
    "            edge_weight = self.create_edge_weight(edge_index=edge_index, keep_in_prob=in_probs[:, 0], keep_out_prob=out_probs[:, 0])\n",
    "\n",
    "            # environment\n",
    "            out = self.env_net[1 + gnn_idx](x=x, edge_index=edge_index, edge_weight=edge_weight, edge_attr=env_edge_embedding)\n",
    "            out = self.dropout(out)\n",
    "            out = self.act(out)\n",
    "\n",
    "            if calc_stats:\n",
    "                edge_ratio = edge_weight[edge_ratio_edge_mask].sum() / edge_weight[edge_ratio_edge_mask].shape[0]\n",
    "                edge_ratio_list.append(edge_ratio.item())\n",
    "\n",
    "                # Node state distribution: 0 -> standard, 1 -> broadcast, 2 -> listen, 3 -> isolate.\n",
    "                active_edges = edge_index[:, edge_weight.bool()]\n",
    "                nodes_u, nodes_v = active_edges\n",
    "                broadcast_nodes = torch.unique(nodes_u)\n",
    "                listen_nodes = torch.unique(nodes_v)\n",
    "                standard_nodes = torch.tensor(list(set(broadcast_nodes.tolist()) & set(listen_nodes.tolist())), dtype=torch.long)\n",
    "                node_state = torch.full((x.shape[0],), fill_value=3) # initialize as isolate\n",
    "                node_state[broadcast_nodes] = 1\n",
    "                node_state[listen_nodes] = 2\n",
    "                node_state[standard_nodes] = 0\n",
    "                node_states.append(node_state)\n",
    "\n",
    "            if self.skip:\n",
    "                x = x + out\n",
    "            else:\n",
    "                x = out\n",
    "\n",
    "        x = self.hidden_layer_norm(x)\n",
    "        # Note: The authors use pooling but for our datasets this would always be None so we removed it from the code to simplify.\n",
    "        x = self.env_net[-1](x) # decoder\n",
    "        result = result + x\n",
    "\n",
    "        # This part is not used for our visualization.\n",
    "        if calc_stats:\n",
    "            edge_ratios = torch.tensor(edge_ratio_list, device=x.device)\n",
    "        else:\n",
    "            edge_ratios = -1 * torch.ones(size=(self.num_layers,), device=x.device)\n",
    "\n",
    "        return result, edge_ratios, node_states\n",
    "\n",
    "    def create_edge_weight(\n",
    "        self,\n",
    "        edge_index: Adj,\n",
    "        keep_in_prob: Tensor,\n",
    "        keep_out_prob: Tensor\n",
    "    ) -> Tensor:\n",
    "        u, v = edge_index\n",
    "        edge_in_prob = keep_in_prob[v]\n",
    "        edge_out_prob = keep_out_prob[u]\n",
    "        return edge_in_prob * edge_out_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training and Evaluation Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Epoch Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_CoGNN` function contains the code for one training epoch/step and will simply return the updated model. The training data is loaded via a training mask that is set during the data loading when the `Data` object is instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CoGNN(\n",
    "    model : torch.nn.Module,\n",
    "    task_loss : torch.nn.CrossEntropyLoss,\n",
    "    dataset : DataSet,\n",
    "    optimizer : torch.optim.Optimizer,\n",
    "    config : dict,\n",
    "    split_mask_name : str=\"train_mask\",\n",
    "):\n",
    "    \"\"\"\n",
    "        Training loop for one epoch.\n",
    "    \"\"\"\n",
    "    device = config[\"device\"]\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    data = dataset.data.to(device)\n",
    "\n",
    "    train_mask = data.train_mask if hasattr(data, \"train_mask\") else torch.ones(data.num_nodes, dtype=torch.bool, device=data.x.device)\n",
    "    edge_ratio_node_mask = dataset.get_edge_ratio_node_mask(split_mask_name=split_mask_name).to(device=device)\n",
    "\n",
    "    predictions, _, _ = model(x=data.x,\n",
    "                              edge_index=data.edge_index,\n",
    "                              edge_attr=data.edge_attr if \"edge_attr\" in data else None,\n",
    "                              edge_ratio_node_mask=edge_ratio_node_mask,\n",
    "                              pestat=config[\"pos_enc\"].get_pe(data=data, device=device))\n",
    "\n",
    "    train_loss = task_loss(predictions[train_mask], data.y.to(device=device)[train_mask])\n",
    "    train_loss.backward()\n",
    "\n",
    "    if config[\"clip_grad\"]:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing the model evaluation function, we will first define how to compute the fractions of nodes in the 4 states `Standard` (broadcast and listen), `Broadcast` (broadcast, no listen), `Listen` (no broadcast, listen), and `Isolate` (no broadcast, no listen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_distribution(node_states : list) -> list:\n",
    "    distribution_per_layer = []\n",
    "    for layer in node_states:\n",
    "        #0 -> standard, 1 -> broadcast, 2 -> listen, 3 -> isolate\n",
    "        standard = layer[layer==0].shape[0]/layer.shape[0]\n",
    "        broadcast = layer[layer==1].shape[0]/layer.shape[0]\n",
    "        listen = layer[layer==2].shape[0]/layer.shape[0]\n",
    "        isolate = layer[layer==3].shape[0]/layer.shape[0]\n",
    "        assert abs(1.0 - sum([standard, broadcast, listen, isolate])) < 1e-6 # check that the distribution sums to 1\n",
    "        distribution_per_layer.append((standard, broadcast, listen, isolate))\n",
    "    return distribution_per_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `evaluate_CoGNN` function takes a model and returns an accuracy score for a given mask (training, validation, testing), the loss, the average edge degree for each layer and the node states for each layer. Since we are computing the node state distribution there is no real need for the edge ratios anymore but we left it in the code for compatability reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_CoGNN(\n",
    "    model : torch.nn.Module,\n",
    "    metric : Metric,\n",
    "    dataset : DataSet,\n",
    "    config : dict,\n",
    "    split_mask_name : str=None,\n",
    "    node_mask : np.ndarray=None\n",
    ") -> Tuple[float, float, torch.Tensor, list]:\n",
    "    \"\"\"\n",
    "        Evaluation function of the model.\n",
    "    \"\"\"\n",
    "    device = config[\"device\"]\n",
    "\n",
    "    model.eval()\n",
    "    data = dataset.data.to(device)\n",
    "\n",
    "    mask = dataset.get_split_mask(split_mask_name=split_mask_name).to(device=device) if node_mask is None else node_mask if isinstance(node_mask, torch.Tensor) else  torch.tensor(node_mask)\n",
    "    edge_ratio_node_mask = dataset.get_edge_ratio_node_mask(split_mask_name=split_mask_name).to(device=device)\n",
    "    edge_attr = data.edge_attr.to(device) if data.edge_attr is not None else None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions, edge_ratios, node_states = model(x=data.x,\n",
    "                                                      edge_index=data.edge_index,\n",
    "                                                      edge_attr=edge_attr,\n",
    "                                                      edge_ratio_node_mask=edge_ratio_node_mask,\n",
    "                                                      pestat=config[\"pos_enc\"].get_pe(data=data, device=device))\n",
    "        \n",
    "        eval_loss = metric.task_loss(predictions[mask], data.y[mask])\n",
    "    \n",
    "    scores_np = predictions[mask].detach().cpu().numpy()\n",
    "    labels_np = data.y[mask].detach().cpu().numpy()\n",
    "\n",
    "    accuracy = metric.apply_metric(scores=scores_np, target=labels_np)\n",
    "    loss = eval_loss.item()\n",
    "\n",
    "    node_state_distribution = get_state_distribution(node_states=node_states)\n",
    "\n",
    "    return accuracy, loss, edge_ratios, node_state_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Epoch Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the training and evaluation function, we arrive at the function that trains the model for one fold, namely `train_fold_CoGNN`. It implements Early Stopping with a given maximum number of training epochs. The sensitivity for resetting the patience is zero, i.e. any improvement is accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold_CoGNN(\n",
    "    config : dict,\n",
    "    dataset: DataSet,\n",
    "    model : torch.nn.Module,\n",
    "    optimizer : torch.optim.Optimizer,\n",
    "    pbar : tqdm.std.tqdm,\n",
    "    num_fold: int\n",
    ") -> Tuple[LossesAndMetrics, OptTensor, List]:\n",
    "    \"\"\"\n",
    "        Training loop for one fold over multiple epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    task_loss = config[\"metric\"].task_loss\n",
    "\n",
    "    best_losses_n_metrics = config[\"metric\"].get_worst_losses_n_metrics\n",
    "    patience_counter = 0\n",
    "    for epoch in range(config[\"max_epochs\"]):\n",
    "        model = train_CoGNN(model=model, task_loss=task_loss, dataset=dataset, optimizer=optimizer, config=config)\n",
    "\n",
    "        train_score, train_loss, _, _ = evaluate_CoGNN(model=model,\n",
    "                                                       metric=config[\"metric\"],\n",
    "                                                       dataset=dataset,\n",
    "                                                       config=config,\n",
    "                                                       split_mask_name=\"train_mask\")\n",
    "\n",
    "        val_score, val_loss, _, _ = evaluate_CoGNN(model=model,\n",
    "                                                   metric=config[\"metric\"],\n",
    "                                                   dataset=dataset,\n",
    "                                                   config=config,\n",
    "                                                   split_mask_name=\"val_mask\")\n",
    "\n",
    "        test_score, test_loss, _, _ = evaluate_CoGNN(model=model,\n",
    "                                                               metric=config[\"metric\"],\n",
    "                                                               dataset=dataset,\n",
    "                                                               config=config,\n",
    "                                                               split_mask_name=\"test_mask\")\n",
    "\n",
    "        losses_n_metrics = LossesAndMetrics(train_loss=train_loss,\n",
    "                                            val_loss=val_loss,\n",
    "                                            test_loss=test_loss,\n",
    "                                            train_metric=train_score,\n",
    "                                            val_metric=val_score,\n",
    "                                            test_metric=test_score)\n",
    "        \n",
    "        if config[\"metric\"].src_better_than_other(src=losses_n_metrics.val_metric, other=best_losses_n_metrics.val_metric):\n",
    "            best_losses_n_metrics = losses_n_metrics\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        log_str = f\"Fold: {1+num_fold}, Epoch: {epoch}, \"\n",
    "        log_str += f\"Train.Acc.={round(losses_n_metrics.train_metric, config[\"decimal\"])}, \"\n",
    "        log_str += f\"Val.Acc.={round(losses_n_metrics.val_metric, config[\"decimal\"])}, \"\n",
    "        log_str += f\"Train.Loss={round(losses_n_metrics.train_loss, config[\"decimal\"])}, \"\n",
    "        log_str += f\"Val.Loss={round(losses_n_metrics.val_loss, config[\"decimal\"])}\"\n",
    "        pbar.set_description(log_str)\n",
    "        pbar.update(n=1)\n",
    "\n",
    "        if patience_counter == config[\"patience\"]:\n",
    "            pbar.set_description(f\"Training stopped after {epoch} epochs following no improvement after {patience_counter} epochs.\")\n",
    "            break\n",
    "    \n",
    "    _, _, _, node_states = evaluate_CoGNN(model=model,\n",
    "                                          metric=config[\"metric\"],\n",
    "                                          dataset=dataset,\n",
    "                                          config=config,\n",
    "                                          split_mask_name=\"test_mask\")\n",
    "\n",
    "    return best_losses_n_metrics, node_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_model_CoGNN` function represents the next abstraction layer where we can run n-fold training by making calls to the `train_fold_CoGNN` function, each fold is run with an incremental seed, i.e. starting seed + fold number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_CoGNN(\n",
    "    data : Data,\n",
    "    config : dict,\n",
    "    num_folds : int\n",
    ") -> Tuple[torch.nn.Module, dict]:\n",
    "    \"\"\"\n",
    "        Training loop for n-fold with multiple epochs.\n",
    "    \"\"\"\n",
    "    decimal = config[\"decimal\"]\n",
    "    seeds = [config[\"seed\"] + n for n in range(num_folds)]\n",
    "\n",
    "    dataset = DataSet(data=data)\n",
    "    config[\"clip_grad\"] = dataset.clip_grad\n",
    "\n",
    "    gin_mlp_func = dataset.gin_mlp_func()\n",
    "    env_act_type = dataset.env_activation_type\n",
    "    dataset_encoder = dataset.dataset_encoders\n",
    "    out_dim = config[\"metric\"].get_out_dim(data=data)\n",
    "\n",
    "    gumbel = [\"learn_temp\", \"temp_model_type\", \"tau0\", \"temp\"]\n",
    "    gumbel_add = {\"gin_mlp_func\" :  gin_mlp_func}\n",
    "    gumbel_args = {key: config.get(key) for key in gumbel}\n",
    "    gumbel_args.update(gumbel_add)\n",
    "\n",
    "    env = [\"env_dim\", \"layer_norm\", \"skip\", \"batch_norm\", \"dropout\", \"metric\", \"dec_num_layers\", \"pos_enc\"]\n",
    "    env_add = {\"model_type\" :       config[\"env_model_type\"],\n",
    "               \"num_layers\" :       config[\"env_num_layers\"],\n",
    "               \"act_type\" :         env_act_type,\n",
    "               \"in_dim\" :           data.x.shape[1],\n",
    "               \"out_dim\" :          out_dim,\n",
    "               \"gin_mlp_func\" :     gin_mlp_func,\n",
    "               \"dataset_encoders\" : dataset_encoder}\n",
    "    env_args = {key: config.get(key) for key in env}\n",
    "    env_args.update(env_add)\n",
    "\n",
    "    action = [\"dropout\", \"env_dim\"]\n",
    "    action_add = {\"model_type\" :    config[\"act_model_type\"],\n",
    "                  \"num_layers\" :    config[\"act_num_layers\"],\n",
    "                  \"hidden_dim\" :    config[\"act_dim\"],\n",
    "                  \"act_type\" :      ActivationType.RELU,\n",
    "                  \"gin_mlp_func\" :  gin_mlp_func}\n",
    "    action_args = {key: config.get(key) for key in action}\n",
    "    action_args.update(action_add)\n",
    "\n",
    "    metrics_list = []\n",
    "    node_states_list = []\n",
    "    for n in range(num_folds):\n",
    "        set_seed(seed=seeds[n])\n",
    "\n",
    "        model = CoGNN(gumbel_args=gumbel_args, env_args=env_args, action_args=action_args).to(device=config[\"device\"])\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "        with tqdm.tqdm(total=config[\"max_epochs\"], file=sys.stdout) as pbar:\n",
    "            best_losses_n_metrics, node_states = train_fold_CoGNN(config=config,\n",
    "                                                                  dataset=dataset,\n",
    "                                                                  model=model,\n",
    "                                                                  optimizer=optimizer,\n",
    "                                                                  pbar=pbar,\n",
    "                                                                  num_fold=n)\n",
    "        \n",
    "        print_str = f\"Fold {n+1}/{num_folds} Results:\\n\"\n",
    "        print_str += f\"Training Accuracy: {round(getattr(best_losses_n_metrics, \"train_metric\"), decimal)}, Training Loss: {round(getattr(best_losses_n_metrics, \"train_loss\"), decimal)}\\n\"\n",
    "        print_str += f\"Validation Accuracy: {round(getattr(best_losses_n_metrics, \"val_metric\"), decimal)}, Validation Loss: {round(getattr(best_losses_n_metrics, \"val_loss\"), decimal)}\\n\"\n",
    "        print_str += f\"Test Accuracy: {round(getattr(best_losses_n_metrics, \"test_metric\"), decimal)}, Test Loss: {round(getattr(best_losses_n_metrics, \"test_loss\"), decimal)}\\n\"\n",
    "        print(print_str)\n",
    "        metrics_list.append(best_losses_n_metrics.get_fold_metrics())\n",
    "        node_states_list.append(node_states)\n",
    "            \n",
    "    metrics_matrix = torch.stack(metrics_list, dim=0)  # (F, 3)\n",
    "    metrics_mean = torch.mean(metrics_matrix, dim=0).tolist()  # (3,)\n",
    "    \n",
    "    if num_folds > 1:\n",
    "        metrics_std = torch.std(metrics_matrix, dim=0).tolist()  # (3,)\n",
    "        print(\"---------------------------------------------------------------------------------------------------------------\\n\"\n",
    "              f\"Final Results:\\nAverage Training Accuracy={round(metrics_mean[0], decimal)}+-{round(metrics_std[0], decimal)}, \"\n",
    "              f\"Average Validation Accuracy={round(metrics_mean[1], decimal)}+-{round(metrics_std[1], decimal)}, \"\n",
    "              f\"Average Test Accuracy={round(metrics_mean[2], decimal)}+-{round(metrics_std[2], decimal)}\\n\"\n",
    "              \"---------------------------------------------------------------------------------------------------------------\\n\")\n",
    "    else:\n",
    "        print(\"---------------------------------------------------------------------------------------------------------------\\n\"\n",
    "              f\"Final Results:\\nAverage Training Accuracy={round(metrics_mean[0], decimal)}+-{round(0.0, decimal)}, \"\n",
    "              f\"Average Validation Accuracy={round(metrics_mean[1], decimal)}+-{round(0.0, decimal)}, \"\n",
    "              f\"Average Test Accuracy={round(metrics_mean[2], decimal)}+-{round(0.0, decimal)}\\n\"\n",
    "              \"---------------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "    return metrics_list, node_states_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `main_CoGNN` performs an optional grid search among all parameter combinations as defined in the code. Either with the best parameters found or with the parameters specified by the user, the n-fold training is then executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_CoGNN(\n",
    "    config : dict,\n",
    "    run_grid_search : bool=False\n",
    ") -> Tuple[torch.nn.Module, Data, dict]:\n",
    "    \"\"\"\n",
    "        Main function with optional grid search.\n",
    "    \"\"\"\n",
    "    device = config[\"device\"]\n",
    "\n",
    "    print(f\"Loading {config[\"dataset\"]} dataset...\")\n",
    "\n",
    "    # This is where we load the data\n",
    "    raw_data = get_dataset(name=config[\"dataset\"])\n",
    "    data = set_train_val_test_split(seed=config[\"seed\"], data=raw_data.data)\n",
    "    data = data.to(device)\n",
    "    \n",
    "    config[\"metric\"] = Metric(task=\"multiclass\", num_classes=raw_data.num_classes)\n",
    "\n",
    "    if run_grid_search:\n",
    "        print(\"Running grid search...\\n\")\n",
    "        # List of the parameters and the values to be searched across\n",
    "\n",
    "        lr_list = [1e-2, 1e-3]\n",
    "        env_num_layers_list = [1, 2, 3, 4, 5]\n",
    "\n",
    "        parameters = [lr_list,\n",
    "                      env_num_layers_list]\n",
    "        parameter_combinations = [list(p) for p in product(*parameters)]\n",
    "\n",
    "        results = []\n",
    "        total_combinations = len(parameter_combinations)\n",
    "        for idx, combination in enumerate(parameter_combinations):\n",
    "            config[\"lr\"] = combination[0]\n",
    "            config[\"env_num_layers\"] = combination[1]\n",
    "\n",
    "            result_dict = {\n",
    "                \"lr\" : combination[0],\n",
    "                \"env_num_layers\" : combination[1]\n",
    "            }\n",
    "\n",
    "            print(f\"Training combination {idx+1}/{total_combinations}:\\n\", result_dict)\n",
    "            metrics_list, _ = train_model_CoGNN(data=data, config=config, num_folds=1)\n",
    "\n",
    "            result_dict[\"metric(trainAcc,valAcc,testAcc)\"] = (metrics_list[-1][0].item(), metrics_list[-1][1].item(), metrics_list[-1][2].item())\n",
    "            results.append(result_dict)\n",
    "\n",
    "        # Store the gridsearch results\n",
    "        pandas.DataFrame(results).to_csv(f\"gridsearch_{config[\"dataset\"]}.csv\", index=False)\n",
    "        best_parameters = max(results, key=lambda x: x[\"metric(trainAcc,valAcc,testAcc)\"][2])\n",
    "\n",
    "        print(f\"Grid Search concluded. Best parameters: lr={best_parameters[\"lr\"]}, \"\n",
    "              f\"env_num_layers={best_parameters[\"env_num_layers\"]}, \"\n",
    "              f\"Accuracy={best_parameters[\"metric(trainAcc,valAcc,testAcc)\"][2]:.4f}\")\n",
    "\n",
    "        # config with best parameters\n",
    "        config[\"lr\"] = best_parameters[\"lr\"]\n",
    "        config[\"env_num_layers\"] = best_parameters[\"env_num_layers\"]\n",
    "\n",
    "    print(\"\\nTraining final model...\")\n",
    "    metrics_list, node_states_list = train_model_CoGNN(data=data, config=config, num_folds=config[\"num_folds\"])\n",
    "    print(\"Training has concluded.\")\n",
    "\n",
    "    return metrics_list, node_states_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we instantiate the `CONFIG` with all the desired parameters. When `run_grid_search` is enabled in the `main_CoGNN` call, then the parameters that the model will train on might be different but it will be logged in the print statements. Moreover, the grid search results will be stored in a `.csv` file.\n",
    "\n",
    "For visualization purposes, the model metrics and node state distribution will be stored in `.csv` files as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the grid search:\n",
    "\n",
    "In the paper, the authors find that, depending on the dataset, the Co-GNN(, ), Co-GNN(, ), Co-GNN(, ), Co-GNN(, ) and Co-GNN(, ) architectures work well. In order to enhance comparability with the other models in this project, we will only use the Co-GNN(, ) architecture, i.e. GCNs.\n",
    "[SUM-GNN: , MEAN-GNN: , GCN: , GIN: , GAT: ]\n",
    "\n",
    "Unfortunately we will not be able to do a great deal of parameter grid search as even with a handful of parameters we would end up with too many combinations (n! number of parameter tuples) which, depending on the dataset, results in compute times of several days therefore going beyond the scope of this project. The idea is that the optimal parameters resulting from the grid search on one dataset, specifically `Cora-ML` will then be used for all other datasets, too.\n",
    "\n",
    "Hence, for the grid search we will optimize the learning rate `lr` and the number of message passing layers `env_num_layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Cora-ML.npz dataset...\n",
      "Running grid search...\n",
      "\n",
      "Training combination 1/10:\n",
      " {'lr': 0.01, 'env_num_layers': 1}\n",
      "Training stopped after 291 epochs following no improvement after 100 epochs.:  29%|       | 292/1000 [00:18<00:44, 15.87it/s]       \n",
      "Fold 1/1 Results:\n",
      "Training Accuracy: 1.0, Training Loss: 0.01\n",
      "Validation Accuracy: 0.72, Validation Loss: 1.07\n",
      "Test Accuracy: 0.71, Test Loss: 1.12\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Final Results:\n",
      "Average Training Accuracy=1.0+-0.0, Average Validation Accuracy=0.72+-0.0, Average Test Accuracy=0.71+-0.0\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Training combination 2/10:\n",
      " {'lr': 0.01, 'env_num_layers': 2}\n",
      "Training stopped after 441 epochs following no improvement after 100 epochs.:  44%|     | 442/1000 [00:40<00:50, 10.98it/s]       \n",
      "Fold 1/1 Results:\n",
      "Training Accuracy: 0.99, Training Loss: 0.04\n",
      "Validation Accuracy: 0.79, Validation Loss: 1.27\n",
      "Test Accuracy: 0.77, Test Loss: 1.69\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Final Results:\n",
      "Average Training Accuracy=0.99+-0.0, Average Validation Accuracy=0.79+-0.0, Average Test Accuracy=0.77+-0.0\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Training combination 3/10:\n",
      " {'lr': 0.01, 'env_num_layers': 3}\n",
      "Training stopped after 676 epochs following no improvement after 100 epochs.:  68%|   | 677/1000 [01:25<00:41,  7.88it/s]       \n",
      "Fold 1/1 Results:\n",
      "Training Accuracy: 0.98, Training Loss: 0.08\n",
      "Validation Accuracy: 0.7, Validation Loss: 1.37\n",
      "Test Accuracy: 0.69, Test Loss: 2.33\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Final Results:\n",
      "Average Training Accuracy=0.98+-0.0, Average Validation Accuracy=0.7+-0.0, Average Test Accuracy=0.69+-0.0\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Training combination 4/10:\n",
      " {'lr': 0.01, 'env_num_layers': 4}\n",
      "Training stopped after 377 epochs following no improvement after 100 epochs.:  38%|      | 378/1000 [01:02<01:42,  6.05it/s]       \n",
      "Fold 1/1 Results:\n",
      "Training Accuracy: 0.61, Training Loss: 1.03\n",
      "Validation Accuracy: 0.46, Validation Loss: 1.59\n",
      "Test Accuracy: 0.43, Test Loss: 1.63\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Final Results:\n",
      "Average Training Accuracy=0.61+-0.0, Average Validation Accuracy=0.46+-0.0, Average Test Accuracy=0.43+-0.0\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Training combination 5/10:\n",
      " {'lr': 0.01, 'env_num_layers': 5}\n",
      "Training stopped after 203 epochs following no improvement after 100 epochs.:  20%|        | 204/1000 [00:39<02:32,  5.23it/s]       \n",
      "Fold 1/1 Results:\n",
      "Training Accuracy: 0.24, Training Loss: 1.82\n",
      "Validation Accuracy: 0.34, Validation Loss: 1.86\n",
      "Test Accuracy: 0.29, Test Loss: 1.84\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Final Results:\n",
      "Average Training Accuracy=0.24+-0.0, Average Validation Accuracy=0.34+-0.0, Average Test Accuracy=0.29+-0.0\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Training combination 6/10:\n",
      " {'lr': 0.001, 'env_num_layers': 1}\n",
      "Training stopped after 342 epochs following no improvement after 100 epochs.:  34%|      | 343/1000 [00:21<00:41, 15.85it/s]       \n",
      "Fold 1/1 Results:\n",
      "Training Accuracy: 0.88, Training Loss: 1.08\n",
      "Validation Accuracy: 0.61, Validation Loss: 1.56\n",
      "Test Accuracy: 0.58, Test Loss: 1.56\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Final Results:\n",
      "Average Training Accuracy=0.88+-0.0, Average Validation Accuracy=0.61+-0.0, Average Test Accuracy=0.58+-0.0\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Training combination 7/10:\n",
      " {'lr': 0.001, 'env_num_layers': 2}\n",
      "Training stopped after 100 epochs following no improvement after 100 epochs.:  10%|         | 101/1000 [00:09<01:22, 10.96it/s]       \n",
      "Fold 1/1 Results:\n",
      "Training Accuracy: 0.14, Training Loss: 1.95\n",
      "Validation Accuracy: 0.16, Validation Loss: 1.93\n",
      "Test Accuracy: 0.15, Test Loss: 1.94\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Final Results:\n",
      "Average Training Accuracy=0.14+-0.0, Average Validation Accuracy=0.16+-0.0, Average Test Accuracy=0.15+-0.0\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Training combination 8/10:\n",
      " {'lr': 0.001, 'env_num_layers': 3}\n",
      "Training stopped after 612 epochs following no improvement after 100 epochs.:  61%|   | 613/1000 [01:26<00:54,  7.09it/s]       \n",
      "Fold 1/1 Results:\n",
      "Training Accuracy: 0.57, Training Loss: 0.97\n",
      "Validation Accuracy: 0.5, Validation Loss: 1.56\n",
      "Test Accuracy: 0.43, Test Loss: 1.64\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Final Results:\n",
      "Average Training Accuracy=0.57+-0.0, Average Validation Accuracy=0.5+-0.0, Average Test Accuracy=0.43+-0.0\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Training combination 9/10:\n",
      " {'lr': 0.001, 'env_num_layers': 4}\n",
      "Training stopped after 351 epochs following no improvement after 100 epochs.:  35%|      | 352/1000 [01:07<02:04,  5.22it/s]       \n",
      "Fold 1/1 Results:\n",
      "Training Accuracy: 0.35, Training Loss: 1.63\n",
      "Validation Accuracy: 0.41, Validation Loss: 1.77\n",
      "Test Accuracy: 0.33, Test Loss: 1.76\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Final Results:\n",
      "Average Training Accuracy=0.35+-0.0, Average Validation Accuracy=0.41+-0.0, Average Test Accuracy=0.33+-0.0\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Training combination 10/10:\n",
      " {'lr': 0.001, 'env_num_layers': 5}\n",
      "Training stopped after 297 epochs following no improvement after 100 epochs.:  30%|       | 298/1000 [01:09<02:43,  4.30it/s]       \n",
      "Fold 1/1 Results:\n",
      "Training Accuracy: 0.21, Training Loss: 1.9\n",
      "Validation Accuracy: 0.33, Validation Loss: 1.9\n",
      "Test Accuracy: 0.26, Test Loss: 1.91\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Final Results:\n",
      "Average Training Accuracy=0.21+-0.0, Average Validation Accuracy=0.33+-0.0, Average Test Accuracy=0.26+-0.0\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Grid Search concluded. Best parameters: lr=0.01, env_num_layers=2, Accuracy=0.7695\n",
      "\n",
      "Training final model...\n",
      "Training stopped after 441 epochs following no improvement after 100 epochs.:  44%|     | 442/1000 [00:53<01:07,  8.24it/s]       \n",
      "Fold 1/3 Results:\n",
      "Training Accuracy: 0.99, Training Loss: 0.04\n",
      "Validation Accuracy: 0.79, Validation Loss: 1.27\n",
      "Test Accuracy: 0.77, Test Loss: 1.69\n",
      "\n",
      "Training stopped after 307 epochs following no improvement after 100 epochs.:  31%|       | 308/1000 [00:35<01:20,  8.57it/s]       \n",
      "Fold 2/3 Results:\n",
      "Training Accuracy: 0.99, Training Loss: 0.05\n",
      "Validation Accuracy: 0.74, Validation Loss: 1.37\n",
      "Test Accuracy: 0.73, Test Loss: 1.5\n",
      "\n",
      "Training stopped after 521 epochs following no improvement after 100 epochs.:  52%|    | 522/1000 [00:58<00:53,  8.93it/s]       \n",
      "Fold 3/3 Results:\n",
      "Training Accuracy: 0.99, Training Loss: 0.02\n",
      "Validation Accuracy: 0.74, Validation Loss: 1.32\n",
      "Test Accuracy: 0.73, Test Loss: 1.54\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Final Results:\n",
      "Average Training Accuracy=0.99+-0.0, Average Validation Accuracy=0.76+-0.03, Average Test Accuracy=0.74+-0.02\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training has concluded.\n",
      "Metrics and State Distribution saved successfully.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # For comparability reasons, run the grid search on the \"Cora-ML.npz\" dataset.\n",
    "    CONFIG = create_config(dataset=\"Cora-ML.npz\",\n",
    "                           seed=42,\n",
    "                           num_folds=3,\n",
    "                           learn_temp=False,\n",
    "                           temp_model_type=ModelType.GCN,\n",
    "                           act_model_type=ModelType.GCN,\n",
    "                           env_model_type=ModelType.GCN,\n",
    "                           env_dim=32,\n",
    "                           dropout=0.5)\n",
    "    \n",
    "    metrics_list, node_states_list = main_CoGNN(config=CONFIG, run_grid_search=True)\n",
    "\n",
    "    # Accuracy Scores\n",
    "    df_metrics = pandas.DataFrame(data=[tensor.tolist() for tensor in metrics_list],\n",
    "                                  columns=[\"Last Epoch Training Accuracy\",\n",
    "                                           \"Last Epoch Validation Accuracy\",\n",
    "                                           \"Final Test Accuracy\"])\n",
    "    df_metrics.index.name = \"Fold ID\"\n",
    "    df_metrics.to_csv(f\"metrics_{CONFIG[\"dataset\"]}.csv\", index=True)\n",
    "\n",
    "    # State Distribution\n",
    "    state_rows = []\n",
    "    for fold_id, fold in enumerate(node_states_list):\n",
    "        for layer_id, layer in enumerate(fold):\n",
    "            state_rows.append((fold_id, layer_id+1, *layer))\n",
    "    df_states = pandas.DataFrame(data=state_rows,\n",
    "                                 columns=[\"Fold ID\",\n",
    "                                          \"Layer ID\",\n",
    "                                          \"Standard\",\n",
    "                                          \"Broadcast\",\n",
    "                                          \"Listen\",\n",
    "                                          \"Isolate\"])\n",
    "    df_states.set_index([\"Fold ID\", \"Layer ID\"], inplace=True)\n",
    "    df_states.to_csv(f\"states_{CONFIG[\"dataset\"]}.csv\", index=True)\n",
    "\n",
    "    print(\"Metrics and State Distribution saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AdvancedTopicsInMachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
