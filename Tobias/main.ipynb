{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cooperative Graph Neural Networks ([CoGNN](https://doi.org/10.48550/arXiv.2310.01267))\n",
    "\n",
    "This part was adapted by Tobias Erbacher from the [authors' github](https://github.com/benfinkelshtein/CoGNN/tree/main). We recommend to run this on a GPU service like [Google Colab](https://colab.research.google.com/). The goal is to modify the original authors' code to work with our codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "To ensure we are using the same modules as the authors, we need to install the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch==2.0.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#%pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
    "#%pip install torch-geometric==2.3.0\n",
    "#%pip install torchmetrics ogb rdkit\n",
    "#%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute this notebook, we will assume that the datasets are already installed. If you cloned our github repository, you will be able to find them in [this folder](https://github.com/TobiasErbacher/gdl/tree/main/replication/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this notebook less crowded, some class definitions have been moved to other files. The overview here is to get an idea of the library dependencies. The file `architectures.py` will perform the following imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from `torch` import `Tensor, cat`\n",
    "- from `torch.nn` import `Linear, Parameter`\n",
    "- from `typing` import `Callable, List`\n",
    "- from `torch_geometric.typing` import `OptTensor, Adj`\n",
    "- from `torch_geometric.nn.conv` import `MessagePassing`\n",
    "- from `torch_geometric.nn.conv.gcn_conv` import `gcn_norm`\n",
    "- from `torch_geometric.utils` import `remove_self_loops, add_remaining_self_loops`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the type classes will be bundled in the file `type_classes.py` and perform the following imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from `torch` import `from_numpy, tensor`\n",
    "- from `torch.nn` import `Module, ReLU, GELU, CrossEntropyLoss`\n",
    "- import `torch.nn.functional` as `F`\n",
    "- from `torchmetrics` import `Accuracy`\n",
    "- from `torch_geometric.data` import `Data`\n",
    "- from `torch_geometric.nn.pool` import `global_mean_pool, global_add_pool`\n",
    "- import `numpy` as `np`\n",
    "- from `math` import `inf`\n",
    "- from `enum` import `Enum, auto`\n",
    "- from `typing` import `Callable, List, NamedTuple`\n",
    "- from `architectures` import `WeightedGCNConv, WeightedGINConv, WeightedGNNConv, GraphLinear, BatchIdentity`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the import from `architectures` is from the file `architectures.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, the encoders will be bundled in the file `encoder_classes.py` and perform the following imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from `enum` import `Enum, auto`\n",
    "- from `torch` import `cat, rand, isnan, sum, Tensor`\n",
    "- from `torch.nn` import `Module, ModuleList, Linear, Sequential, BatchNorm1d, ReLU, TransformerEncoder, TransformerEncoderLayer, Embedding`\n",
    "- from `torch.nn.init` import `xavier_uniform_`\n",
    "- from `ogb.utils.features` import `get_atom_feature_dims, get_bond_feature_dims`\n",
    "- from `torch_geometric.data` import `Data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Custom Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from type_classes import ModelType, LossesAndMetrics, Pool, ActivationType, Metric\n",
    "from encoder_classes import PosEncoder, DataSetEncoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Official Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.typing import OptTensor, Adj\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Union, List, Tuple, NamedTuple, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first check whether a GPU is available and if so then set it as the default device:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define the root directory `ROOT_DIR`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # If running as a .py\n",
    "    ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except:\n",
    "    # If running as a .ipynb\n",
    "    from os import getcwd\n",
    "    ROOT_DIR = os.path.dirname(os.path.abspath(getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure reproducibility, we define the `set_seed()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The device to be used for training (GPU, ...) will be set automatically in the `Experiment` class (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are copied from the replication/data_loading/ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spla\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "import warnings\n",
    "\n",
    "data_dir = os.path.join(Path(getcwd()).parent, \"replication/data\")\n",
    "development_seed = 4143496719\n",
    "\n",
    "sparse_graph_properties = [\n",
    "        'adj_matrix', 'attr_matrix', 'labels',\n",
    "        'node_names', 'attr_names', 'class_names',\n",
    "        'metadata']\n",
    "\n",
    "class SparseGraph:\n",
    "    \"\"\"Attributed labeled graph stored in sparse matrix form.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adj_matrix\n",
    "        Adjacency matrix in CSR format. Shape [num_nodes, num_nodes]\n",
    "    attr_matrix\n",
    "        Attribute matrix in CSR or numpy format. Shape [num_nodes, num_attr]\n",
    "    labels\n",
    "        Array, where each entry represents respective node's label(s). Shape [num_nodes]\n",
    "        Alternatively, CSR matrix with labels in one-hot format. Shape [num_nodes, num_classes]\n",
    "    node_names\n",
    "        Names of nodes (as strings). Shape [num_nodes]\n",
    "    attr_names\n",
    "        Names of the attributes (as strings). Shape [num_attr]\n",
    "    class_names\n",
    "        Names of the class labels (as strings). Shape [num_classes]\n",
    "    metadata\n",
    "        Additional metadata such as text.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, adj_matrix: sp.spmatrix,\n",
    "            attr_matrix: Union[np.ndarray, sp.spmatrix] = None,\n",
    "            labels: Union[np.ndarray, sp.spmatrix] = None,\n",
    "            node_names: np.ndarray = None,\n",
    "            attr_names: np.ndarray = None,\n",
    "            class_names: np.ndarray = None,\n",
    "            metadata: Any = None):\n",
    "        # Make sure that the dimensions of matrices / arrays all agree\n",
    "        if sp.isspmatrix(adj_matrix):\n",
    "            adj_matrix = adj_matrix.tocsr().astype(np.float32)\n",
    "        else:\n",
    "            raise ValueError(\"Adjacency matrix must be in sparse format (got {0} instead).\"\n",
    "                             .format(type(adj_matrix)))\n",
    "\n",
    "        if adj_matrix.shape[0] != adj_matrix.shape[1]:\n",
    "            raise ValueError(\"Dimensions of the adjacency matrix don't agree.\")\n",
    "\n",
    "        if attr_matrix is not None:\n",
    "            if sp.isspmatrix(attr_matrix):\n",
    "                attr_matrix = attr_matrix.tocsr().astype(np.float32)\n",
    "            elif isinstance(attr_matrix, np.ndarray):\n",
    "                attr_matrix = attr_matrix.astype(np.float32)\n",
    "            else:\n",
    "                raise ValueError(\"Attribute matrix must be a sp.spmatrix or a np.ndarray (got {0} instead).\"\n",
    "                                 .format(type(attr_matrix)))\n",
    "\n",
    "            if attr_matrix.shape[0] != adj_matrix.shape[0]:\n",
    "                raise ValueError(\"Dimensions of the adjacency and attribute matrices don't agree.\")\n",
    "\n",
    "        if labels is not None:\n",
    "            if labels.shape[0] != adj_matrix.shape[0]:\n",
    "                raise ValueError(\"Dimensions of the adjacency matrix and the label vector don't agree.\")\n",
    "\n",
    "        if node_names is not None:\n",
    "            if len(node_names) != adj_matrix.shape[0]:\n",
    "                raise ValueError(\"Dimensions of the adjacency matrix and the node names don't agree.\")\n",
    "\n",
    "        if attr_names is not None:\n",
    "            if len(attr_names) != attr_matrix.shape[1]:\n",
    "                raise ValueError(\"Dimensions of the attribute matrix and the attribute names don't agree.\")\n",
    "\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.attr_matrix = attr_matrix\n",
    "        self.labels = labels\n",
    "        self.node_names = node_names\n",
    "        self.attr_names = attr_names\n",
    "        self.class_names = class_names\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def num_nodes(self) -> int:\n",
    "        \"\"\"Get the number of nodes in the graph.\n",
    "        \"\"\"\n",
    "        return self.adj_matrix.shape[0]\n",
    "\n",
    "    def num_edges(self) -> int:\n",
    "        \"\"\"Get the number of edges in the graph.\n",
    "\n",
    "        For undirected graphs, (i, j) and (j, i) are counted as _two_ edges.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.adj_matrix.nnz\n",
    "\n",
    "    def get_neighbors(self, idx: int) -> np.ndarray:\n",
    "        \"\"\"Get the indices of neighbors of a given node.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx\n",
    "            Index of the node whose neighbors are of interest.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.adj_matrix[idx].indices\n",
    "\n",
    "    def get_edgeid_to_idx_array(self) -> np.ndarray:\n",
    "        \"\"\"Return a Numpy Array that maps edgeids to the indices in the adjacency matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The i'th entry contains the x- and y-coordinates of edge i in the adjacency matrix.\n",
    "            Shape [num_edges, 2]\n",
    "\n",
    "        \"\"\"\n",
    "        return np.transpose(self.adj_matrix.nonzero())\n",
    "\n",
    "    def is_directed(self) -> bool:\n",
    "        \"\"\"Check if the graph is directed (adjacency matrix is not symmetric).\n",
    "        \"\"\"\n",
    "        return (self.adj_matrix != self.adj_matrix.T).sum() != 0\n",
    "\n",
    "    def to_undirected(self) -> 'SparseGraph':\n",
    "        \"\"\"Convert to an undirected graph (make adjacency matrix symmetric).\n",
    "        \"\"\"\n",
    "        idx = self.get_edgeid_to_idx_array().T\n",
    "        ridx = np.ravel_multi_index(idx, self.adj_matrix.shape)\n",
    "        ridx_rev = np.ravel_multi_index(idx[::-1], self.adj_matrix.shape)\n",
    "\n",
    "        # Get duplicate edges (self-loops and opposing edges)\n",
    "        dup_ridx = ridx[np.isin(ridx, ridx_rev)]\n",
    "        dup_idx = np.unravel_index(dup_ridx, self.adj_matrix.shape)\n",
    "\n",
    "        # Check if the adjacency matrix weights are symmetric (if nonzero)\n",
    "        if len(dup_ridx) > 0 and not np.allclose(self.adj_matrix[dup_idx], self.adj_matrix[dup_idx[::-1]]):\n",
    "            raise ValueError(\"Adjacency matrix weights of opposing edges differ.\")\n",
    "\n",
    "        # Create symmetric matrix\n",
    "        new_adj_matrix = self.adj_matrix + self.adj_matrix.T\n",
    "        if len(dup_ridx) > 0:\n",
    "            new_adj_matrix[dup_idx] = (new_adj_matrix[dup_idx] - self.adj_matrix[dup_idx]).A1\n",
    "\n",
    "        self.adj_matrix = new_adj_matrix\n",
    "        return self\n",
    "\n",
    "    def is_weighted(self) -> bool:\n",
    "        \"\"\"Check if the graph is weighted (edge weights other than 1).\n",
    "        \"\"\"\n",
    "        return np.any(np.unique(self.adj_matrix[self.adj_matrix.nonzero()].A1) != 1)\n",
    "\n",
    "    def to_unweighted(self) -> 'SparseGraph':\n",
    "        \"\"\"Convert to an unweighted graph (set all edge weights to 1).\n",
    "        \"\"\"\n",
    "        self.adj_matrix.data = np.ones_like(self.adj_matrix.data)\n",
    "        return self\n",
    "\n",
    "    def is_connected(self) -> bool:\n",
    "        \"\"\"Check if the graph is connected.\n",
    "        \"\"\"\n",
    "        return sp.csgraph.connected_components(self.adj_matrix, return_labels=False) == 1\n",
    "\n",
    "    def has_self_loops(self) -> bool:\n",
    "        \"\"\"Check if the graph has self-loops.\n",
    "        \"\"\"\n",
    "        return not np.allclose(self.adj_matrix.diagonal(), 0)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        props = []\n",
    "        for prop_name in sparse_graph_properties:\n",
    "            prop = getattr(self, prop_name)\n",
    "            if prop is not None:\n",
    "                if prop_name == 'metadata':\n",
    "                    props.append(prop_name)\n",
    "                else:\n",
    "                    shape_string = 'x'.join([str(x) for x in prop.shape])\n",
    "                    props.append(\"{} ({})\".format(prop_name, shape_string))\n",
    "        dir_string = 'Directed' if self.is_directed() else 'Undirected'\n",
    "        weight_string = 'weighted' if self.is_weighted() else 'unweighted'\n",
    "        conn_string = 'connected' if self.is_connected() else 'disconnected'\n",
    "        loop_string = 'has self-loops' if self.has_self_loops() else 'no self-loops'\n",
    "        return (\"<{}, {} and {} SparseGraph with {} edges ({}). Data: {}>\"\n",
    "                .format(dir_string, weight_string, conn_string,\n",
    "                        self.num_edges(), loop_string,\n",
    "                        ', '.join(props)))\n",
    "\n",
    "    # Quality of life (shortcuts)\n",
    "    def standardize(\n",
    "            self, make_unweighted: bool = True,\n",
    "            make_undirected: bool = True,\n",
    "            no_self_loops: bool = True,\n",
    "            select_lcc: bool = True\n",
    "            ) -> 'SparseGraph':\n",
    "        \"\"\"Perform common preprocessing steps: remove self-loops, make unweighted/undirected, select LCC.\n",
    "\n",
    "        All changes are done inplace.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        make_unweighted\n",
    "            Whether to set all edge weights to 1.\n",
    "        make_undirected\n",
    "            Whether to make the adjacency matrix symmetric. Can only be used if make_unweighted is True.\n",
    "        no_self_loops\n",
    "            Whether to remove self loops.\n",
    "        select_lcc\n",
    "            Whether to select the largest connected component of the graph.\n",
    "\n",
    "        \"\"\"\n",
    "        G = self\n",
    "        if make_unweighted and G.is_weighted():\n",
    "            G = G.to_unweighted()\n",
    "        if make_undirected and G.is_directed():\n",
    "            G = G.to_undirected()\n",
    "        if no_self_loops and G.has_self_loops():\n",
    "            G = remove_self_loops(G)\n",
    "        if select_lcc and not G.is_connected():\n",
    "            G = largest_connected_components(G, 1)\n",
    "        return G\n",
    "\n",
    "    def unpack(self) -> Tuple[sp.csr_matrix,\n",
    "                              Union[np.ndarray, sp.csr_matrix],\n",
    "                              Union[np.ndarray, sp.csr_matrix]]:\n",
    "        \"\"\"Return the (A, X, E, z) quadruplet.\n",
    "        \"\"\"\n",
    "        return self.adj_matrix, self.attr_matrix, self.labels\n",
    "\n",
    "    def to_flat_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return flat dictionary containing all SparseGraph properties.\n",
    "        \"\"\"\n",
    "        data_dict = {}\n",
    "        for key in sparse_graph_properties:\n",
    "            val = getattr(self, key)\n",
    "            if sp.isspmatrix(val):\n",
    "                data_dict['{}.data'.format(key)] = val.data\n",
    "                data_dict['{}.indices'.format(key)] = val.indices\n",
    "                data_dict['{}.indptr'.format(key)] = val.indptr\n",
    "                data_dict['{}.shape'.format(key)] = val.shape\n",
    "            else:\n",
    "                data_dict[key] = val\n",
    "        return data_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def from_flat_dict(data_dict: Dict[str, Any]) -> 'SparseGraph':\n",
    "        \"\"\"Initialize SparseGraph from a flat dictionary.\n",
    "        \"\"\"\n",
    "        init_dict = {}\n",
    "        del_entries = []\n",
    "\n",
    "        # Construct sparse matrices\n",
    "        for key in data_dict.keys():\n",
    "            if key.endswith('_data') or key.endswith('.data'):\n",
    "                if key.endswith('_data'):\n",
    "                    sep = '_'\n",
    "                    warnings.warn(\n",
    "                            \"The separator used for sparse matrices during export (for .npz files) \"\n",
    "                            \"is now '.' instead of '_'. Please update (re-save) your stored graphs.\",\n",
    "                            DeprecationWarning, stacklevel=2)\n",
    "                else:\n",
    "                    sep = '.'\n",
    "                matrix_name = key[:-5]\n",
    "                mat_data = key\n",
    "                mat_indices = '{}{}indices'.format(matrix_name, sep)\n",
    "                mat_indptr = '{}{}indptr'.format(matrix_name, sep)\n",
    "                mat_shape = '{}{}shape'.format(matrix_name, sep)\n",
    "                if matrix_name == 'adj' or matrix_name == 'attr':\n",
    "                    warnings.warn(\n",
    "                            \"Matrices are exported (for .npz files) with full names now. \"\n",
    "                            \"Please update (re-save) your stored graphs.\",\n",
    "                            DeprecationWarning, stacklevel=2)\n",
    "                    matrix_name += '_matrix'\n",
    "                init_dict[matrix_name] = sp.csr_matrix(\n",
    "                        (data_dict[mat_data],\n",
    "                         data_dict[mat_indices],\n",
    "                         data_dict[mat_indptr]),\n",
    "                        shape=data_dict[mat_shape])\n",
    "                del_entries.extend([mat_data, mat_indices, mat_indptr, mat_shape])\n",
    "\n",
    "        # Delete sparse matrix entries\n",
    "        for del_entry in del_entries:\n",
    "            del data_dict[del_entry]\n",
    "\n",
    "        # Load everything else\n",
    "        for key, val in data_dict.items():\n",
    "            if ((val is not None) and (None not in val)):\n",
    "                init_dict[key] = val\n",
    "\n",
    "        # Check if the dictionary contains only entries in sparse_graph_properties\n",
    "        unknown_keys = [key for key in init_dict.keys() if key not in sparse_graph_properties]\n",
    "        if len(unknown_keys) > 0:\n",
    "            raise ValueError(\"Input dictionary contains keys that are not SparseGraph properties ({}).\"\n",
    "                             .format(unknown_keys))\n",
    "\n",
    "        return SparseGraph(**init_dict)\n",
    "\n",
    "def create_subgraph(\n",
    "        sparse_graph: SparseGraph,\n",
    "        _sentinel: None = None,\n",
    "        nodes_to_remove: np.ndarray = None,\n",
    "        nodes_to_keep: np.ndarray = None\n",
    "        ) -> SparseGraph:\n",
    "    \"\"\"Create a graph with the specified subset of nodes.\n",
    "\n",
    "    Exactly one of (nodes_to_remove, nodes_to_keep) should be provided, while the other stays None.\n",
    "    Note that to avoid confusion, it is required to pass node indices as named arguments to this function.\n",
    "\n",
    "    The subgraph partially points to the old graph's data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sparse_graph\n",
    "        Input graph.\n",
    "    _sentinel\n",
    "        Internal, to prevent passing positional arguments. Do not use.\n",
    "    nodes_to_remove\n",
    "        Indices of nodes that have to removed.\n",
    "    nodes_to_keep\n",
    "        Indices of nodes that have to be kept.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    SparseGraph\n",
    "        Graph with specified nodes removed.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check that arguments are passed correctly\n",
    "    if _sentinel is not None:\n",
    "        raise ValueError(\"Only call `create_subgraph` with named arguments',\"\n",
    "                         \" (nodes_to_remove=...) or (nodes_to_keep=...).\")\n",
    "    if nodes_to_remove is None and nodes_to_keep is None:\n",
    "        raise ValueError(\"Either nodes_to_remove or nodes_to_keep must be provided.\")\n",
    "    elif nodes_to_remove is not None and nodes_to_keep is not None:\n",
    "        raise ValueError(\"Only one of nodes_to_remove or nodes_to_keep must be provided.\")\n",
    "    elif nodes_to_remove is not None:\n",
    "        nodes_to_keep = [i for i in range(sparse_graph.num_nodes()) if i not in nodes_to_remove]\n",
    "    elif nodes_to_keep is not None:\n",
    "        nodes_to_keep = sorted(nodes_to_keep)\n",
    "    else:\n",
    "        raise RuntimeError(\"This should never happen.\")\n",
    "\n",
    "    sparse_graph.adj_matrix = sparse_graph.adj_matrix[nodes_to_keep][:, nodes_to_keep]\n",
    "    if sparse_graph.attr_matrix is not None:\n",
    "        sparse_graph.attr_matrix = sparse_graph.attr_matrix[nodes_to_keep]\n",
    "    if sparse_graph.labels is not None:\n",
    "        sparse_graph.labels = sparse_graph.labels[nodes_to_keep]\n",
    "    if sparse_graph.node_names is not None:\n",
    "        sparse_graph.node_names = sparse_graph.node_names[nodes_to_keep]\n",
    "    return sparse_graph\n",
    "\n",
    "\n",
    "def largest_connected_components(sparse_graph: SparseGraph, n_components: int = 1) -> SparseGraph:\n",
    "    \"\"\"Select the largest connected components in the graph.\n",
    "\n",
    "    Changes are returned in a partially new SparseGraph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sparse_graph\n",
    "        Input graph.\n",
    "    n_components\n",
    "        Number of largest connected components to keep.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    SparseGraph\n",
    "        Subgraph of the input graph where only the nodes in largest n_components are kept.\n",
    "\n",
    "    \"\"\"\n",
    "    _, component_indices = sp.csgraph.connected_components(sparse_graph.adj_matrix)\n",
    "    component_sizes = np.bincount(component_indices)\n",
    "    components_to_keep = np.argsort(component_sizes)[::-1][:n_components]  # reverse order to sort descending\n",
    "    nodes_to_keep = [\n",
    "        idx for (idx, component) in enumerate(component_indices) if component in components_to_keep\n",
    "    ]\n",
    "    return create_subgraph(sparse_graph, nodes_to_keep=nodes_to_keep)\n",
    "\n",
    "\n",
    "def remove_self_loops(sparse_graph: SparseGraph) -> SparseGraph:\n",
    "    \"\"\"Remove self loops (diagonal entries in the adjacency matrix).\n",
    "\n",
    "    Changes are returned in a partially new SparseGraph.\n",
    "\n",
    "    \"\"\"\n",
    "    num_self_loops = (~np.isclose(sparse_graph.adj_matrix.diagonal(), 0)).sum()\n",
    "    if num_self_loops > 0:\n",
    "        sparse_graph.adj_matrix = sparse_graph.adj_matrix.tolil()\n",
    "        sparse_graph.adj_matrix.setdiag(0)\n",
    "        sparse_graph.adj_matrix = sparse_graph.adj_matrix.tocsr()\n",
    "        warnings.warn(\"{0} self loops removed\".format(num_self_loops))\n",
    "\n",
    "    return sparse_graph\n",
    "\n",
    "def load_from_npz(file_name: str) -> SparseGraph:\n",
    "    \"\"\"Load a SparseGraph from a Numpy binary file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name\n",
    "        Name of the file to load.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    SparseGraph\n",
    "        Graph in sparse matrix format.\n",
    "\n",
    "    \"\"\"\n",
    "    with np.load(file_name, allow_pickle=True) as loader:\n",
    "        loader = dict(loader)\n",
    "        dataset = SparseGraph.from_flat_dict(loader)\n",
    "    return dataset\n",
    "\n",
    "def load_dataset(name: str,\n",
    "                 directory: Union[Path, str] = data_dir\n",
    "                 ) -> SparseGraph:\n",
    "    \"\"\"Load a dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name\n",
    "        Name of the dataset to load.\n",
    "    directory\n",
    "        Path to the directory where the datasets are stored.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    SparseGraph\n",
    "        The requested dataset in sparse format.\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(directory, str):\n",
    "        directory = Path(directory)\n",
    "    if not name.endswith('.npz'):\n",
    "        name += '.npz'\n",
    "    path_to_file = directory / name\n",
    "    if path_to_file.exists():\n",
    "        return load_from_npz(path_to_file)\n",
    "    else:\n",
    "        raise ValueError(\"{} doesn't exist.\".format(path_to_file))\n",
    "\n",
    "def normalize_attributes(attr_matrix):\n",
    "    epsilon = 1e-12\n",
    "    if isinstance(attr_matrix, sp.csr_matrix):\n",
    "        attr_norms = spla.norm(attr_matrix, ord=1, axis=1)\n",
    "        attr_invnorms = 1 / np.maximum(attr_norms, epsilon)\n",
    "        attr_mat_norm = attr_matrix.multiply(attr_invnorms[:, np.newaxis])\n",
    "    else:\n",
    "        attr_norms = np.linalg.norm(attr_matrix, ord=1, axis=1)\n",
    "        attr_invnorms = 1 / np.maximum(attr_norms, epsilon)\n",
    "        attr_mat_norm = attr_matrix * attr_invnorms[:, np.newaxis]\n",
    "    return attr_mat_norm\n",
    "\n",
    "def get_dataset(name: str, use_lcc: bool = True) -> InMemoryDataset:\n",
    "    \"\"\"\n",
    "    :param name: The name of the dataset\n",
    "    :param use_lcc: Largest Connected Component\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dataset = InMemoryDataset\n",
    "    graph = load_dataset(name)\n",
    "    graph.standardize(select_lcc=use_lcc)  # This was changed from =True to use_lcc by JK\n",
    "    new_y = torch.LongTensor(graph.labels)\n",
    "    data = Data(\n",
    "        x=torch.FloatTensor(normalize_attributes(graph.attr_matrix).toarray()),\n",
    "        edge_index=torch.LongTensor(graph.get_edgeid_to_idx_array().T),\n",
    "        y=new_y,\n",
    "        train_mask=torch.zeros(new_y.size(0), dtype=torch.bool),\n",
    "        test_mask=torch.zeros(new_y.size(0), dtype=torch.bool),\n",
    "        val_mask=torch.zeros(new_y.size(0), dtype=torch.bool)\n",
    "    )\n",
    "    dataset.data = data\n",
    "    dataset.name = name\n",
    "    dataset.num_classes = len(np.unique(new_y))\n",
    "    return dataset\n",
    "\n",
    "def set_train_val_test_split(\n",
    "        seed: int,\n",
    "        data: Data,\n",
    "        num_development: int = 1500,\n",
    "        num_per_class: int = 20) -> Data:\n",
    "    rnd_state = np.random.RandomState(development_seed)\n",
    "    num_nodes = data.y.shape[0]\n",
    "    development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
    "    test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
    "\n",
    "    train_idx = []\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    for c in range(data.y.max() + 1):\n",
    "        class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
    "        train_idx.extend(rnd_state.choice(class_idx, num_per_class, replace=False))\n",
    "\n",
    "    val_idx_tmp = [i for i in development_idx if i not in train_idx]\n",
    "\n",
    "    val_idx = rnd_state.choice(val_idx_tmp, 500, replace=False)\n",
    "\n",
    "    def get_mask(idx):\n",
    "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        mask[idx] = 1\n",
    "        return mask\n",
    "\n",
    "    data.train_mask = get_mask(train_idx)\n",
    "    data.val_mask = get_mask(val_idx)\n",
    "    data.test_mask = get_mask(test_idx)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next one was custom written and initially used to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_custom(\n",
    "    dataset_name : str\n",
    ") -> Data:\n",
    "    \"\"\"\n",
    "        Loads the datasets from .npz files into a PyTorch_Geometric Data-object where\n",
    "        -   x:              Feature matrix,\n",
    "        -   y:              Labels,\n",
    "        -   edge_index:     List of edges,\n",
    "        -   edge_weight:    List of edge weights.\n",
    "\n",
    "        The function is currently implemented for the following datasets:\n",
    "        -   A.Computer\n",
    "        -   A.Photo\n",
    "        -   Citeseer\n",
    "        -   Cora-ML\n",
    "        -   MS-Academic\n",
    "        -   PubMed\n",
    "    \"\"\"\n",
    "    path = os.path.join(ROOT_DIR, \"replication/data/\" + dataset_name + \".npz\")\n",
    "    npz = np.load(path)\n",
    "    assert isinstance(npz, np.lib.npyio.NpzFile)\n",
    "\n",
    "    variant1 = [(key in [\"adj_data\",\n",
    "                         \"adj_indices\",\n",
    "                         \"adj_indptr\",\n",
    "                         \"adj_shape\",\n",
    "                         \"attr_data\",\n",
    "                         \"attr_indices\",\n",
    "                         \"attr_indptr\",\n",
    "                         \"attr_shape\",\n",
    "                         \"labels\",\n",
    "                         \"class_names\"])\n",
    "                for key in npz.keys()]\n",
    "    isvar1 = False not in variant1\n",
    "    \n",
    "    variant2 = [(key in [\"adj_data\",\n",
    "                         \"adj_indices\",\n",
    "                         \"adj_indptr\",\n",
    "                         \"adj_shape\",\n",
    "                         \"attr_data\",\n",
    "                         \"attr_indices\",\n",
    "                         \"attr_indptr\",\n",
    "                         \"attr_shape\",\n",
    "                         \"labels\",\n",
    "                         \"node_names\",\n",
    "                         \"attr_names\",\n",
    "                         \"class_names\"])\n",
    "                for key in npz.keys()]\n",
    "    isvar2 = False not in variant2\n",
    "    \n",
    "    variant3 = [(key in [\"adj_matrix.data\",\n",
    "                         \"adj_matrix.indices\",\n",
    "                         \"adj_matrix.indptr\",\n",
    "                         \"adj_matrix.shape\",\n",
    "                         \"attr_matrix.data\",\n",
    "                         \"attr_matrix.indices\",\n",
    "                         \"attr_matrix.indptr\",\n",
    "                         \"attr_matrix.shape\",\n",
    "                         \"edge_attr_matrix\",\n",
    "                         \"labels\",\n",
    "                         \"node_names\",\n",
    "                         \"attr_names\",\n",
    "                         \"edge_attr_names\",\n",
    "                         \"class_names\",\n",
    "                         \"metadata\"])\n",
    "                for key in npz.keys()]\n",
    "    isvar3 = False not in variant3\n",
    "\n",
    "    match dataset_name:\n",
    "        case name if name in [\"A.Computer\", \"A.Photo\", \"MS-Academic\"]:\n",
    "            assert isvar1 or isvar2\n",
    "            features_csr = csr_matrix((npz[\"attr_data\"], npz[\"attr_indices\"], npz[\"attr_indptr\"]), shape=npz[\"attr_shape\"])\n",
    "            adjacency_csr = csr_matrix((npz[\"adj_data\"], npz[\"adj_indices\"], npz[\"adj_indptr\"]), shape=npz[\"adj_shape\"])\n",
    "        case name if name in name in [\"Citeseer\", \"Cora-ML\", \"PubMed\"]:\n",
    "            assert isvar3\n",
    "            features_csr = csr_matrix((npz[\"attr_matrix.data\"], npz[\"attr_matrix.indices\"], npz[\"attr_matrix.indptr\"]), shape=npz[\"attr_matrix.shape\"])\n",
    "            adjacency_csr = csr_matrix((npz[\"adj_matrix.data\"], npz[\"adj_matrix.indices\"], npz[\"adj_matrix.indptr\"]), shape=npz[\"adj_matrix.shape\"])\n",
    "        case _:\n",
    "            raise NotImplementedError(f\"The dataset cannot be loaded as it contains unexpected keys. The keys obtained are \\n{list(npz.keys())}\")\n",
    "    \n",
    "    x = torch.FloatTensor(features_csr.toarray())\n",
    "    edge_index, edge_weight = from_scipy_sparse_matrix(adjacency_csr)\n",
    "    y = torch.LongTensor(npz[\"labels\"])\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=y)\n",
    "    if len(np.unique(x)) > 2:\n",
    "        transform = NormalizeFeatures()\n",
    "        data = transform(data)\n",
    "\n",
    "    num_classes = len(set(npz[\"labels\"]))\n",
    "    \n",
    "    return data, num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will define the parameters to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config(\n",
    "    dataset: str,                                                                           # Only the dataset file name, without the .npz ending.\n",
    "    device : torch.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),     # Can override this manually as torch.device object.\n",
    "    pool: Pool = Pool.NONE,\n",
    "\n",
    "    # gumbel\n",
    "    learn_temp=False,\n",
    "    temp_model_type: ModelType.from_string=ModelType.LIN,\n",
    "    tau0: float=0.5,\n",
    "    temp: float=0.01,\n",
    "\n",
    "    # optimization\n",
    "    num_epochs: int=1000,\n",
    "    batch_size: int=32,\n",
    "    lr: float=1e-3,\n",
    "    dropout: float=0.2,\n",
    "\n",
    "    # env cls parameters\n",
    "    env_model_type: ModelType.from_string=ModelType.MEAN_GNN,\n",
    "    env_num_layers: int=3,\n",
    "    env_dim: int=128,\n",
    "    skip=False,\n",
    "    batch_norm=False,\n",
    "    layer_norm=False,\n",
    "    dec_num_layers: int=1,\n",
    "    pos_enc: PosEncoder.from_string=PosEncoder.NONE,\n",
    "\n",
    "    # policy cls parameters\n",
    "    act_model_type: ModelType=ModelType.MEAN_GNN,\n",
    "    act_num_layers: int=1,\n",
    "    act_dim: int=16,\n",
    "\n",
    "    # reproduce\n",
    "    seed: int=0,\n",
    "    gpu: int=None, # In the original argument parser, there is no default value defined here\n",
    "\n",
    "    # dataset dependant parameters\n",
    "    fold: int=None,\n",
    "\n",
    "    # optimizer and scheduler\n",
    "    weight_decay: float=0.0,\n",
    "    ## for steplr scheduler only\n",
    "    step_size: int=None,\n",
    "    gamma: float=None,\n",
    "    ## for cosine with warmup scheduler only\n",
    "    num_warmup_epochs: int=None,\n",
    "\n",
    "    decimal: int=2\n",
    "):\n",
    "    return {\n",
    "        'dataset' : dataset,\n",
    "        'device' : device,\n",
    "        'pool' : pool,\n",
    "        'learn_temp' : learn_temp,\n",
    "        'temp_model_type' : temp_model_type,\n",
    "        'tau0' : tau0,\n",
    "        'temp' : temp,\n",
    "        'num_epochs' : num_epochs,\n",
    "        'batch_size' : batch_size,\n",
    "        'lr' : lr,\n",
    "        'dropout' : dropout,\n",
    "        'env_model_type' : env_model_type,\n",
    "        'env_num_layers' : env_num_layers,\n",
    "        'env_dim' : env_dim,\n",
    "        'skip' : skip,\n",
    "        'batch_norm' : batch_norm,\n",
    "        'layer_norm' : layer_norm,\n",
    "        'dec_num_layers' : dec_num_layers,\n",
    "        'pos_enc' : pos_enc,\n",
    "        'act_model_type' : act_model_type,\n",
    "        'act_num_layers' : act_num_layers,\n",
    "        'act_dim' : act_dim,\n",
    "        'seed' : seed,\n",
    "        'gpu' : gpu,\n",
    "        'fold' : fold,\n",
    "        'weight_decay' : weight_decay,\n",
    "        'step_size' : step_size,\n",
    "        'gamma' : gamma,\n",
    "        'num_warmup_epochs' : num_warmup_epochs,\n",
    "        \"decimal\" : decimal\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self, data: Data):\n",
    "        self.data = data\n",
    "        self.family = True\n",
    "        self.is_node_based = True\n",
    "        self.not_synthetic = True\n",
    "        self.is_expressivity = False\n",
    "        self.clip_grad = False\n",
    "        self.dataset_encoders = DataSetEncoders.NONE\n",
    "        self.num_after_decimal = 2\n",
    "        self.env_activation_type = ActivationType.RELU\n",
    "\n",
    "    def gin_mlp_func(self) -> Callable:\n",
    "        def mlp_func(in_channels: int, out_channels: int, bias: bool):\n",
    "            return torch.nn.Sequential(\n",
    "                torch.nn.Linear(in_channels, 2 * in_channels, bias=bias),\n",
    "                torch.nn.BatchNorm1d(2 * in_channels),\n",
    "                torch.nn.ReLU(), torch.nn.Linear(2 * in_channels, out_channels, bias=bias)\n",
    "            )\n",
    "        return mlp_func\n",
    "    \n",
    "    def get_split_mask(self, batch_size: int, split_mask_name: str) -> Tensor:\n",
    "        if hasattr(self.data, split_mask_name):\n",
    "            return getattr(self.data, split_mask_name)\n",
    "        elif self.is_node_based():\n",
    "            return torch.ones(size=(self.data.x.shape[0],), dtype=torch.bool)\n",
    "        else:\n",
    "            return torch.ones(size=(batch_size,), dtype=torch.bool)\n",
    "    \n",
    "    def get_edge_ratio_node_mask(self, split_mask_name: str) -> Tensor:\n",
    "        if hasattr(self.data, split_mask_name):\n",
    "            return getattr(self.data, split_mask_name)\n",
    "        else:\n",
    "            return torch.ones(size=(self.data.x.shape[0],), dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Action Network (Policy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_act_net(action_args : dict) -> torch.nn.ModuleList:\n",
    "    model_type = action_args[\"model_type\"]\n",
    "    env_dim = action_args[\"env_dim\"]\n",
    "    hidden_dim = action_args[\"hidden_dim\"]\n",
    "    num_layers = action_args[\"num_layers\"]\n",
    "    gin_mlp_func = action_args[\"gin_mlp_func\"]\n",
    "\n",
    "    net = model_type.get_component_list(in_dim=env_dim,\n",
    "                                        hidden_dim=hidden_dim,\n",
    "                                        out_dim=2,\n",
    "                                        num_layers=num_layers,\n",
    "                                        bias=True,\n",
    "                                        edges_required=False,\n",
    "                                        gin_mlp_func=gin_mlp_func)\n",
    "\n",
    "    return torch.nn.ModuleList(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNet(torch.nn.Module):\n",
    "    def __init__(self, action_args: dict):\n",
    "        \"\"\"\n",
    "        Create a model which represents the agent's policy.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_layers = action_args[\"num_layers\"]\n",
    "        self.net = load_act_net(action_args=action_args)\n",
    "        self.dropout = torch.nn.Dropout(action_args[\"dropout\"])\n",
    "        self.act = action_args[\"act_type\"].get()\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj, env_edge_attr: OptTensor, act_edge_attr: OptTensor) -> Tensor:\n",
    "        edge_attrs = [env_edge_attr] + (self.num_layers - 1) * [act_edge_attr]\n",
    "        for idx, (edge_attr, layer) in enumerate(zip(edge_attrs[:-1], self.net[:-1])):\n",
    "            x = layer(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "            x = self.dropout(x)\n",
    "            x = self.act(x)\n",
    "        x = self.net[-1](x=x, edge_index=edge_index, edge_attr=edge_attrs[-1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional dynamic temperature calculation for the Gumbel Softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempSoftPlus(torch.nn.Module):\n",
    "    def __init__(self, gumbel_args: dict, env_dim: int):\n",
    "        super(TempSoftPlus, self).__init__()\n",
    "        model_list = gumbel_args[\"temp_model_type\"].get_component_list(in_dim=env_dim, hidden_dim=env_dim, out_dim=1, num_layers=1,\n",
    "                                                                       bias=False, edges_required=False,\n",
    "                                                                       gin_mlp_func=gumbel_args[\"gin_mlp_func\"])\n",
    "        self.linear_model = torch.nn.ModuleList(model_list)\n",
    "        self.softplus = torch.nn.Softplus(beta=1)\n",
    "        self.tau0 = gumbel_args[\"tau0\"]\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj, edge_attr: Tensor):\n",
    "        x = self.linear_model[0](x=x, edge_index=edge_index,edge_attr = edge_attr)\n",
    "        x = self.softplus(x) + self.tau0\n",
    "        temp = x.pow_(-1)\n",
    "        return temp.masked_fill_(temp == float(\"inf\"), 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoGNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will implement the CoGNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_env_net(env_args : dict) -> torch.nn.ModuleList:\n",
    "    in_dim = env_args[\"in_dim\"]\n",
    "    env_dim = env_args[\"env_dim\"]\n",
    "    num_layers = env_args[\"num_layers\"]\n",
    "    gin_mlp_func = env_args[\"gin_mlp_func\"]\n",
    "    dec_num_layers = env_args[\"dec_num_layers\"]\n",
    "    dropout = env_args[\"dropout\"]\n",
    "    act_type = env_args[\"act_type\"]\n",
    "    out_dim = env_args[\"out_dim\"]\n",
    "\n",
    "    enc_list = [env_args[\"dataset_encoders\"].node_encoder(in_dim=in_dim, emb_dim=env_dim)]\n",
    "\n",
    "    component_list = env_args[\"model_type\"].get_component_list(in_dim=env_dim, hidden_dim=env_dim, out_dim=env_dim,\n",
    "                                                               num_layers=num_layers, bias=True, edges_required=True,\n",
    "                                                               gin_mlp_func=gin_mlp_func)\n",
    "\n",
    "    if dec_num_layers > 1:\n",
    "        mlp_list = (dec_num_layers - 1) * [torch.nn.Linear(env_dim, env_dim), torch.nn.Dropout(dropout), act_type.nn()]\n",
    "        mlp_list = mlp_list + [torch.nn.Linear(env_dim, out_dim)]\n",
    "        dec_list = [torch.nn.Sequential(*mlp_list)]\n",
    "    else:\n",
    "        dec_list = [torch.nn.Linear(env_dim, out_dim)]\n",
    "\n",
    "    return torch.nn.ModuleList(enc_list + component_list + dec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoGNN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gumbel_args: dict,\n",
    "        env_args: dict,\n",
    "        action_args: dict,\n",
    "        pool: Pool\n",
    "    ):\n",
    "        super(CoGNN, self).__init__()\n",
    "        self.env_args = env_args\n",
    "        self.learn_temp = gumbel_args[\"learn_temp\"]\n",
    "        if self.learn_temp:\n",
    "            self.temp_model = TempSoftPlus(gumbel_args=gumbel_args, env_dim=env_args[\"env_dim\"])\n",
    "        self.temp = gumbel_args[\"temp\"]\n",
    "\n",
    "        self.num_layers = env_args[\"num_layers\"]\n",
    "        self.env_net = load_env_net(env_args=env_args)\n",
    "        self.use_encoders = env_args[\"dataset_encoders\"].use_encoders()\n",
    "\n",
    "        layer_norm_cls = torch.nn.LayerNorm if env_args[\"layer_norm\"] else torch.nn.Identity\n",
    "        self.hidden_layer_norm = layer_norm_cls(env_args[\"env_dim\"])\n",
    "        self.skip = env_args[\"skip\"]\n",
    "        self.drop_ratio = env_args[\"dropout\"]\n",
    "        self.dropout = torch.nn.Dropout(p=self.drop_ratio)\n",
    "        self.act = env_args[\"act_type\"].get()\n",
    "        self.in_act_net = ActionNet(action_args=action_args)\n",
    "        self.out_act_net = ActionNet(action_args=action_args)\n",
    "\n",
    "        # Encoder types\n",
    "        self.dataset_encoder = env_args[\"dataset_encoders\"]\n",
    "        self.env_bond_encoder = self.dataset_encoder.edge_encoder(emb_dim=env_args[\"env_dim\"], model_type=env_args[\"model_type\"])\n",
    "        self.act_bond_encoder = self.dataset_encoder.edge_encoder(emb_dim=action_args[\"hidden_dim\"], model_type=action_args[\"model_type\"])\n",
    "\n",
    "        # Pooling function to generate whole-graph embeddings\n",
    "        self.pooling = pool.get()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        edge_index: Adj,\n",
    "        pestat,\n",
    "        edge_attr: OptTensor = None,\n",
    "        batch: OptTensor = None,\n",
    "        edge_ratio_node_mask: OptTensor = None\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        result = 0\n",
    "\n",
    "        calc_stats = edge_ratio_node_mask is not None\n",
    "        if calc_stats:\n",
    "            edge_ratio_edge_mask = edge_ratio_node_mask[edge_index[0]] & edge_ratio_node_mask[edge_index[1]]\n",
    "            edge_ratio_list = []\n",
    "\n",
    "        # bond encode\n",
    "        if edge_attr is None or self.env_bond_encoder is None:\n",
    "            env_edge_embedding = None\n",
    "        else:\n",
    "            env_edge_embedding = self.env_bond_encoder(edge_attr)\n",
    "        if edge_attr is None or self.act_bond_encoder is None:\n",
    "            act_edge_embedding = None\n",
    "        else:\n",
    "            act_edge_embedding = self.act_bond_encoder(edge_attr)\n",
    "\n",
    "        # node encode  \n",
    "        x = self.env_net[0](x, pestat) # (N, F) encoder\n",
    "        if not self.use_encoders:\n",
    "            x = self.dropout(x)\n",
    "            x = self.act(x)\n",
    "\n",
    "        for gnn_idx in range(self.num_layers):\n",
    "            x = self.hidden_layer_norm(x)\n",
    "\n",
    "            # action\n",
    "            in_logits = self.in_act_net(x=x, edge_index=edge_index, env_edge_attr=env_edge_embedding, act_edge_attr=act_edge_embedding) # (N, 2)\n",
    "            out_logits = self.out_act_net(x=x, edge_index=edge_index, env_edge_attr=env_edge_embedding, act_edge_attr=act_edge_embedding) # (N, 2)\n",
    "\n",
    "            temp = self.temp_model(x=x, edge_index=edge_index, edge_attr=env_edge_embedding) if self.learn_temp else self.temp\n",
    "            in_probs = torch.nn.functional.gumbel_softmax(logits=in_logits, tau=temp, hard=True)\n",
    "            out_probs = torch.nn.functional.gumbel_softmax(logits=out_logits, tau=temp, hard=True)\n",
    "            edge_weight = self.create_edge_weight(edge_index=edge_index, keep_in_prob=in_probs[:, 0], keep_out_prob=out_probs[:, 0])\n",
    "\n",
    "            # environment\n",
    "            out = self.env_net[1 + gnn_idx](x=x, edge_index=edge_index, edge_weight=edge_weight, edge_attr=env_edge_embedding)\n",
    "            out = self.dropout(out)\n",
    "            out = self.act(out)\n",
    "\n",
    "            if calc_stats:\n",
    "                edge_ratio = edge_weight[edge_ratio_edge_mask].sum() / edge_weight[edge_ratio_edge_mask].shape[0]\n",
    "                edge_ratio_list.append(edge_ratio.item())\n",
    "\n",
    "            if self.skip:\n",
    "                x = x + out\n",
    "            else:\n",
    "                x = out\n",
    "\n",
    "        x = self.hidden_layer_norm(x)\n",
    "        x = self.pooling(x, batch=batch)\n",
    "        x = self.env_net[-1](x) # decoder\n",
    "        result = result + x\n",
    "\n",
    "        if calc_stats:\n",
    "            edge_ratio_tensor = torch.tensor(edge_ratio_list, device=x.device)\n",
    "        else:\n",
    "            edge_ratio_tensor = -1 * torch.ones(size=(self.num_layers,), device=x.device)\n",
    "\n",
    "        return result, edge_ratio_tensor\n",
    "\n",
    "    def create_edge_weight(\n",
    "        self,\n",
    "        edge_index: Adj,\n",
    "        keep_in_prob: Tensor,\n",
    "        keep_out_prob: Tensor\n",
    "    ) -> Tensor:\n",
    "        u, v = edge_index\n",
    "        edge_in_prob = keep_in_prob[v]\n",
    "        edge_out_prob = keep_out_prob[u]\n",
    "        return edge_in_prob * edge_out_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Experiment` class contains the procedure to load a dataset, run the training and evaluation, as well as return the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CoGNN(\n",
    "    model : torch.nn.Module,\n",
    "    task_loss : torch.nn.CrossEntropyLoss,\n",
    "    dataset : DataSet,\n",
    "    optimizer : torch.optim.Optimizer,\n",
    "    config : dict\n",
    "):\n",
    "    \"\"\"\n",
    "        Training loop for one epoch.\n",
    "    \"\"\"\n",
    "    device = config[\"device\"]\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    data = dataset.data.to(device)\n",
    "\n",
    "    train_mask = data.train_mask\n",
    "\n",
    "    predictions, edge_ratio_tensor = model(x=data.x,\n",
    "                                           edge_index=data.edge_index,\n",
    "                                           batch = torch.zeros(data.x.size(0), dtype=torch.long, device=device), # dummy batch for pooling in CoGNN\n",
    "                                           edge_attr=data.edge_attr if 'edge_attr' in data else None,\n",
    "                                           edge_ratio_node_mask=None,\n",
    "                                           pestat=config[\"pos_enc\"].get_pe(data=data, device=device))\n",
    "\n",
    "    train_loss = task_loss(predictions[train_mask], data.y.to(device=device)[train_mask])\n",
    "    train_loss.backward()\n",
    "\n",
    "    if config[\"clip_grad\"]:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return model, train_loss.item(), edge_ratio_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_CoGNN(\n",
    "    model : torch.nn.Module,\n",
    "    metric : Metric,\n",
    "    dataset : DataSet,\n",
    "    config : dict,\n",
    "    split_mask_name : str=None,\n",
    "    calc_edge_ratio : bool=True,\n",
    "    node_mask : np.ndarray=None\n",
    "): \n",
    "    \"\"\"\n",
    "        Evaluation function of the model.\n",
    "    \"\"\"\n",
    "    device = config[\"device\"]\n",
    "\n",
    "    model.eval()\n",
    "    data = dataset.data.to(device)\n",
    "\n",
    "    mask = dataset.get_split_mask(batch_size=None, split_mask_name=split_mask_name).to(device=device) if node_mask is None else node_mask\n",
    "    edge_ratio_node_mask = dataset.get_edge_ratio_node_mask(split_mask_name=split_mask_name).to(device) if calc_edge_ratio else None\n",
    "    edge_attr = data.edge_attr.to(device) if data.edge_attr is not None else None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions, edge_ratio_tensor = model(x=data.x,\n",
    "                                               edge_index=data.edge_index,\n",
    "                                               batch = torch.zeros(data.x.size(0), dtype=torch.long, device=device), # dummy batch for pooling in CoGNN\n",
    "                                               edge_attr=edge_attr,\n",
    "                                               edge_ratio_node_mask=edge_ratio_node_mask,\n",
    "                                               pestat=config[\"pos_enc\"].get_pe(data=data, device=device))\n",
    "        \n",
    "        eval_loss = metric.task_loss(predictions[mask], data.y[mask])\n",
    "    \n",
    "    scores_np = predictions[mask].detach().cpu().numpy()\n",
    "    labels_np = data.y[mask].detach().cpu().numpy()\n",
    "\n",
    "    accuracy = metric.apply_metric(scores=scores_np, target=labels_np)\n",
    "    loss = eval_loss.item()\n",
    "    edge_ratios = edge_ratio_tensor\n",
    "\n",
    "    return accuracy, loss, edge_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold_CoGNN(\n",
    "    config : dict,\n",
    "    dataset: DataSet,\n",
    "    model : torch.nn.Module,\n",
    "    optimizer : torch.optim.Optimizer,\n",
    "    pbar : tqdm.std.tqdm,\n",
    "    num_fold: int\n",
    ") -> Tuple[LossesAndMetrics, OptTensor]:\n",
    "    \"\"\"\n",
    "        Training loop for one fold.\n",
    "    \"\"\"\n",
    "\n",
    "    task_loss = config[\"metric\"].task_loss\n",
    "\n",
    "    best_losses_n_metrics = config[\"metric\"].get_worst_losses_n_metrics\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        model, epoch_loss, epoch_edge_ratio_tensor = train_CoGNN(model=model, task_loss=task_loss, dataset=dataset, optimizer=optimizer, config=config)\n",
    "\n",
    "        epoch_train_accuracy, epoch_train_loss, epoch_edge_ratios = evaluate_CoGNN(model=model,\n",
    "                                                                                   metric=config[\"metric\"],\n",
    "                                                                                   dataset=dataset,\n",
    "                                                                                   config=config,\n",
    "                                                                                   split_mask_name=\"train_mask\",\n",
    "                                                                                   calc_edge_ratio=False)\n",
    "\n",
    "        val_loss, val_metric, _ = evaluate_CoGNN(model=model,\n",
    "                                                 metric=config[\"metric\"],\n",
    "                                                 dataset=dataset,\n",
    "                                                 config=config,\n",
    "                                                 split_mask_name=\"val_mask\",\n",
    "                                                 calc_edge_ratio=False)\n",
    "\n",
    "        test_loss, test_metric, _ = evaluate_CoGNN(model=model,\n",
    "                                                   metric=config[\"metric\"],\n",
    "                                                   dataset=dataset,\n",
    "                                                   config=config,\n",
    "                                                   split_mask_name=\"test_mask\",\n",
    "                                                   calc_edge_ratio=False)\n",
    "\n",
    "        losses_n_metrics = LossesAndMetrics(train_loss=epoch_loss,\n",
    "                                            val_loss=val_loss,\n",
    "                                            test_loss=test_loss,\n",
    "                                            train_metric=epoch_train_accuracy,\n",
    "                                            val_metric=val_metric,\n",
    "                                            test_metric=test_metric)\n",
    "        \n",
    "        if config[\"metric\"].src_better_than_other(src=losses_n_metrics.val_metric,\n",
    "                                             other=best_losses_n_metrics.val_metric):\n",
    "            best_losses_n_metrics = losses_n_metrics\n",
    "\n",
    "        log_str = f\"Split: {num_fold}, epoch: {epoch}\"\n",
    "        for name in losses_n_metrics._fields:\n",
    "            log_str += f\",{name}={round(getattr(losses_n_metrics, name), config[\"decimal\"])}\"\n",
    "        log_str += f\"({round(best_losses_n_metrics.test_metric, config[\"decimal\"])})\"\n",
    "        pbar.set_description(log_str)\n",
    "        pbar.update(n=1)\n",
    "    \n",
    "    edge_ratios = None\n",
    "    _, _, edge_ratios = evaluate_CoGNN(model=model,\n",
    "                                       metric=config[\"metric\"],\n",
    "                                       dataset=dataset,\n",
    "                                       config=config,\n",
    "                                       split_mask_name=\"test_mask\",\n",
    "                                       calc_edge_ratio=False)\n",
    "\n",
    "    return best_losses_n_metrics, edge_ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_CoGNN(\n",
    "    data : Data,\n",
    "    config : dict,\n",
    "    num_folds : int\n",
    ") -> Tuple[torch.nn.Module, dict]:\n",
    "    \"\"\"\n",
    "        Training loop for n-fold with multiple epochs.\n",
    "    \"\"\"\n",
    "    #task_loss = config[\"metric\"].task_loss\n",
    "    decimal = config[\"decimal\"]\n",
    "    seeds = [config[\"seed\"] + n for n in range(num_folds)]\n",
    "\n",
    "    dataset = DataSet(data=data)\n",
    "    config[\"clip_grad\"] = dataset.clip_grad\n",
    "\n",
    "    gin_mlp_func = dataset.gin_mlp_func()\n",
    "    env_act_type = dataset.env_activation_type\n",
    "    dataset_encoder = dataset.dataset_encoders\n",
    "    out_dim = config[\"metric\"].get_out_dim(data=data)\n",
    "\n",
    "    gumbel = [\"learn_temp\", \"temp_model_type\", \"tau0\", \"temp\"]\n",
    "    gumbel_add = {\"gin_mlp_func\" :  gin_mlp_func}\n",
    "    gumbel_args = {key: config.get(key) for key in gumbel}\n",
    "    gumbel_args.update(gumbel_add)\n",
    "\n",
    "    env = [\"env_dim\", \"layer_norm\", \"skip\", \"batch_norm\", \"dropout\", \"metric\", \"dec_num_layers\", \"pos_enc\"]\n",
    "    env_add = {\"model_type\" :       config[\"env_model_type\"],\n",
    "               \"num_layers\" :       config[\"env_num_layers\"],\n",
    "               \"act_type\" :         env_act_type,\n",
    "               \"in_dim\" :           data.x.shape[1],\n",
    "               \"out_dim\" :          out_dim,\n",
    "               \"gin_mlp_func\" :     gin_mlp_func,\n",
    "               \"dataset_encoders\" : dataset_encoder}\n",
    "    env_args = {key: config.get(key) for key in env}\n",
    "    env_args.update(env_add)\n",
    "\n",
    "    action = [\"dropout\", \"env_dim\"]\n",
    "    action_add = {\"model_type\" :    config[\"act_model_type\"],\n",
    "                  \"num_layers\" :    config[\"act_num_layers\"],\n",
    "                  \"hidden_dim\" :    config[\"act_dim\"],\n",
    "                  \"act_type\" :      ActivationType.RELU,\n",
    "                  \"gin_mlp_func\" :  gin_mlp_func}\n",
    "    action_args = {key: config.get(key) for key in action}\n",
    "    action_args.update(action_add)\n",
    "\n",
    "    metrics_list = []\n",
    "    edge_ratios_list = []\n",
    "    for n in range(num_folds):\n",
    "        set_seed(seed=seeds[n])\n",
    "\n",
    "        model = CoGNN(gumbel_args=gumbel_args, env_args=env_args, action_args=action_args, pool=config[\"pool\"]).to(device=config[\"device\"])\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "        with tqdm.tqdm(total=config[\"num_epochs\"], file=sys.stdout) as pbar:\n",
    "            best_losses_n_metrics, edge_ratios = train_fold_CoGNN(config=config,\n",
    "                                                                  dataset=dataset,\n",
    "                                                                  model=model,\n",
    "                                                                  optimizer=optimizer,\n",
    "                                                                  pbar=pbar,\n",
    "                                                                  num_fold=n)\n",
    "        \n",
    "        print_str = f\"Fold {n}/{num_folds}\"\n",
    "        for name in best_losses_n_metrics._fields:\n",
    "            print_str += f\",{name}={round(getattr(best_losses_n_metrics, name), decimal)}\"\n",
    "        print(print_str)\n",
    "        print()\n",
    "        metrics_list.append(best_losses_n_metrics.get_fold_metrics())\n",
    "\n",
    "        if edge_ratios is not None:\n",
    "                edge_ratios_list.append(edge_ratios)\n",
    "    \n",
    "    metrics_matrix = torch.stack(metrics_list, dim=0)  # (F, 3)\n",
    "    metrics_mean = torch.mean(metrics_matrix, dim=0).tolist()  # (3,)\n",
    "    if len(edge_ratios_list) > 0:\n",
    "        edge_ratios = torch.mean(torch.stack(edge_ratios_list, dim=0), dim=0)\n",
    "    else:\n",
    "        edge_ratios = None\n",
    "    \n",
    "    print(f\"Final Rewired train={round(metrics_mean[0], decimal)},\"\n",
    "          f\"val={round(metrics_mean[1], decimal)},\"\n",
    "          f\"test={round(metrics_mean[2], decimal)}\")\n",
    "    \n",
    "    if num_folds > 1:\n",
    "        metrics_std = torch.std(metrics_matrix, dim=0).tolist()  # (3,)\n",
    "        print(f\"Final Rewired train={round(metrics_mean[0], decimal)}+-{round(metrics_std[0], decimal)},\"\n",
    "              f\"val={round(metrics_mean[1], decimal)}+-{round(metrics_std[1], decimal)},\"\n",
    "              f\"test={round(metrics_mean[2], decimal)}+-{round(metrics_std[2], decimal)}\")\n",
    "\n",
    "    # return metrics_mean, edge_ratios\n",
    "    history = None\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_CoGNN(\n",
    "    config : dict,\n",
    "    num_folds : int=10,\n",
    "    run_grid_search : bool=False\n",
    ") -> Tuple[torch.nn.Module, Data, dict]:\n",
    "    \"\"\"\n",
    "        Main function with optional grid search.\n",
    "    \"\"\"\n",
    "    device = config[\"device\"]\n",
    "\n",
    "    print(f\"Loading {config[\"dataset\"]} dataset...\")\n",
    "\n",
    "    # This is where we load the data\n",
    "    raw_data = get_dataset(name=config[\"dataset\"])\n",
    "    data = set_train_val_test_split(seed=42, data=raw_data.data)\n",
    "    data = data.to(device)\n",
    "    \n",
    "    config[\"metric\"] = Metric(task=\"multiclass\", num_classes=raw_data.num_classes)\n",
    "\n",
    "    if run_grid_search:\n",
    "        print(\"Running grid search...\")\n",
    "        raise NotImplementedError(\"Grid search hasn't been implemented yet!\")\n",
    "    \n",
    "        # List of the parameters and the values to be searched across\n",
    "        param_list_1 = []\n",
    "        param_list_2 = []\n",
    "        # ...\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # Adjust the nested loops for the various parameters\n",
    "        for param_1 in param_list_1:\n",
    "            for param_2 in param_list_2:\n",
    "                # ...\n",
    "                config_copy = config.copy()\n",
    "                config_copy[\"param_1_name\"] = param_1\n",
    "                config_copy[\"param_2_name\"] = param_2\n",
    "\n",
    "                print(f\"\\nTrying param_1_name={param_1}, param_2_name={param_2} ...\")\n",
    "                model = CoGNN(dataset, config_copy).to(device)\n",
    "                model, _ = train_model_CoGNN(model, data, config_copy)\n",
    "\n",
    "                test_acc, test_steps, _, _ = evaluate_CoGNN(model, data, data.test_mask)\n",
    "\n",
    "                results.append({\n",
    "                    \"param_1_name\": param_1,\n",
    "                    \"param_2_name\": param_2,\n",
    "                    # ...\n",
    "                    \"test_acc\": test_acc,\n",
    "                    \"test_steps\": test_steps\n",
    "                })\n",
    "\n",
    "                print(f\"Result: Test Acc: {test_acc:.4f}, Test Steps: {test_steps:.2f}\")\n",
    "\n",
    "        best_parameters = max(results, key=lambda x: x[\"test_acc\"])\n",
    "\n",
    "        print(\"\\nGrid Search Results:\")\n",
    "        for r in results:\n",
    "            print(f\"param_1_name={r[\"param_1_name\"]}, param_2_name={r[\"param_2_name\"]}: \" # ...\n",
    "                  f\"Acc={r[\"test_acc\"]:.4f}, Steps={r[\"test_steps\"]:.2f}\")\n",
    "\n",
    "        print(f\"\\nBest parameters: param_1_name={best_result[\"param_1_name\"]}, \" # ...\n",
    "              f\"param_2_name={best_result[\"param_2_name\"]}, \"\n",
    "              f\"Accuracy={best_result[\"test_acc\"]:.4f}\")\n",
    "\n",
    "        # config with best parameters\n",
    "        config[\"param_1_name\"] = best_result[\"param_1_name\"]\n",
    "        config[\"param_2_name\"] = best_result[\"param_2_name\"]\n",
    "        # ...    \n",
    "\n",
    "    # Instantiating the model with obtained config\n",
    "    print(\"\\nTraining final model...\")\n",
    "    #model = CoGNN(gumbel_args, env_args, action_args, config[\"pool\"]).to(device)\n",
    "\n",
    "    # Training the model\n",
    "    model, history = train_model_CoGNN(data=data, config=config, num_folds=num_folds)\n",
    "\n",
    "    # test eval\n",
    "    test_acc, test_steps, _, _ = evaluate_CoGNN(model, data, data.test_mask)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}, Average Steps: {test_steps:.2f}\")\n",
    "\n",
    "    # visualizations\n",
    "    # TO-DO!\n",
    "\n",
    "    # optional: impact of various parameters\n",
    "\n",
    "    return model, data, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    CONFIG = create_config(dataset=\"A.Computer.npz\", seed=42)\n",
    "    model, data, history = main_CoGNN(config=CONFIG, num_folds=10, run_grid_search=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textcolor{red}{\\texttt{TO - DO!}}$\n",
    "\n",
    "- train fold CoGNN and below needs to be adjusted.\n",
    "- grid search in `main` function. What parameters to search across and what should the values be?\n",
    "- Check that all quotations are \" not '\n",
    "- Check whether the dataset classification (homophilic etc.) is okay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AdvancedTopicsInMachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
